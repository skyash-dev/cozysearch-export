> Sta­tis­ti­cal folk­lore as­serts that “every­thing is cor­re­lated”: in any real- world dataset, most or all mea­sured vari­ables will have non- zero cor­re­la­tions, even be­tween vari­ables which ap­pear to be com­pletely in­de­pen­dent of each other, and that these cor­re­la­tions are not merely sam­pling error flukes but will ap­pear in large- scale datasets to ar­bi­trar­ily des­ig­nated lev­els of [statistical- significance ⁠](https://en.wikipedia.org/wiki/Statistical_significance) or pos­te­rior prob­a­bil­ity.
> 
> This raises se­ri­ous ques­tions for null- hypothesis statistical- significance test­ing, as it im­plies the null hy­poth­e­sis of 0 will al­ways be re­jected with suf­fi­cient data, mean­ing that a fail­ure to re­ject only im­plies in­suf­fi­cient data, and pro­vides no ac­tual test or con­fir­ma­tion of a the­ory. Even a di­rec­tional pre­dic­tion is min­i­mally con­fir­ma­tory since there is a 50% chance of pick­ing the right di­rec­tion at ran­dom.
> 
> It also has im­pli­ca­tions for con­cep­tu­al­iza­tions of the­o­ries & causal mod­els, in­ter­pre­ta­tions of struc­tural mod­els, and other sta­tis­ti­cal prin­ci­ples such as the “spar­sity prin­ci­ple”.
> 
> ![Figure 1 (Smith et al 2007): Histogram of Statistically-Significant (at α = 1%) Age-Adjusted Pairwise Correlation Coefficients between 96 Nongenetic Characteristics. British Women Aged 60--79 years old. Demonstrates that pair-wise correlations are common even in apparently unrelated traits.](https://gwern.net/doc/genetics/heritable/correlation/2007-smith-figure1-correlationdistribution.jpg)

Know­ing one vari­able tells you (a lit­tle) about every­thing else. In sta­tis­tics & psy­chol­ogy folk­lore, this idea cir­cu­lates under many names: “every­thing is cor­re­lated”, “every­thing is re­lated to every­thing else”, “crud fac­tor”, “the null hy­poth­e­sis is al­ways false”, “co­ef­fi­cients are never zero”, “am­bi­ent cor­re­la­tional noise”, [Thorndike’s ⁠](https://en.wikipedia.org/wiki/Edward_Thorndike) dic­tum (“in human na­ture good traits go to­gether”⁠ [1](#fn:1)), etc. Closely re­lated are the [“bet on spar­sity prin­ci­ple” ⁠](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.644.2669&rep=rep1&type=pdf#page=518) ⁠ [2](#fn:2), [Anna Karen­ina prin­ci­ple ⁠](https://en.wikipedia.org/wiki/Anna_Karenina_principle), [Barry Com­moner’s ⁠](https://en.wikipedia.org/wiki/Barry_Commoner) “first law of ecol­ogy” (“Every­thing is con­nected to every­thing else”) & [Waldo R. To­bler’s ⁠](https://en.wikipedia.org/wiki/Waldo_R._Tobler) [“first law of ge­og­ra­phy” ⁠](https://en.wikipedia.org/wiki/Tobler%27s_first_law_of_geography) ([“every­thing is re­lated to every­thing else, but near things are more re­lated than dis­tant things” ⁠](https://pdfs.semanticscholar.org/eaa5/eefedd4fa34b7de7448c0c8e0822e9fdf956.pdf)).⁠ [3](#fn:3)

The core idea here is that in any real- world dataset, it is ex­cep­tion­ally un­likely that any par­tic­u­lar re­la­tion­ship will be ex­actly 0 for rea­sons of arith­metic (eg. it may be im­pos­si­ble for a bi­nary vari­able to be an equal per­cent­age in 2 un­bal­anced groups); prior prob­a­bil­ity (0 is only one num­ber out of the in­fi­nite reals); and be­cause real- world prop­er­ties & traits are linked by a myr­iad of causal net­works, dy­nam­ics, & [la­tent ⁠](https://en.wikipedia.org/wiki/Latent_and_observable_variables) vari­ables (eg. the [ge­netic cor­re­la­tions ⁠](https://en.wikipedia.org/wiki/Genetic_correlation) which af­fect all human traits, see [⁠heat maps in ap­pen­dix ⁠](https://gwern.net/everything#genetic-correlations) for vi­su­al­iza­tions) which mu­tu­ally af­fect each other which will pro­duce gen­uine cor­re­la­tions be­tween apparently- independent vari­ables, and these cor­re­la­tions may be of sur­pris­ingly large & im­por­tant size.

These rea­sons are un­af­fected by sam­ple size and are not sim­ply due to ‘small _n_ ’. If we sim­u­late out un­cor­re­lated ran­dom vari­ables, even with small sizes, they quickly ap­proach ab­solute cor­re­la­tions of ~0, and few will be of mean­ing­ful size like | _r_ | > 0.10:

```
# Simulation of the 'crud factor' in psychology, where variables seem to always be intercorrelated non-zero, with a rough absolute correlation of 0.1.
# I would like to simulate out the null hypothesis of completely uncorrelated variables in plausible dataset sizes.
# So this is a R Monte Carlo simulation to simulate a multivariate normal distribution of 1,000 independent, uncorrelated N(0,1) variables and drawing 1,000 datapoints from it; calculate the sample r correlation of all the variables, and what fraction are correlated r > |0.1|. Then let's Monte Carlo that, say, 100 times (to avoid taking too long) and plot the averaged distributions of the |r| histogram.
# Output:
# # Mean proportion of |r| > 0.1: 0.002 (SD: 0.000)
library(MASS)
library(ggplot2)
library(reshape2)
library(parallel)
 
# Parameters
n_vars <- 1000
n_samples <- 1000
n_sims <- 100
threshold <- 0.1
 
# Function to run one simulation and return correlation metrics
run_simulation <- function(seed, n_vars, n_samples, threshold) {
  set.seed(seed)
  sigma <- diag(n_vars)
  data <- mvrnorm(n = n_samples,
                  mu = rep(0, n_vars),
                  Sigma = sigma)
 
  cors <- cor(data)
  cors_upper <- cors[upper.tri(cors)]
 
  # Return both the histogram counts and proportion above threshold
  hist_data <- hist(abs(cors_upper), breaks = seq(0, 1, by = 0.01), plot = FALSE)
 
  return(list(
    hist_counts = hist_data$counts,
    prop_above_thresh = mean(abs(cors_upper) > threshold)
  ))
}
 
# Set up parallel processing
n_cores <- detectCores() - 1
cl <- makeCluster(n_cores)
 
# Export required packages and variables to the cluster
clusterEvalQ(cl, {
  library(MASS)
})
clusterExport(cl, c("n_vars", "n_samples", "threshold"))
 
# Run simulations in parallel
seeds <- 1:n_sims
results <- parLapply(cl, seeds, run_simulation,
                    n_vars = n_vars,
                    n_samples = n_samples,
                    threshold = threshold)
 
# Clean up
stopCluster(cl)
 
# Process results
props <- sapply(results, function(x) x$prop_above_thresh)
mean_prop <- mean(props)
 
# Average the histogram counts across simulations
avg_counts <- Reduce('+', lapply(results, function(x) x$hist_counts)) / n_sims
 
# Calculate total possible correlations for one simulation
total_cors <- (n_vars * (n_vars - 1)) / 2
 
# Create plotting data
plot_data <- data.frame(
  correlation = seq(0, 0.99, by = 0.01)[1:length(avg_counts)],
  count = avg_counts
)
 
# Create histogram
p1 <- ggplot(plot_data, aes(x = correlation, y = count/total_cors * 100)) +
  geom_bar(stat = "identity", fill = "blue", alpha = 0.7) +
  theme_bw(base_size = 40) +
  geom_vline(xintercept = threshold, color = "red", linetype = "dashed", size=3) +
  theme(plot.title = element_text(face = "bold")) +
  scale_y_continuous(
    labels = function(x) paste0(round(x, 1), "%"),
    name = "Percentage of All Inter-Correlations",
    expand = c(0, 0),
    limits = c(0, NA)
  ) +
  scale_x_continuous(
    name = expression(paste("Absolute Correlation (|", italic("r"), "|)")),
    breaks = seq(0, 0.13, by = 0.02),
    expand = c(0, 0),
    limits = c(0, 0.13)
  ) +
  annotate("text",
           x = 0.101,
           y = max(avg_counts/total_cors * 100)/3,
           label = substitute(paste(value, "% exceed |", italic("r"), "| > ", thresh),
                            list(value = sprintf("%.1f", 100 * mean_prop),
                                 thresh = sprintf("%.2f", threshold))),
           size = 10,
           hjust = 0) +
  labs(title = "Distribution of Absolute Correlations")
 
# Print summary statistics
cat(sprintf("Mean proportion of |r| > 0.1: %.3f (SD: %.3f)\n", mean(props), sd(props)))
 
print(p1)
```

![A Monte Carlo simulation of an uncorrelated multivariate normal in R shows that even with p = 1,000 and n = 1,000 (many variables and a small dataset), we rarely will observe ‘crud factor’-style correlations between uncorrelated variables, and so the crud factor is not a statistical triviality.](https://gwern.net/doc/statistics/probability/2025-01-29-gwern-r-uncorrelatedmultivariatenormalsarerarelycorrelatedrgreaterthanpoint1.jpg)

A [Monte Carlo sim­u­la­tion ⁠](https://en.wikipedia.org/wiki/Monte_Carlo_method) of an un­cor­re­lated mul­ti­vari­ate nor­mal in R shows that even with _p_ = 1,000 and _n_ = 1,000 (many vari­ables and a small dataset), we rarely will ob­serve ‘crud fac­tor’- style cor­re­la­tions be­tween un­cor­re­lated vari­ables, and so the crud fac­tor is not a sta­tis­ti­cal triv­i­al­ity.

The claim is gen­er­ally backed up by per­sonal ex­pe­ri­ence and rea­son­ing, al­though in a few in­stances like Meehl large datasets are men­tioned in which al­most all vari­ables are cor­re­lated at high lev­els of statistical- significance.

## Importance

This claim has sev­eral im­pli­ca­tions:

1.  **Sharp null hy­pothe­ses are mean­ing­less**: The most com­monly men­tioned, and the ap­par­ent mo­ti­va­tion for early dis­cus­sions, is that in the null- hypothesis [significance- testing ⁠](https://en.wikipedia.org/wiki/Statistical_hypothesis_test) par­a­digm dom­i­nant in psy­chol­ogy and many sci­ences, any sharp null- hypothesis such as a pa­ra­me­ter (like a Pear­son’s _r_ cor­re­la­tion) being ex­actly equal to 0 is known—in ad­vance—to al­ready be false and so it will in­evitably be re­jected as soon as suf­fi­cient data col­lec­tion per­mits sam­pling to the fore­gone con­clu­sion.
    
    The ex­is­tence of per­va­sive cor­re­la­tions, in ad­di­tion to the pres­ence of sys­tem­atic error⁠ [4](#fn:4), guar­an­tees nonzero ‘ef­fects’. This ren­ders the mean­ing of significance- testing un­clear; it is cal­cu­lat­ing pre­cisely the odds of the data under sce­nar­ios known _a pri­ori_ to be false.
    
2.  **Di­rec­tional hy­pothe­ses are lit­tle bet­ter**: bet­ter null- hypotheses, such as >0 or <0, are also prob­lem­atic since if the true value of a pa­ra­me­ter is never 0 then one’s the­o­ries have at least a 50-50 chance of guess­ing the right di­rec­tion and so cor­rect ‘pre­dic­tions’ of the sign count for lit­tle.
    
    This ren­ders any suc­cess­ful pre­dic­tions of lit­tle value.
    
3.  **Model in­ter­pre­ta­tion is dif­fi­cult**: This ex­ten­sive in­ter­cor­re­la­tion threat­ens many naive sta­tis­ti­cal mod­els or the­o­ret­i­cal in­ter­pre­ta­tions thereof, quite aside from _p_ - values
    
    For ex­am­ple, given the large amounts of [mea­sure­ment error ⁠](https://en.wikipedia.org/wiki/Observational_error) in most so­ci­o­log­i­cal or psy­cho­log­i­cal traits such as [SES ⁠](https://en.wikipedia.org/wiki/Socioeconomic_status), home en­vi­ron­ment, or IQ, fully ‘con­trol­ling for’ a la­tent vari­able based on mea­sured vari­ables is dif­fi­cult or im­pos­si­ble and said vari­able will in fact be cor­re­lated with the pri­mary vari­able of in­ter­est, lead­ing to [“resid­ual con­found­ing” ⁠](https://gwern.net/doc/statistics/bayes/regression-to-mean/index)
    
4.  **In­ter­cor­re­la­tion im­plies causal net­works**: The em­pir­i­cal fact of ex­ten­sive in­ter­cor­re­la­tions is con­sis­tent with the [ex­is­tence of com­plex causal net­works & la­tent vari­ables ⁠](https://gwern.net/causality) (often fac­tors) link­ing all mea­sured traits, such as ex­ten­sive her­i­tabil­ity & ge­netic cor­re­la­tions of human traits, lead­ing to [ex­ten­sive ex­am­ples of cor­re­la­tion ≠ cau­sa­tion ⁠](https://gwern.net/correlation).
    
    The ex­is­tence of both “every­thing is cor­re­lated” and the suc­cess of the “bet on spar­sity” prin­ci­ple sug­gests that these causal net­works may be best thought of as hav­ing hubs or la­tent vari­ables: there are a rel­a­tively few vari­ables such as ‘arousal’ or ‘IQ’ which play cen­tral roles, ex­plain­ing much of [vari­ance ⁠](https://en.wikipedia.org/wiki/Variance), fol­lowed by al­most all other vari­ables ac­count­ing for a lit­tle bit each with most of their in­flu­ence me­di­ated through the key vari­ables.
    
    The fact that these vari­ables can be suc­cess­fully mod­eled as sub­stan­tively lin­ear or ad­di­tive fur­ther im­plies that in­ter­ac­tions be­tween vari­ables will be typ­i­cally rare or small or both (im­ply­ing fur­ther that most such hits will be false pos­i­tives, as in­ter­ac­tions are al­ready harder to de­tect than main ef­fects, and more so if they are _a pri­ori_ un­likely or of small size). Even ex­tremely large & deeply phe­no­typed datasets may strug­gle to achieve im­pres­sive im­prove­ments over base­lines using the core vari­ables (eg. [Sal­ganik et al 2020 ⁠](https://www.pnas.org/doi/10.1073/pnas.1915006117)).
    
    To the ex­tent that these key vari­ables are un­mod­i­fi­able, the many pe­riph­eral vari­ables may also be un­mod­i­fi­able (which may be re­lated to the [broad fail­ure of so­cial in­ter­ven­tion pro­grams ⁠](https://gwern.net/doc/sociology/1987-rossi)). Any in­ter­ven­tion on those pe­riph­eral vari­ables, being ‘down­stream’, will tend to ei­ther be ‘hol­low’ or fade out or have no ef­fect at all on the true de­sired goals no mat­ter how con­sis­tently they are cor­re­lated.
    
    On a more con­tem­po­rary note, these the­o­ret­i­cal & em­pir­i­cal con­sid­er­a­tions also throw doubt on con­cerns about ‘al­go­rith­mic bias’ or in­fer­ences draw­ing on ‘pro­tected classes’: not draw­ing on them may not be de­sir­able, pos­si­ble, or even mean­ing­ful.
    
5.  **Un­cor­re­lated vari­ables may be mean­ing­less**: given this em­pir­i­cal re­al­ity, any vari­able which is un­cor­re­lated with the major vari­ables is sus­pi­cious (some­what like the [per­va­sive­ness of her­i­tabil­ity ⁠](https://gwern.net/doc/genetics/heritable/2015-polderman.pdf) in human traits ren­ders traits with zero her­i­tabil­ity sus­pi­cious, sug­gest­ing is­sues like mea­sur­ing at the wrong time). The lack of cor­re­la­tion sug­gests that the analy­sis is un­der­pow­ered, some­thing has gone wrong in the con­struc­tion of the vari­able/ dataset, or that the vari­able is part of a sys­tem whose causal net­work ren­ders con­ven­tional analy­ses dan­ger­ously mis­lead­ing.
    
    For ex­am­ple, the dataset may be cor­rupted by a sys­tem­atic bias such as [range re­stric­tion ⁠](https://en.wikipedia.org/wiki/Statistical_conclusion_validity#Restriction_of_range) or a se­lec­tion ef­fect such as [Simp­son’s para­dox ⁠](https://en.wikipedia.org/wiki/Simpson%27s_paradox), which erases from the data a cor­re­la­tion that ac­tu­ally ex­ists. Or the data may be ran­dom noise, due to soft­ware error or fraud or ex­tremely high lev­els of mea­sure­ment error (such as [“lizard­man con­stant” ⁠](https://gwern.net/doc/sociology/survey/lizardman/index) re­spon­dents an­swer­ing at ran­dom); or the vari­able may not be real in the first place. An­other pos­si­bil­ity is that the vari­able is causally con­nected, in feed­back loops (es­pe­cially com­mon in eco­nom­ics or bi­ol­ogy), to an­other vari­able, in which case the stan­dard sta­tis­ti­cal ma­chin­ery is mis­lead­ing—the clas­sic ex­am­ple is Mil­ton Fried­man’s ther­mo­stat, not­ing that a ther­mo­stat would be al­most en­tirely un­cor­re­lated with room tem­per­a­ture.
    

This idea, as sug­gested by the many names, is not due to any sin­gle the­o­ret­i­cal or em­pir­i­cal re­sult or re­searcher, but has been made many times by many dif­fer­ent re­searchers in many con­texts, cir­cu­lat­ing as in­for­mal ‘folk­lore’. To bring some order to this, I have com­piled ex­cerpts of some rel­e­vant ref­er­ences in chrono­log­i­cal order. (Ad­di­tional ci­ta­tions are wel­come.)

## Gosset / Student1904

A ver­sion of this is at­trib­uted to [William Sealy Gos­set ⁠](https://en.wikipedia.org/wiki/William_Sealy_Gosset) (Stu­dent) in his [1904 in­ter­nal re­port ⁠](https://gwern.net/doc/statistics/decision/1904-gosset.pdf) ⁠ [5](#fn:5) by [Zil­iak 2008 ⁠](https://gwern.net/doc/statistics/decision/2008-ziliak.pdf):

> In early No­vem­ber 1904 121ya, Gos­set dis­cussed his first break­through in an in­ter­nal re­port en­ti­tled “The Ap­pli­ca­tion of the ‘Law of Error’ to the Work of the Brew­ery” (Gos­set 1904; _Lab­o­ra­tory Re­port_, Nov. 3, 1904 121ya, pg3). Gos­set (p. 3–16) wrote:
> 
> > Re­sults are only valu­able when the amount by which they prob­a­bly dif­fer from the truth is so small as to be in­signif­i­cant for the pur­poses of the ex­per­i­ment. What the odds should be de­pends
> > 
> > 1.  On the de­gree of ac­cu­racy which the na­ture of the ex­per­i­ment al­lows, and
> >     
> > 2.  On the im­por­tance of the is­sues at stake.
> >     
> 
> Two fea­tures of Gos­set’s re­port are es­pe­cially worth high­light­ing here. First, he sug­gested that judg­ments about “sig­nif­i­cant” dif­fer­ences were not a purely prob­a­bilis­tic ex­er­cise: they de­pend on the “im­por­tance of the is­sues at stake.” Sec­ond, Gos­set un­der­scored a pos­i­tive cor­re­la­tion in the [nor­mal dis­tri­b­u­tion ⁠](https://en.wikipedia.org/wiki/Normal_distribution) curve be­tween “the square root of the num­ber of ob­ser­va­tions” and the level of statistical- significance. Other things equal, he wrote, “the greater the num­ber of ob­ser­va­tions of which means are taken [the larger the sam­ple size], the smaller the [prob­a­ble or stan­dard] error” (pg5). “And the curve which rep­re­sents their fre­quency of error”, he il­lus­trated, “be­comes taller and nar­rower” (pg7).
> 
> Since its dis­cov­ery in the early 19 th cen­tury, ta­bles of the nor­mal prob­a­bil­ity curve had been cre­ated for large sam­ples…The re­la­tion be­tween sam­ple size and “sig­nif­i­cance” was rarely ex­plored. For ex­am­ple, while look­ing at bio­met­ric sam­ples with up to thou­sands of ob­ser­va­tions, Karl Pear­son de­clared that a re­sult de­part­ing by more than 3 stan­dard de­vi­a­tions is “def­i­nitely sig­nif­i­cant.” 12 Yet Gos­set, a self- trained sta­tis­ti­cian, found that at such large sam­ples, nearly every­thing is _sta­tis­ti­cally_ “sig­nif­i­cant”—though not, in Gos­set’s terms, eco­nom­i­cally or sci­en­tif­i­cally “im­por­tant.” Re­gard­less, Gos­set didn’t have the lux­ury of large sam­ples. One of his ear­li­est ex­per­i­ments em­ployed a sam­ple size of 2 (Gos­set, 1904 121ya, p.7) and in fact in [“The Prob­a­ble Error of a Mean” ⁠](https://bayes.wustl.edu/Manual/Student.pdf) he cal­cu­lated a _t_ sta­tis­tic for _n_ = 2 (Stu­dent, 1908 117ya b, p. 23).
> 
> …the “de­gree of cer­tainty to be aimed at”, Gos­set wrote, de­pends on the op­por­tu­nity cost of fol­low­ing a re­sult as if true, added to the op­por­tu­nity cost of con­duct­ing the ex­per­i­ment it­self. Gos­set never de­vi­ated from this cen­tral po­si­tion.15 [See, for ex­am­ple, Stu­dent ([1923, p. 271, para­graph one ⁠](https://gwern.net/doc/statistics/decision/1923-student.pdf): “The ob­ject of test­ing va­ri­eties of ce­re­als is to find out which will pay the farmer best.”) and Stu­dent ([⁠1931c ⁠](https://archive.org/details/in.ernet.dli.2015.233812/page/n168), p. 1342, para­graph one) reprinted in Stu­dent (1942 83ya, p. 90 and p. 150).]

## Thorndike1920

[“In­tel­li­gence and Its Uses” ⁠](https://gwern.net/doc/iq/1920-thorndike-2.pdf), Ed­ward L. Thorndike 1920 ([_Harper’s Monthly_ ⁠](https://en.wikipedia.org/wiki/Harper%27s_Magazine)):

> …the sig­nif­i­cance of in­tel­li­gence for suc­cess in a given ac­tiv­ity of life is mea­sured by the co­ef­fi­cient of cor­re­la­tion be­tween them. Sci­en­tific in­ves­ti­ga­tions of these mat­ters is just be­gin­ning; and it is a mat­ter of great dif­fi­culty and ex­pense to mea­sure the in­tel­li­gence of, say, a thou­sand cler­gy­men, and then se­cure suf­fi­cient ev­i­dence to rate them ac­cu­rately for their suc­cess as min­is­ters of the Gospel. Con­se­quently, one can re­port no final, per­fectly au­thor­i­ta­tive re­sults in this field. One can only or­ga­nize rea­son­able es­ti­mates from the var­i­ous par­tial in­ves­ti­ga­tions that have been made. Doing this, I find the fol­low­ing:
> 
> -   In­tel­li­gence and suc­cess in the el­e­men­tary schools, _r_ = +0.80
>     
> -   In­tel­li­gence and suc­cess in high- school and col­leges in the case of those who go, _r_ = +0.60; but if all were forced to try to do this ad­vanced work, the cor­re­la­tion would be +0.80 or more.
>     
> -   In­tel­li­gence and salary, _r_ = +0.35.
>     
> -   In­tel­li­gence and suc­cess in ath­letic sports, _r_ = +0.25
>     
> -   In­tel­li­gence and char­ac­ter, _r_ = +0.40
>     
> -   In­tel­li­gence and pop­u­lar­ity, _r_ = +0.20
>     
> 
> What­ever be the even­tual exact find­ings, two sound prin­ci­ples are il­lus­trated by our pro­vi­sional list. First, there is al­ways some re­sem­blance; in­tel­lect al­ways counts. Sec­ond, the re­sem­blance varies greatly; in­tel­lect counts much more in some lines than in oth­ers.
> 
> The first fact is in part a con­se­quence of a still broader fact or prin­ci­ple—namely, that in human na­ture good traits go to­gether. To him that hath a su­pe­rior in­tel­lect is given also on the av­er­age a su­pe­rior char­ac­ter; the quick boy is also in the long run more ac­cu­rate; the able boy is also more in­dus­tri­ous. There is no prin­ci­ple of com­pen­sa­tion whereby a weak in­tel­lect is off­set by a strong will, a poor mem­ory by good judg­ment, or a lack of am­bi­tion by an at­trac­tive per­son­al­ity. Every pair of such sup­posed com­pen­sat­ing qual­i­ties that have been in­ves­ti­gated has been found re­ally to show cor­re­spon­dence. Pop­u­lar opin­ion has been mis­led by at­tend­ing to strik­ing in­di­vid­ual cases which at­tracted at­ten­tion partly be­cause they were re­ally ex­cep­tions to the rule. The rule is that de­sir­able qual­i­ties are pos­i­tively cor­re­lated. In­tel­lect is good in and of it­self, and also for what it im­plies about other traits.

## Berkson1938

[“Some dif­fi­cul­ties of in­ter­pre­ta­tion en­coun­tered in the ap­pli­ca­tion of the chi- square test” ⁠](http://www.stats.org.uk/statistical-inference/Berkson1938.pdf), [Berk­son ⁠](https://en.wikipedia.org/wiki/Joseph_Berkson) 1938 87ya:

> I be­lieve that an ob­ser­vant sta­tis­ti­cian who has had any con­sid­er­able ex­pe­ri­ence with ap­ply­ing the chi- square test re­peat­edly will agree with my state­ment that, as a mat­ter of ob­ser­va­tion, when the num­bers in the data are quite large, the _P_ ’s tend to come out small. Hav­ing ob­served this, and on re­flec­tion, I make the fol­low­ing dog­matic state­ment, re­fer­ring for il­lus­tra­tion to the nor­mal curve: “If the nor­mal curve is fit­ted to a body of data rep­re­sent­ing any real ob­ser­va­tions what­ever of quan­ti­ties in the phys­i­cal world, then if the num­ber of ob­ser­va­tions is ex­tremely large—for in­stance, on an order of 200,000—the chi- square _P_ will be small be­yond any usual limit of sig­nif­i­cance.”
> 
> This dog­matic state­ment is made on the basis of an ex­trap­o­la­tion of the ob­ser­va­tion re­ferred to and can also be de­fended as a pre­dic­tion from _a pri­ori_ con­sid­er­a­tions. For we may as­sume that it is prac­ti­cally cer­tain that any se­ries of real ob­ser­va­tions does not ac­tu­ally fol­low a nor­mal curve _with ab­solute ex­ac­ti­tude_ in all re­spects, and no mat­ter how small the dis­crep­ancy be­tween the nor­mal curve and the true curve of ob­ser­va­tions, the chi- square _P_ will be small if the sam­ple has a suf­fi­ciently large num­ber of ob­ser­va­tions in it.
> 
> If this be so, then we have some­thing here that is apt to trou­ble the con­science of a re­flec­tive sta­tis­ti­cian using the chi- square test. For I sup­pose it would be agreed by sta­tis­ti­cians that a large sam­ple is al­ways bet­ter than a small sam­ple. If, then, we know in ad­vance the _P_ that will re­sult from an ap­pli­ca­tion of a chi- square test to a large sam­ple, there would seem to be no use in doing it on a smaller one. But since the re­sult of the for­mer test is known, it is no test at all.

## Thorndike1939

[_Your City_ ⁠](https://gwern.net/doc/sociology/1939-thorndike-yourcity.pdf), Thorndike 1939 (and the fol­lowup [_144 Smaller Cities_ ⁠](https://gwern.net/doc/sociology/1940-thorndike-144smallercities.pdf) pro­vid­ing ta­bles for 144 cities) com­piles var­i­ous sta­tis­tics about Amer­i­can cities such as in­fant mor­tal­ity, spend­ing on the arts, crime etc and finds ex­ten­sive in­ter­cor­re­la­tions & fac­tors.

The gen­eral fac­tor of so­cioe­co­nomic sta­tus or ‘S- factor’ also ap­plies across coun­tries as well: eco­nomic growth is by far the largest in­flu­ence on all mea­sures of well- being, and at­tempts at com­put­ing in­ter­na­tional rank­ings of things like ma­ter­nal founder on this fact, as they typ­i­cally wind up sim­ply re­pro­duc­ing GDP rank- orderings and being re­dun­dant. For ex­am­ple, [Jones & Klenow 2016 ⁠](http://klenow.com/Jones_Klenow.pdf) com­pute an in­ter­na­tional well­be­ing met­ric using “life ex­pectancy, the ratio of con­sump­tion to in­come, an­nual hours worked per capita, the stan­dard de­vi­a­tion of log con­sump­tion, and the stan­dard de­vi­a­tion of an­nual hours worked” to in­cor­po­rate fac­tors like in­equal­ity, but this still winds up just being equiv­a­lent to GDP (_r_ = 0.98). Or [Gill & Geb­hart 2016 ⁠](https://mpra.ub.uni-muenchen.de/74268/1/MPRA_paper_74268.pdf), who note that of 9 in­ter­na­tional in­dices they con­sider, all cor­re­late pos­i­tively with per capita GDP, & 6 have [rank- correlations ⁠](https://en.wikipedia.org/wiki/Kendall_rank_correlation_coefficient) τ > 0.5.

## Good1950

[_Prob­a­bil­ity and the Weigh­ing of Ev­i­dence_ ⁠](https://gwern.net/doc/statistics/bayes/1950-good-probabilityandtheweighingofevidence.pdf#page=96), [I. J. Good ⁠](https://en.wikipedia.org/wiki/I._J._Good):

> The gen­eral ques­tion of sig­nif­i­cance tests was raised in 7.3 and a sim­ple ex­am­ple will now be con­sid­ered. Sup­pose that a die is thrown _n_ times and that it shows an _r_ -face on _m r_ oc­ca­sions (_r_ = 1, 2, …, 6). The ques­tion is whether the die is loaded. The an­swer de­pends on the mean­ing of “loaded”. From one point of view, it is un­nec­es­sary to look at the sta­tis­tics since it is ob­vi­ous that no die could be ab­solutely sym­met­ri­cal. [It would be no con­tra­dic­tion of 4.3 (2) to say that the hy­poth­e­sis that the die is ab­solutely sym­met­ri­cal is al­most im­pos­si­ble. In fact, this hy­poth­e­sis is an ide­alised propo­si­tion rather than an em­pir­i­cal one.] It is pos­si­ble that a sim­i­lar re­mark ap­plies to all ex­per­i­ments—even to the ESP ex­per­i­ment, since there may be no way of de­sign­ing it so that the prob­a­bil­i­ties are _ex­actly_ equal to 1⁄2.

## Hodges & Lehmann1954

[“Test­ing the ap­prox­i­mate va­lid­ity of sta­tis­ti­cal hy­pothe­ses” ⁠](https://gwern.net/doc/statistics/decision/1954-hodges.pdf), Hodges & Lehmann 1954:

> When test­ing sta­tis­ti­cal hy­pothe­ses, we usu­ally do not wish to take the ac­tion of re­jec­tion un­less the hy­poth­e­sis being tested is false to an ex­tent suf­fi­cient to mat­ter. For ex­am­ple, we may for­mu­late the hy­poth­e­sis that a pop­u­la­tion is nor­mally dis­trib­uted, but we re­al­ize that no nat­ural pop­u­la­tion is ever ex­actly nor­mal. We would want to re­ject nor­mal­ity only if the de­par­ture of the ac­tual dis­tri­b­u­tion from the nor­mal form were great enough to be ma­te­r­ial for our in­ves­ti­ga­tion. Again, when we for­mu­late the hy­poth­e­sis that the sex ratio is the same in two pop­u­la­tions, we do not re­ally be­lieve that it could be ex­actly the same, and would only wish to re­ject equal­ity if they are suf­fi­ciently dif­fer­ent. Fur­ther ex­am­ples of the phe­nom­e­non will occur to the reader.

## Savage1954

[_⁠The Foun­da­tions of Sta­tis­tics_ 1 st edi­tion ⁠](https://archive.org/details/in.ernet.dli.2015.223806/page/n267), [Leonard Jim­mie ⁠](https://en.wikipedia.org/wiki/Leonard_Jimmie_Savage) Sav­age 1954 ⁠ [6](#fn:6), pg252–255:

> The de­vel­op­ment of the the­ory of test­ing has been much in­flu­enced by the spe­cial prob­lem of sim­ple di­chotomy, that is, test­ing prob­lems in which _H_ 0 and _H_ 1 have ex­actly one el­e­ment each. Sim­ple di­chotomy is sus­cep­ti­ble of neat and full analy­sis (as in Ex­er­cise 7.5.2 and in §14.4), likelihood- ratio tests here being the only ad­mis­si­ble tests; and sim­ple di­chotomy often gives in­sight into more com­pli­cated prob­lems, though the point is not ex­plic­itly il­lus­trated in this book.
> 
> Coin and ball ex­am­ples of sim­ple di­chotomy are easy to con­struct, but in­stances seem rare in real life. The as­tro­nom­i­cal ob­ser­va­tions made to dis­tin­guish be­tween the New­ton­ian and Ein­stein­ian hy­pothe­ses are a good, but not per­fect, ex­am­ple, and I sup­pose that re­search in Mendelian ge­net­ics some­times leads to oth­ers. There is, how­ever, a tra­di­tion of ap­ply­ing the con­cept of sim­ple di­chotomy to some sit­u­a­tions to which it is, to say the best, only crudely adapted. Con­sider, for ex­am­ple, the de­ci­sion prob­lem of a per­son who must buy, **f** 0, or refuse to buy, **f** 1, a lot of man­u­fac­tured ar­ti­cles on the basis of an ob­ser­va­tion _x_. Sup­pose that _i_ is the dif­fer­ence be­tween the value of the lot to the per­son and the price at which the lot is of­fered for sale, and that _P(x | i)_ is known to the per­son. Clearly, _H_ 0, _H_ 1, and _N_ are sets char­ac­ter­ized re­spec­tively by _i_ > 0, _i_ < 0, _i_ = 0. This analy­sis of this, and sim­i­lar, prob­lems has re­cently been ex­plored in terms of the min­i­max rule, for ex­am­ple by Sprowls [S16] and a lit­tle more fully by Rudy [R4], and by Allen [A3]. It seems to me nat­ural and promis­ing for many fields of ap­pli­ca­tion, but it is not a tra­di­tional analy­sis. On the con­trary, much lit­er­a­ture rec­om­mends, in ef­fect, that the per­son pre­tend that only two val­ues of _i_, _i_ 0 > 0 and _i_ 1 < 0, are pos­si­ble and that the per­son then choose a test for the re­sult­ing sim­ple di­chotomy. The se­lec­tion of the two val­ues _i_ 0 and _i_ 1 is left to the per­son, though they are some­times sup­posed to cor­re­spond to the per­son’s judg­ment of what con­sti­tutes good qual­ity and poor qual­ity—terms re­ally quite with­out de­f­i­n­i­tion. The em­pha­sis on sim­ple di­chotomy is tem­pered in some acceptance- sampling lit­er­a­ture, where it is rec­om­mended that the per­son choose among avail­able tests by some largely un­spec­i­fied over­all con­sid­er­a­tion of op­er­at­ing char­ac­ter­is­tics and costs, and that he fa­cil­i­tate his sur­vey of the avail­able tests by fo­cus­ing on a pair of points that hap­pen to in­ter­est him and con­sid­er­ing the test whose op­er­at­ing char­ac­ter­is­tic passes (eco­nom­i­cally, in the case of se­quen­tial test­ing) through the pair of points. These tra­di­tional analy­ses are cer­tainly in­fe­rior in the the­o­ret­i­cal frame­work of the present dis­cus­sion, and I think they will be found in­fe­rior in prac­tice.
> 
> …I turn now to a dif­fer­ent and, at least for me, del­i­cate topic in con­nec­tion with ap­pli­ca­tions of the the­ory of test­ing. Much at­ten­tion is given in the lit­er­a­ture of sta­tis­tics to what pur­port to be tests of hy­pothe­ses, in which the null hy­poth­e­sis is such that it would not re­ally be ac­cepted by any­one. The fol­low­ing 3 propo­si­tions, though play­ful in con­tent, are typ­i­cal in form of these _ex­treme_ null hy­pothe­ses, as I shall call them for the mo­ment.
> 
> -   A. The mean noise out­put of the ce­real Krakl is a lin­ear func­tion of the at­mos­pheric pres­sure, in the range 900–1,100 mil­libars.
>     
> -   B. The basal meta­bolic con­sump­tion of sperm whales is nor­mally dis­trib­uted [Wll].
>     
> -   C. New York taxi dri­vers of Irish, Jew­ish, and Scan­di­na­vian ex­trac­tion are equally pro­fi­cient in abu­sive lan­guage.
>     
> 
> Lit­er­ally to test such hy­pothe­ses as these is pre­pos­ter­ous. If, for ex­am­ple, the loss as­so­ci­ated with **f** 1 is zero, ex­cept in case Hy­poth­e­sis A is ex­actly sat­is­fied, what pos­si­ble ex­pe­ri­ence with Krakl could dis­suade you from adopt­ing **f** 1?
> 
> The un­ac­cept­abil­ity of ex­treme null hy­pothe­ses is per­fectly well known; it is closely re­lated to the often heard maxim that sci­ence dis­proves, but never proves, hy­pothe­ses. The role of ex­treme hy­pothe­ses in sci­ence and other sta­tis­ti­cal ac­tiv­i­ties seems to be im­por­tant but ob­scure. In par­tic­u­lar, though I, like every­one who prac­tices sta­tis­tics, have often “tested” ex­treme hy­pothe­ses, I can­not give a very sat­is­fac­tory analy­sis of the process, nor say clearly how it is re­lated to test­ing as de­fined in this chap­ter and other the­o­ret­i­cal dis­cus­sions. None the less, it seems worth while to ex­plore the sub­ject ten­ta­tively; I will do so largely in terms of two ex­am­ples.
> 
> Con­sider first the prob­lem of a ce­real dy­nam­i­cist who must es­ti­mate the noise out­put of Krakl at each of 10 at­mos­pheric pres­sures be­tween 900 and 1,100 mil­libars. It may well be that he can prop­erly re­gard the prob­lem as that of es­ti­mat­ing the 10 pa­ra­me­ters in ques­tion, in which case there is no ques­tion of test­ing. But sup­pose, for ex­am­ple, that one or both of the fol­low­ing con­sid­er­a­tions apply. First, the en­gi­neer and his col­leagues may at­tach con­sid­er­able per­sonal prob­a­bil­ity to the pos­si­bil­ity that A is very nearly sat­is­fied—very nearly, that is, in terms of the dis­per­sion of his mea­sure­ments. Sec­ond, the ad­min­is­tra­tive, com­pu­ta­tional, and other in­ci­den­tal costs of using 10 in­di­vid­ual es­ti­mates might be con­sid­er­ably greater than that of using a lin­ear for­mula.
> 
> It might be im­prac­ti­cal to deal with ei­ther of these con­sid­er­a­tions very rig­or­ously. One rough at­tack is for the en­gi­neer first to ex­am­ine the ob­served data _x_ and then to pro­ceed ei­ther as though he ac­tu­ally be­lieved Hy­poth­e­sis A or else in some other way. The other way might be to make the es­ti­mate ac­cord­ing to the ob­jec­tivis­tic for­mu­lae that would have been used had there been no com­pli­cat­ing con­sid­er­a­tions, or it might take into ac­count dif­fer­ent but re­lated com­pli­cat­ing con­sid­er­a­tions not ex­plic­itly men­tioned here, such as the ad­van­tage of using a qua­dratic ap­prox­i­ma­tion. It is ar­ti­fi­cial and in­ad­e­quate to re­gard this de­ci­sion be­tween one class of basic acts or an­other as a test, but that is what in cur­rent prac­tice we seem to do. The choice of which test to adopt in such a con­text is at least partly mo­ti­vated by the vague idea that the test should read­ily ac­cept, that is, re­sult in act­ing as though the ex­treme null hy­pothe­ses were true, in the far­fetched case that the null hy­poth­e­sis is in­deed true, and that the worse the ap­prox­i­ma­tion of the null hy­pothe­ses to the truth the less prob­a­ble should be the ac­cep­tance.
> 
> The method just out­lined is crude, to say the best. It is often mod­i­fied in ac­cor­dance with com­mon sense, es­pe­cially so far as the sec­ond con­sid­er­a­tion is con­cerned. Thus, if the mea­sure­ments are suf­fi­ciently pre­cise, no or­di­nary test might ac­cept the null hy­pothe­ses, for the ex­per­i­ment will lead to a clear and sure idea of just what the de­par­tures from the null hy­pothe­ses ac­tu­ally are. But, if the en­gi­neer con­sid­ers those de­par­tures unim­por­tant for the con­text at hand, he will jus­ti­fi­ably de­cide to ne­glect them.
> 
> Re­jec­tion of an ex­treme null hy­poth­e­sis, in the sense of the fore­go­ing dis­cus­sion, typ­i­cally gives rise to a com­pli­cated sub­sidiary de­ci­sion prob­lem. Some as­pects of this sit­u­a­tion have re­cently been ex­plored, for ex­am­ple by Paul­son [P3], [P4]; Dun­can [Dll], [D12]; [Tukey ⁠](https://en.wikipedia.org/wiki/John_Tukey) [T4], [[T5 ⁠](https://arxiv.org/abs/1910.10683#google)]; Schefte [S7]; and W. D. Fisher [F7].

## Fisher1956

[_⁠Sta­tis­ti­cal Meth­ods and Sci­en­tific In­fer­ence_ ⁠](https://archive.org/details/in.ernet.dli.2015.134555/page/n47), [R. A.⁠](https://en.wikipedia.org/wiki/Ronald_Fisher) Fisher 1956 (pg42):

> …How­ever, the cal­cu­la­tion [of error rates of ‘re­ject­ing the null’] is ab­surdly aca­d­e­mic, for in fact no sci­en­tific worker has a fixed level of sig­nif­i­cance at which from year to year and in all cir­cum­stances, he re­jects hy­pothe­ses; he rather gives his mind to each par­tic­u­lar case in the light of his ev­i­dence and his ideas. Fur­ther, the cal­cu­la­tion is based solely on a hy­poth­e­sis, which, in the light of the ev­i­dence, is often not be­lieved to be true at all, so that the ac­tual prob­a­bil­ity of er­ro­neous de­ci­sion, sup­pos­ing such a phrase to have any mean­ing, may be much less than the fre­quency spec­i­fy­ing the level of sig­nif­i­cance.

## Wallis & Roberts1956

[_⁠Sta­tis­tics: A New Ap­proach_ ⁠](https://archive.org/details/in.ernet.dli.2015.214331/page/n421), Wal­lis & Roberts 1956 (pg384–388):

> A dif­fi­culty with this view­point is that it is often known that the hy­poth­e­sis tested could not be pre­cisely true. No coin, for ex­am­ple, has a prob­a­bil­ity of pre­cisely 1⁄2 of com­ing heads. The true prob­a­bil­ity will al­ways dif­fer from 1⁄2 even if it dif­fers by only 0.000,000,000,1. Nei­ther will any treat­ment cure _pre­cisely_ one- third of the pa­tients in the pop­u­la­tion to which it might be ap­plied, nor will the pro­por­tion of vot­ers in a pres­i­den­tial elec­tion fa­vor­ing one can­di­date be _pre­cisely_ 1⁄2. Recog­ni­tion of this leads to the no­tion of dif­fer­ences that are or are not of prac­ti­cal im­por­tance. “Prac­ti­cal im­por­tance” de­pends on the ac­tions that are going to be taken on the basis of the data, and on the losses from tak­ing cer­tain ac­tions when oth­ers would be more ap­pro­pri­ate.
> 
> Thus, the focus is shifted to de­ci­sions: Would the same de­ci­sion about prac­ti­cal ac­tion be ap­pro­pri­ate if the coin pro­duces heads 0.500,000,000,1 of the time as if it pro­duces heads 0.5 of the time pre­cisely? Does it mat­ter whether the coin pro­duces heads 0.5 of the time or 0.6 of the time, and if so does it mat­ter enough to be worth the cost of the data needed to de­cide be­tween the ac­tions ap­pro­pri­ate to these sit­u­a­tions? Ques­tions such as these carry us to­ward a com­pre­hen­sive the­ory of ra­tio­nal ac­tion, in which the con­se­quences of each pos­si­ble ac­tion are weighed in the light of each pos­si­ble state of re­al­ity. The value of a cor­rect de­ci­sion, or the costs of var­i­ous de­grees of error, are then bal­anced against the costs of re­duc­ing the risks of error by col­lect­ing fur­ther data. It is this view­point that un­der­lies the de­f­i­n­i­tion of sta­tis­tics given in the first sen­tence of this book. [“Sta­tis­tics is a body of meth­ods for mak­ing wise de­ci­sions in the face of un­cer­tainty.”]

## Savage1957

[“Non­para­met­ric sta­tis­tics” ⁠](https://gwern.net/doc/statistics/decision/1957-savage.pdf), [I. Richard Sav­age](https://projecteuclid.org/journals/statistical-science/volume-14/issue-1/A-conversation-with-I-Richard-Savage-with-the-assistance-of/10.1214/ss/1009211808.full) ⁠ [7](#fn:7) 1957 68ya:

> Siegel does not ex­plain why his in­ter­est is con­fined to tests of sig­nif­i­cance; to make mea­sure­ments and then ig­nore their mag­ni­tudes would or­di­nar­ily be point­less. Ex­clu­sive re­liance on tests of sig­nif­i­cance ob­scures the fact that statistical- significance does not imply sub­stan­tive sig­nif­i­cance. The tests given by Siegel apply only to null hy­pothe­ses of “no dif­fer­ence.” In re­search, how­ever, null hy­pothe­ses of the form “Pop­u­la­tion A has a me­dian at least 5 units _larger_ than the me­dian of Pop­u­la­tion B” arise. Null hy­pothe­ses of no dif­fer­ence are usu­ally known to be false be­fore the data are col­lected [[⁠9 ⁠](https://gwern.net/everything#fisher-1956), p. 42; [⁠48 ⁠](https://gwern.net/everything#wallis-roberts-1956), pp. 384–8]; when they are, their re­jec­tion or ac­cep­tance sim­ply re­flects the size of the sam­ple and the power of the test, and is not a con­tri­bu­tion to sci­ence.

## Nunnally1960

[“The place of sta­tis­tics in psy­chol­ogy” ⁠](https://gwern.net/doc/statistics/causality/1960-nunnally.pdf), Nun­nally 1960:

> The most mis­used and mis­con­ceived hypothesis- testing model em­ployed in psy­chol­ogy is re­ferred to as the “null- hypothesis” model. Stat­ing it crudely, one null hy­poth­e­sis would be that two treat­ments do not pro­duce dif­fer­ent mean ef­fects in the long run. Using the ob­tained means and sam­ple es­ti­mates of”pop­u­la­tion” vari­ances, prob­a­bil­ity state­ments can be made about the ac­cep­tance or re­jec­tion of the null hy­poth­e­sis. Sim­i­lar null hy­pothe­ses are ap­plied to cor­re­la­tions, com­plex ex­per­i­men­tal de­signs, factor- analytic re­sults, and most all ex­per­i­men­tal re­sults.
> 
> Al­though from a math­e­mat­i­cal point of view the null- hypothesis mod­els are in­ter­nally neat, they share a crip­pling flaw: in the real world the null hy­poth­e­sis is al­most never true, and it is usu­ally non­sen­si­cal to per­form an ex­per­i­ment with the _sole_ aim of re­ject­ing the null hy­poth­e­sis. This is a per­sonal point of view, and it can­not be proved di­rectly. How­ever, it is sup­ported both by com­mon sense and by prac­ti­cal ex­pe­ri­ence. The common- sense ar­gu­ment is that dif­fer­ent psy­cho­log­i­cal treat­ments will al­most al­ways (in the long run) pro­duce dif­fer­ences in mean ef­fects, even though the dif­fer­ences may be very small. Also, just as na­ture ab­hors a vac­uum, it prob­a­bly ab­hors zero cor­re­la­tions be­tween vari­ables.
> 
> …Ex­pe­ri­ence shows that when large num­bers of sub­jects are used in stud­ies, nearly all com­par­isons of means are “sig­nif­i­cantly” dif­fer­ent and all cor­re­la­tions are “sig­nif­i­cantly” dif­fer­ent from zero. The au­thor once had oc­ca­sion to use 700 sub­jects in a study of pub­lic opin­ion. After a [fac­tor analy­sis ⁠](https://en.wikipedia.org/wiki/Factor_analysis) of the re­sults, the fac­tors were cor­re­lated with individual- difference vari­ables such as amount of ed­u­ca­tion, age, in­come, sex, and oth­ers. In look­ing at the re­sults I was happy to find so many “sig­nif­i­cant” cor­re­la­tions (under the null- hypothesis model)- indeed, nearly all cor­re­la­tions were sig­nif­i­cant, in­clud­ing ones that made lit­tle sense. Of course, with an _N_ of 700 cor­re­la­tions as large as 0.08 are “be­yond the 0.05 level.” Many of the “sig­nif­i­cant” cor­re­la­tions were of no the­o­ret­i­cal or prac­ti­cal im­por­tance.
> 
> The point of view taken here is that if the null hy­poth­e­sis is not re­jected, it usu­ally is be­cause the _N_ is too small. If enough data is gath­ered, the hy­poth­e­sis will gen­er­ally be re­jected. If re­jec­tion of the null hy­poth­e­sis were the real in­ten­tion in psy­cho­log­i­cal ex­per­i­ments, there usu­ally would be no need to gather data.
> 
> …Sta­tis­ti­cians are not to blame for the mis­con­cep­tions in psy­chol­ogy about the use of sta­tis­ti­cal meth­ods. They have warned us about the use of the hypothesis- testing mod­els and the re­lated con­cepts. In par­tic­u­lar they have crit­i­cized the null- hypothesis model and have rec­om­mended al­ter­na­tive pro­ce­dures sim­i­lar to those rec­om­mended here (See [⁠Sav­age, 1957 ⁠](https://gwern.net/everything#savage-1957); [Tukey, 1954 ⁠](https://gwern.net/doc/statistics/decision/1954-tukey.pdf); and [Yates, 1951 ⁠](https://gwern.net/doc/statistics/causality/1951-yates.pdf)).

## Smith1960

[“Re­view of N. T. J. Bai­ley, _Sta­tis­ti­cal meth­ods in bi­ol­ogy_ ” ⁠](https://gwern.net/doc/statistics/causality/1960-smith.pdf), Smith 1960:

> How­ever, it is in­ter­est­ing to look at this book from an­other angle. Here we have set be­fore us with great clar­ity a panorama of mod­ern sta­tis­ti­cal meth­ods, as used in bi­ol­ogy, med­i­cine, phys­i­cal sci­ence, so­cial and men­tal sci­ence, and in­dus­try. How far does this show that these meth­ods ful­fil their aims of analysing the data re­li­ably, and how many gaps are there still in our knowl­edge?…One fea­ture which can puz­zle an out­sider, and which re­quires much more jus­ti­fi­ca­tion than is usu­ally given, is the set­ting up of un­plau­si­ble null hy­pothe­ses. For ex­am­ple, a sta­tis­ti­cian may set out a test to see whether two drugs have ex­actly the same ef­fect, or whether a re­gres­sion line is ex­actly straight. These hy­pothe­ses can scarcely be taken lit­er­ally, but a sta­tis­ti­cian may say, quite rea­son­ably, that he wishes to test whether there is an ap­pre­cia­ble dif­fer­ence be­tween the ef­fects of the two drugs, or an ap­pre­cia­ble cur­va­ture in the re­gres­sion line. But this raises at once the ques­tion: how large is ‘ap­pre­cia­ble’? Or in other words, are we not re­ally con­cerned with some kind of es­ti­ma­tion, rather than sig­nif­i­cance?

## Edwards1963

[“Bayesian sta­tis­ti­cal in­fer­ence for psy­cho­log­i­cal re­search” ⁠](https://pdfs.semanticscholar.org/f5ee/44c98c68fc1d9a6a7a10d3af3ba13fc9d058.pdf), Ed­wards et al 1963:

> The most pop­u­lar no­tion of a test is, roughly, a ten­ta­tive de­ci­sion be­tween two hy­pothe­ses on the basis of data, and this is the no­tion that will dom­i­nate the present treat­ment of tests. Some qual­i­fi­ca­tion is needed if only be­cause, in typ­i­cal ap­pli­ca­tions, one of the hy­pothe­ses—the null hy­poth­e­sis—is known by all con­cerned to be false from the out­set ([⁠Berk­son 1938 ⁠](https://gwern.net/everything#berkson-1938); [⁠Hodges & Lehmann 1954 ⁠](https://gwern.net/everything#hodges-lehmann-1954); [Lehmann 1959 ⁠](https://gwern.net/doc/statistics/decision/1959-lehmann-testingstatisticalhypotheses.pdf) ⁠ [8](#fn:8); [⁠I. R. Sav­age 1957 ⁠](https://gwern.net/everything#savage-1957); L. [⁠J. Sav­age 1954, p. 254 ⁠](https://gwern.net/everything#savage-1954)); some ways of re­solv­ing the seem­ing ab­sur­dity will later be pointed out, and at least one of them will be im­por­tant for us here…Clas­si­cal pro­ce­dures some­times test null hy­pothe­ses that no one would be­lieve for a mo­ment, no mat­ter what the data; our list of sit­u­a­tions that might stim­u­late hy­poth­e­sis tests ear­lier in the sec­tion in­cluded sev­eral ex­am­ples. Test­ing an un­be­liev­able null hy­poth­e­sis amounts, in prac­tice, to as­sign­ing an un­rea­son­ably large prior prob­a­bil­ity to a very small re­gion of pos­si­ble val­ues of the true pa­ra­me­ter. In such cases, the more the pro­ce­dure is against the null hy­poth­e­sis, the bet­ter. The fre­quent re­luc­tance of em­pir­i­cal sci­en­tists to ac­cept null hy­pothe­ses which their data do not clas­si­cally re­ject sug­gests their ap­pro­pri­ate skep­ti­cism about the orig­i­nal plau­si­bil­ity of these null hy­pothe­ses.

## Bakan1966

[“The test of sig­nif­i­cance in psy­cho­log­i­cal re­search” ⁠](http://stats.org.uk/statistical-inference/Bakan1966.pdf), Bakan 1966:

> Let us con­sider some of the dif­fi­cul­ties as­so­ci­ated with the null hy­poth­e­sis.
> 
> 1.  The _a pri­ori_ rea­sons for be­liev­ing that the null hy­poth­e­sis is gen­er­ally false any­way. One of the com­mon ex­pe­ri­ences of re­search work­ers is the very high fre­quency with which sig­nif­i­cant re­sults are ob­tained with large sam­ples. Some years ago, the au­thor had oc­ca­sion to run a num­ber of tests of sig­nif­i­cance on a bat­tery of tests col­lected on about 60,000 sub­jects from all over the United States. Every test came out sig­nif­i­cant. Di­vid­ing the cards by such ar­bi­trary cri­te­ria as east ver­sus west of the Mis­sis­sippi River, Maine ver­sus the rest of the coun­try, North ver­sus South, etc., all pro­duced sig­nif­i­cant dif­fer­ences in means. In some in­stances, the dif­fer­ences in the sam­ple means were quite small, but nonethe­less, the _p_ val­ues were all very low. [⁠Nun­nally 1960 ⁠](https://gwern.net/everything#nunnally-1960) has re­ported a sim­i­lar ex­pe­ri­ence in­volv­ing cor­re­la­tion co­ef­fi­cients on 700 sub­jects. [⁠Joseph Berk­son 1938 ⁠](https://gwern.net/everything#berkson-1938) made the ob­ser­va­tion al­most 30 years in con­nec­tion with chi- square:
>     
> 
> > I be­lieve that an ob­ser­vant sta­tis­ti­cian who has had any con­sid­er­able ex­pe­ri­ence with ap­ply­ing the chi- square test re­peat­edly will agree with my state­ment that, as a mat­ter of ob­ser­va­tion, when the num­bers in the data are quite large, the P’s tend to come out small. Hav­ing ob­served this, and on re­flec­tion, I make the fol­low­ing dog­matic state­ment, re­fer­ring for il­lus­tra­tion to the nor­mal curve: “If the nor­mal curve is fit­ted to a body of data rep­re­sent­ing any real ob­ser­va­tions what­ever of quan­ti­ties in the phys­i­cal world, then if the num­ber of ob­ser­va­tions is ex­tremely large—for in­stance, on an order of 200,000—the chi- square _p_ will be small be­yond any usual limit of sig­nif­i­cance.”
> > 
> > This dog­matic state­ment is made on the basis of an ex­trap­o­la­tion of the ob­ser­va­tion re­ferred to and can also be de­fended as a pre­dic­tion from _a pri­ori_ con­sid­er­a­tions. For we may as­sume that it is prac­ti­cally cer­tain that any se­ries of real ob­ser­va­tions does not ac­tu­ally fol­low a nor­mal curve _with ab­solute ex­ac­ti­tude_ in all re­spects, and no mat­ter how small the dis­crep­ancy be­tween the nor­mal curve and the true curve of ob­ser­va­tions, the chi- square _P_ will be small if the sam­ple has a suf­fi­ciently large num­ber of ob­ser­va­tions in it.
> > 
> > If this be so, then we have some­thing here that is apt to trou­ble the con­science of a re­flec­tive sta­tis­ti­cian using the chi- square test. For I sup­pose it would be agreed by sta­tis­ti­cians that a large sam­ple is al­ways bet­ter than a small sam­ple. If, then, we know in ad­vance the _P_ that will re­sult from an ap­pli­ca­tion of a chi- square test to a large sam­ple, there would seem to be no use in doing it on a smaller one. But since the re­sult of the for­mer test is known, it is no test at all [pp. 526–527].
> 
> As one group of au­thors has put it, “in typ­i­cal ap­pli­ca­tions... the null hy­poth­e­sis... is known by all con­cerned to be false from the out­set [[⁠Ed­wards et al 1963 ⁠](https://gwern.net/everything#edwards-1963), p. 214].” The fact of the mat­ter is that _there is re­ally no good rea­son to ex­pect the null hy­poth­e­sis to be true in any pop­u­la­tion._ Why should the mean, say, of all scores east of the Mis­sis­sippi be _iden­ti­cal_ to all scores west of the Mis­sis­sippi? Why should any cor­re­la­tion co­ef­fi­cient be _ex­actly_ 0.00 in the pop­u­la­tion? Why should we ex­pect the ratio of males to fe­males be _ex­actly_ 50:50 in any pop­u­la­tion? Or why should dif­fer­ent drugs have _ex­actly_ the same ef­fect on any pop­u­la­tion pa­ra­me­ter ([⁠Smith 1960 ⁠](https://gwern.net/everything#smith-1960))? _A glance at any set of sta­tis­tics on total pop­u­la­tions will quickly con­firm the rar­ity of the null hy­poth­e­sis in na­ture._
> 
> …Should there be any de­vi­a­tion from the null hy­poth­e­sis in the pop­u­la­tion, _no mat­ter how small_ —and we have lit­tle doubt but that such a de­vi­a­tion usu­ally ex­ists—a suf­fi­ciently large num­ber of ob­ser­va­tions will lead to the re­jec­tion of the null hy­poth­e­sis. As Nun­nally 1960 put it,
> 
> > if the null hy­poth­e­sis is not re­jected, it is usu­ally be­cause the _N_ is too small. If enough data are gath­ered, the hy­poth­e­sis will gen­er­ally be re­jected. If re­jec­tion of the null hy­poth­e­sis were the real in­ten­tion in psy­cho­log­i­cal ex­per­i­ments, there usu­ally would be no need to gather data [p. 643].

## Meehl1967

[“Theory- testing in psy­chol­ogy and physics: A method­olog­i­cal para­dox” ⁠](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.693.8918&rep=rep1&type=pdf), Meehl 1967

> One rea­son why the di­rec­tional null hy­poth­e­sis (_H_ 02: μ g ≤ μ b) is the ap­pro­pri­ate can­di­date for ex­per­i­men­tal refu­ta­tion is the uni­ver­sal agree­ment that the old point- null hy­poth­e­sis (_H_ 0: μ g = μ b) is [quasi- ] al­ways false in bi­o­log­i­cal and so­cial sci­ence. Any de­pen­dent vari­able of in­ter­est, such as I.Q., or aca­d­e­mic achieve­ment, or per­cep­tual speed, or emo­tional re­ac­tiv­ity as mea­sured by skin re­sis­tance, or what­ever, de­pends mainly upon a fi­nite num­ber of “strong” vari­ables char­ac­ter­is­tic of the or­gan­isms stud­ied (em­body­ing the ac­cu­mu­lated re­sults of their ge­netic makeup and their learn­ing his­to­ries) plus the in­flu­ences ma­nip­u­lated by the ex­per­i­menter. Upon some com­pli­cated, un­known math­e­mat­i­cal func­tion of this fi­nite list of “im­por­tant” de­ter­min­ers is then su­per­im­posed an in­def­i­nitely large num­ber of es­sen­tially “ran­dom” fac­tors which con­tribute to the in­tra­group vari­a­tion and there­fore boost the error term of the statistical- significance test. In order for two groups which dif­fer in some iden­ti­fied prop­er­ties (such as so­cial class, in­tel­li­gence, di­ag­no­sis, racial or re­li­gious back­ground) to dif­fer not at all in the “out­put” vari­able of in­ter­est, it would be nec­es­sary that all de­ter­min­ers of the out­put vari­able have pre­cisely the same av­er­age val­ues in both groups, or else that their val­ues should dif­fer by a _pat­tern of amounts of dif­fer­ence_ which pre­cisely coun­ter­bal­ance one an­other to yield a net dif­fer­ence of zero. Now our gen­eral back­ground knowl­edge in the so­cial sci­ences, or, for that mat­ter, even “com­mon sense” con­sid­er­a­tions, makes such an exact equal­ity of all de­ter­min­ing vari­ables, or a pre­cise “ac­ci­den­tal” coun­ter­bal­anc­ing of them, so ex­tremely un­likely that no psy­chol­o­gist or sta­tis­ti­cian would as­sign more than a neg­li­gi­bly small prob­a­bil­ity to such a state of af­fairs.
> 
> … _Ex­am­ple_: Sup­pose we are study­ing a sim­ple perceptual- verbal task like rate of color- naming in school chil­dren, and the in­de­pen­dent vari­able is fa­ther’s re­li­gious pref­er­ence. Su­per­fi­cial con­sid­er­a­tion might sug­gest that these two vari­ables would not be re­lated, but a lit­tle thought leads one to con­clude that they will al­most cer­tainly be re­lated by _some_ amount, how­ever small. Con­sider, for in­stance, that a child’s re­ac­tion to any sort of school- context task will be to some ex­tent de­pen­dent upon his so­cial class, since the de­sire to please aca­d­e­mic per­son­nel and the de­sire to achieve at a per­for­mance (just be­cause it is a _task_, re­gard­less of its in­trin­sic in­ter­est) are both re­lated to the kinds of sub- cultural and per­son­al­ity traits in the par­ents that lead to up­ward mo­bil­ity, eco­nomic suc­cess, the gain­ing of fur­ther ed­u­ca­tion, and the like. Again, since there is known to be a sex dif­fer­ence in color nam­ing, it is likely that fa­thers who have en­tered oc­cu­pa­tions more at­trac­tive to “fem­i­nine” males will (on the av­er­age) pro­vide a some­what more fem­i­nine fa­ther fig­ure for iden­ti­fi­ca­tion on the part of their male off­spring, and that a more re­fined color vo­cab­u­lary, mak­ing closer dis­crim­i­na­tions be­tween sim­i­lar hues, will be char­ac­ter­is­tic of the or­di­nary lan­guage of such a house­hold. Fur­ther, it is known that there is a cor­re­la­tion be­tween a child’s gen­eral in­tel­li­gence and its fa­ther’s oc­cu­pa­tion, and of course there will be _some_ re­la­tion, even though it may be small, be­tween a child’s gen­eral in­tel­li­gence and his color vo­cab­u­lary, aris­ing from the fact that _vo­cab­u­lary in gen­eral_ is heav­ily sat­u­rated with the gen­eral in­tel­li­gence fac­tor. Since re­li­gious pref­er­ence is a cor­re­late of so­cial class, all of these so­cial class fac­tors, as well as the in­tel­li­gence vari­able, would tend to in­flu­ence color- naming per­for­mance. Or con­sider a more ex­treme and faint kind of re­la­tion­ship. It is quite con­ceiv­able that a child who be­longs to a more litur­gi­cal re­li­gious de­nom­i­na­tion would be some­what more color- oriented than a child for whom bright col­ors were not as­so­ci­ated with the re­li­gious life. Every­one fa­mil­iar with psy­cho­log­i­cal re­search knows that nu­mer­ous “puz­zling, un­ex­pected” cor­re­la­tions pop up all the time, and that it re­quires only a mod­er­ate amount of motivation- plus-ingenuity to con­struct very plau­si­ble al­ter­na­tive the­o­ret­i­cal ex­pla­na­tions for them.
> 
> …These arm­chair con­sid­er­a­tions are borne out by the find­ing that in psy­cho­log­i­cal and so­ci­o­log­i­cal in­ves­ti­ga­tions in­volv­ing very large num­bers of sub­jects, it is reg­u­larly found that al­most all cor­re­la­tions or dif­fer­ences be­tween means are statistically- significant. See, for ex­am­ple, the pa­pers by [⁠Bakan 1966 ⁠](https://gwern.net/everything#bakan-1966) and [⁠Nun­nally 1960 ⁠](https://gwern.net/everything#nunnally-1960). Data cur­rently being an­a­lyzed by Dr. David Lykken and my­self⁠ [9](#fn:9), de­rived from a huge sam­ple of over 55,000 Min­nesota high school se­niors, re­veal statistically- significant re­la­tion­ships in 91% of pair­wise as­so­ci­a­tions among a con­geries of 45 mis­cel­la­neous vari­ables such as sex, birth order, re­li­gious pref­er­ence, num­ber of sib­lings, vo­ca­tional choice, club mem­ber­ship, col­lege choice, mother’s ed­u­ca­tion, danc­ing, in­ter­est in wood­work­ing, lik­ing for school, and the like. The 9% of non- statistically-significant as­so­ci­a­tions are heav­ily con­cen­trated among a small mi­nor­ity of vari­ables hav­ing du­bi­ous [re­li­a­bil­ity ⁠](https://en.wikipedia.org/wiki/Reliability_\(statistics\)), or in­volv­ing ar­bi­trary group­ings of non- homogeneous or non­mo­not­o­nic sub- categories. The ma­jor­ity of vari­ables ex­hib­ited sig­nif­i­cant re­la­tion­ships _with all but 3 of the oth­ers_, often at a very high con­fi­dence level (_p_ < 10 −6).
> 
> …Con­sid­er­ing the fact that “every­thing in the brain is con­nected with every­thing else”, and that there exist sev­eral “gen­eral state- variables” (such as arousal, at­ten­tion, anx­i­ety, and the like) which are known to be at least _slightly_ in­flu­ence­able by prac­ti­cally any kind of stim­u­lus input, it is highly un­likely that _any_ psy­cho­log­i­cally dis­crim­inable stim­u­la­tion which we apply to an ex­per­i­men­tal sub­ject would exert lit­er­ally _zero_ ef­fect upon any as­pect of his per­for­mance. The psy­cho­log­i­cal lit­er­a­ture abounds with ex­am­ples of small but de­tectable in­flu­ences of this kind. Thus it is known that if a sub­ject mem­o­rizes a list of non­sense syl­la­bles in the pres­ence of a faint odor of pep­per­mint, his re­call will be fa­cil­i­tated by the pres­ence of that odor. Or, again, we know that in­di­vid­u­als solv­ing in­tel­lec­tual prob­lems in a “messy” room do not per­form quite as well as in­di­vid­u­als work­ing in a neat, well- ordered sur­round. Again, cog­ni­tive processes un­dergo a de­tectable fa­cil­i­ta­tion when the think­ing sub­ject is con­cur­rently per­form­ing the ir­rel­e­vant, noncog­ni­tive task of squeez­ing a hand dy­namome­ter. It would re­quire con­sid­er­able in­ge­nu­ity to con­coct ex­per­i­men­tal ma­nip­u­la­tions, ex­cept the most min­i­mal and triv­ial (such as a very slight mod­i­fi­ca­tion in the word order of in­struc­tions given a sub­ject) where one could have con­fi­dence that the ma­nip­u­la­tion would be ut­terly with­out ef­fect upon the sub­ject’s mo­ti­va­tional level, at­ten­tion, arousal, fear of fail­ure, achieve­ment drive, de­sire to please the ex­per­i­menter, dis­trac­tion, so­cial fear, etc., etc. So that, for ex­am­ple, while there is no very “in­ter­est­ing” psy­cho­log­i­cal the­ory that links hunger drive with color- naming abil­ity, I my­self would con­fi­dently pre­dict a sig­nif­i­cant dif­fer­ence in color- naming abil­ity be­tween per­sons tested after a full meal and per­sons who had not eaten for 10 hours, pro­vided the sam­ple size were suf­fi­ciently large and the color- naming mea­sure­ments suf­fi­ciently re­li­able, since one of the ef­fects of the in­creased hunger drive is height­ened “arousal”, and any­thing which height­ens arousal would be ex­pected to af­fect a perceptual- cognitive per­for­mance like color- naming. Suf­fice it to say that there are very good rea­sons for ex­pect­ing at least _some_ slight in­flu­ence of al­most any ex­per­i­men­tal ma­nip­u­la­tion which would dif­fer suf­fi­ciently in its form and con­tent from the ma­nip­u­la­tion im­posed upon a con­trol group to be in­cluded in an ex­per­i­ment in the first place. In what fol­lows I shall there­fore as­sume that the point- null hy­poth­e­sis _H_ 0 is, in psy­chol­ogy, [quasi- ] al­ways false.

See also [⁠Waller 2004 ⁠](https://gwern.net/everything#waller-2004), and Meehl’s 2003 CSS talk, [“Cri­tique of Null Hy­poth­e­sis Sig­nif­i­cance Test­ing”](https://meehl.umn.edu/files/aumeehl2003sigtests-trimmedmp3#.mp3) (MP3 audio; [slides ⁠](https://meehl.umn.edu/sites/g/files/pua1696/f/aumeehl2003ccshandout.pdf)).

## Lykken1968

[“Statistical- Significance in Psy­cho­log­i­cal Re­search” ⁠](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.220.5553&rep=rep1&type=pdf), Lykken 1968:

> Most the­o­ries in the areas of per­son­al­ity, clin­i­cal, and so­cial psy­chol­ogy pre­dict no more than the di­rec­tion of a cor­re­la­tion, group dif­fer­ence, or treat­ment ef­fect. Since the null hy­poth­e­sis is never strictly true, such pre­dic­tions have about a 50-50 chance of being con­firmed by ex­per­i­ment when the the­ory in ques­tion is false, since the statistical- significance of the re­sult is a func­tion of the sam­ple size.
> 
> …Most psy­cho­log­i­cal ex­per­i­ments are of 3 kinds: (1) stud­ies of the ef­fect of some treat­ment on some out­put vari­ables, which can be re­garded as a spe­cial case of (2) stud­ies of the dif­fer­ence be­tween two or more groups of in­di­vid­u­als with re­spect to some vari­able, which in turn are a spe­cial case of (3) the study of the re­la­tion­ship or cor­re­la­tion be­tween two or more vari­ables within some spec­i­fied pop­u­la­tion. Using the bi­vari­ate cor­re­la­tion de­sign as par­a­dig­matic, then, one notes first that the strict null hy­poth­e­sis must al­ways be as­sumed to be false (this idea is not new and has re­cently been il­lu­mi­nated by [⁠Bakan 1966 ⁠](https://gwern.net/everything#bakan-1966)). Un­less one of the vari­ables is wholly un­re­li­able so that the val­ues ob­tained are strictly ran­dom, it would be fool­ish to sup­pose that the cor­re­la­tion be­tween any two vari­ables is iden­ti­cally equal to 0.0000... (or that the ef­fect of some treat­ment or the dif­fer­ence be­tween two groups is ex­actly _zero_). The molar de­pen­dent vari­ables em­ployed in psy­cho­log­i­cal re­search are ex­tremely com­pli­cated in the sense that the mea­sured value of such a vari­able tends to be af­fected by the in­ter­ac­tion of a vast num­ber of fac­tors, both in the present sit­u­a­tion and in the his­tory of the sub­ject or­gan­ism. It is ex­ceed­ingly un­likely that any two such vari­ables will not share at least some of these fac­tors and equally un­likely that their ef­fects will ex­actly can­cel one an­other out.
> 
> It might be ar­gued that the more com­plex the vari­ables the smaller their av­er­age cor­re­la­tion ought to be since a larger pool of com­mon fac­tors al­lows more chance for mu­tual can­cel­la­tion of ef­fects in obe­di­ence to the [Law of Large Num­bers ⁠](https://en.wikipedia.org/wiki/Law_of_large_numbers) ⁠ [10](#fn:10). How­ever, one knows of a num­ber of un­usu­ally po­tent and per­va­sive fac­tors which op­er­ate to un­bal­ance such con­ve­nient sym­me­tries and to pro­duce cor­re­la­tions large enough to rival the ef­fects of what­ever causal fac­tors the ex­per­i­menter may have had in mind. Thus, we know that (1) “good” psy­cho­log­i­cal and phys­i­cal vari­ables tend to be pos­i­tively cor­re­lated; (6) ex­per­i­menters, with­out de­lib­er­ate in­ten­tion, can some­how sub­tly bias their find­ings in the ex­pected di­rec­tion (Rosen­thal, 1963 62ya); (3) the ef­fects of com­mon method are often as strong as or stronger than those pro­duced by the ac­tual vari­ables of in­ter­est (eg. in a large and care­ful study of the fac­to­r­ial struc­ture of ad­just­ment to stress among of­fi­cer can­di­dates, Holtz­man & Bit­ter­man, 1956 69ya, found that their 101 orig­i­nal vari­ables con­tained 5 main com­mon fac­tors rep­re­sent­ing, re­spec­tively, their rat­ing scales, their perceptual- motor tests, the McK­in­ney Re­port­ing Test, their GSR vari­ables, and the [MMPI ⁠](https://en.wikipedia.org/wiki/Minnesota_Multiphasic_Personality_Inventory)); (4) tran­si­tory state vari­ables such as the sub­ject’s anx­i­ety level, fa­tigue, or his de­sire to please, may broadly af­fect all mea­sures ob­tained in a sin­gle ex­per­i­men­tal ses­sion. This av­er­age shared vari­ance of “un­re­lated” vari­ables can be thought of as a kind of am­bi­ent noise level char­ac­ter­is­tic of the do­main. It would be in­ter­est­ing to ob­tain em­pir­i­cal es­ti­mates of this quan­tity in our field to serve as a kind of [Plim­soll mark ⁠](https://en.wikipedia.org/wiki/Waterline) against which to com­pare ob­tained re­la­tion­ships pre­dicted by some the­ory under test. If, as I think, it is not un­rea­son­able to sup­pose that “un­re­lated” molar psy­cho­log­i­cal vari­ables share on the av­er­age about 4% to 5% of com­mon vari­ance, then the ex­pected cor­re­la­tion be­tween any such vari­ables would be about 0.20 in ab­solute value and the ex­pected dif­fer­ence be­tween any two groups on some such vari­able would be nearly 0.5 stan­dard de­vi­a­tion units. (Note that these es­ti­mates as­sume zero mea­sure­ment error. One can bet­ter ex­plain the near- zero cor­re­la­tions often ob­served in psy­cho­log­i­cal re­search in terms of un­re­li­a­bil­ity of mea­sures than in terms of the as­sump­tion that the true scores are in fact un­re­lated.)

## Nichols1968

[“Hered­ity, En­vi­ron­ment, and School Achieve­ment” ⁠](https://gwern.net/doc/iq/1968-nichols.pdf), Nichols 1968:

> There are 3 main fac­tors or types of vari­ables that seem likely to have an im­por­tant in­flu­ence on abil­ity and school achieve­ment. These are (1) the school fac­tor or or­ga­nized ed­u­ca­tional in­flu­ences; (2) the fam­ily fac­tor or all of the so­cial in­flu­ences of fam­ily life on a child; and (3) the ge­netic fac­tor…the sep­a­ra­tion of the ef­fects of the major types of in­flu­ences has proved to be ex­tra­or­di­nar­ily dif­fi­cult, and all of the re­search so far has not re­sulted in a clear- cut con­clu­sion.
> 
> …This messy sit­u­a­tion is due pri­mar­ily to the fact that in human so­ci­ety all good things tend to go to­gether. The most in­tel­li­gent par­ents—those with the best ge­netic po­ten­tial—also tend to pro­vide the most com­fort­able and in­tel­lec­tu­ally stim­u­lat­ing home en­vi­ron­ments for their chil­dren, and also tend to send their chil­dren to the most af­flu­ent and well- equipped schools. Thus, the ubiq­ui­tous cor­re­la­tion be­tween fam­ily socio- economic sta­tus and school achieve­ment is am­bigu­ous in mean­ing, and iso­lat­ing the in­de­pen­dent con­tri­bu­tion of the fac­tors in­volved is dif­fi­cult. How­ever, the strong emo­tion­ally mo­ti­vated at­ti­tudes and vested in­ter­ests in this area have also tended to in­hibit the sort of dis­pas­sion­ate, ob­jec­tive eval­u­a­tion of the avail­able ev­i­dence that is nec­es­sary for the ad­vance of sci­ence.

## Hays1973

[_Sta­tis­tics for the so­cial sci­ences_ ⁠](https://www.amazon.com/Statistics-Social-Sciences-W-Hays/dp/B000OEF3EK) (2nd edi­tion), Hays 1973; [chap­ter 10 ⁠](https://gwern.net/doc/statistics/causality/1973-hays.pdf), page 413–417:

> 10.19: Test­man­ship, or how big is a dif­fer­ence?
> 
> …As we saw in Chap­ter 4, the com­plete ab­sence of a sta­tis­ti­cal re­la­tion, or no as­so­ci­a­tion, oc­curs only when the con­di­tional dis­tri­b­u­tion of the de­pen­dent vari­able is the same re­gard­less of which treat­ment is ad­min­is­tered. Thus if the in­de­pen­dent vari­able is not as­so­ci­ated at all with the de­pen­dent vari­able the pop­u­la­tion dis­tri­b­u­tions must be iden­ti­cal over the treat­ments. If, on the other hand, the means of the dif­fer­ent treat­ment pop­u­la­tions are dif­fer­ent, the con­di­tional dis­tri­b­u­tions them­selves must be dif­fer­ent and the in­de­pen­dent and de­pen­dent vari­ables must be as­so­ci­ated. The re­jec­tion of the hy­poth­e­sis of no dif­fer­ence be­tween pop­u­la­tion means is tan­ta­mount to the as­ser­tion that the treat­ment given does have some sta­tis­ti­cal as­so­ci­a­tion with the de­pen­dent vari­able score.
> 
> …How­ever, the oc­cur­rence of a sig­nif­i­cant re­sult says noth­ing at all about the strength of the as­so­ci­a­tion be­tween treat­ment and score. A sig­nif­i­cant re­sult leads to the in­fer­ence that some as­so­ci­a­tion ex­ists, but in no sense does this mean that an im­por­tant de­gree of as­so­ci­a­tion nec­es­sar­ily ex­ists. Con­versely, ev­i­dence of a strong sta­tis­ti­cal as­so­ci­a­tion can occur in data even when the re­sults are not sig­nif­i­cant. The game of in­fer­ring the true de­gree of sta­tis­ti­cal as­so­ci­a­tion has a joker: this is the sam­ple size. The time has come to de­fine the no­tion of the strength of a sta­tis­ti­cal as­so­ci­a­tion more sharply, and to link this idea with that of the true dif­fer­ence be­tween pop­u­la­tion means.
> 
> . When does it seem ap­pro­pri­ate to say that a strong as­so­ci­a­tion ex­ists be­tween the ex­per­i­men­tal fac­tor _X_ and the de­pen­dent vari­able _Y_? Over all of the dif­fer­ent pos­si­bil­i­ties for _X_ there is a prob­a­bil­ity dis­tri­b­u­tion of _Y_ val­ues, which is the mar­ginal dis­tri­b­u­tion of _Y_ over (_x_,_y_) events. The ex­is­tence of this dis­tri­b­u­tion im­plies that we do not know ex­actly what the _Y_ value for any ob­ser­va­tion will be; we are al­ways un­cer­tain about _Y_ to some ex­tent. How­ever, given any par­tic­u­lar _X_, there is also a con­di­tional dis­tri­b­u­tion of _Y_, and it may be that in this con­di­tional dis­tri­b­u­tion the highly prob­a­ble val­ues of _Y_ tend to “shrink” within a much nar­rower range than in the mar­ginal dis­tri­b­u­tion. If so, we can say that the in­for­ma­tion about _X_ tends to re­duce un­cer­tainty about _Y_. **In gen­eral we will say that the strength of a sta­tis­ti­cal re­la­tion is re­flected by the ex­tent to which know­ing _X_ re­duces un­cer­tainty about _Y_.** One of the best in­di­ca­tors of our un­cer­tainty about the value of a vari­able is σ 2, the vari­ance of its dis­tri­b­u­tion…This index re­flects the pre­dic­tive power af­forded by a re­la­tion­ship: when _w_ 2 is zero, then _X_ does not aid us at all in pre­dict­ing the value of _Y_. On the other hand, when _w_ 2 is 1.00, this tells us that _X_ lets us know _Y_ ex­actly…About now you should be won­der­ing what the index _w_ 2 has to do with the dif­fer­ence be­tween pop­u­la­tion means.
> 
> …When the dif­fer­ence _u_ 1 - _u_ 2 is zero, then _w_ 2 must be zero. In the usual _t_ -test for a dif­fer­ence, the hy­poth­e­sis of no dif­fer­ence be­tween means is equiv­a­lent to the hy­poth­e­sis that _w_ 2 = 0. On the other hand, when there is any dif­fer­ence at all be­tween pop­u­la­tion means, the value of _w_ 2 must be greater than 0. In short, a true dif­fer­ence is “big” in the sense of pre­dic­tive power only if the square of that dif­fer­ence is large rel­a­tive to . How­ever, in sig­nif­i­cance tests such as _t_, we com­pare the dif­fer­ence we get with an es­ti­mate of σ diff. The stan­dard error of the dif­fer­ence can be made al­most as small as we choose if we are given a free choice of sam­ple size. Un­less sam­ple size is spec­i­fied, there is no _nec­es­sary_ con­nec­tion be­tween sig­nif­i­cance and the true strength of as­so­ci­a­tion.
> 
> This points up the fal­lacy of eval­u­at­ing the “good­ness” of a re­sult in terms of statistical- significance alone, with­out al­low­ing for the sam­ple size used. All sig­nif­i­cant re­sults do not imply the same de­gree of true as­so­ci­a­tion be­tween in­de­pen­dent and de­pen­dent vari­ables.
> 
> It is sad but true that re­searchers have been known to cap­i­tal­ize on this fact. There is a cer­tain amount of “test­man­ship” in­volved in using in­fer­en­tial sta­tis­tics. _Vir­tu­ally any study can be made to show sig­nif­i­cant re­sults if one uses enough sub­jects, re­gard­less of how non­sen­si­cal the con­tent may be._ There is surely noth­ing on earth that is com­pletely in­de­pen­dent of any­thing else. The strength of an as­so­ci­a­tion may ap­proach zero, but it should sel­dom or never be ex­actly zero. If one ap­plies a large enough sam­ple of the study of any re­la­tion, triv­ial or mean­ing­less as it may be, sooner or later he is al­most cer­tain to achieve a sig­nif­i­cant re­sult. Such a re­sult may be a valid find­ing, but only in the sense that one can say with as­sur­ance that some as­so­ci­a­tion is not ex­actly zero. The de­gree to which such a find­ing en­hances our knowl­edge is de­bat­able. If the cri­te­rion of strength of as­so­ci­a­tion is ap­plied to such a re­sult, it be­comes ob­vi­ous that lit­tle or noth­ing is ac­tu­ally con­tributed to our abil­ity to pre­dict one thing from an­other.
> 
> For ex­am­ple, sup­pose that two meth­ods of teach­ing first grade chil­dren to read are being com­pared. A ran­dom sam­ple of 1000 chil­dren are taught to read by method I, an­other sam­ple of 1000 chil­dren by method II. The re­sults of the in­struc­tion are eval­u­ated by a test that pro­vides a score, in whole units, for each child. Sup­pose that the re­sults turned out as fol­lows:
> 
> Then, the es­ti­mated stan­dard error of the dif­fer­ence is about 0.145, and the _z_ value is
> 
> This cer­tainly per­mits re­jec­tion of the null hy­poth­e­sis of no dif­fer­ence be­tween the groups. How­ever, does it re­ally tell us very much about what to ex­pect of an in­di­vid­ual child’s score on the test, given the in­for­ma­tion that he was taught by method I or method II? If we look at the group of chil­dren taught by method II, and as­sume that the dis­tri­b­u­tion of their scores is ap­prox­i­mately nor­mal, we find that about 45% of these chil­dren fall _below_ the mean score for chil­dren in group I. Sim­i­larly, about 45% of chil­dren in group I fall above the mean score for group II. Al­though the dif­fer­ence be­tween the two groups is sig­nif­i­cant, the two groups ac­tu­ally over­lap a great deal in terms of their per­for­mances on the test. In this sense, the two groups are re­ally not very dif­fer­ent at all, even though the dif­fer­ence be­tween the means is quite sig­nif­i­cant in a purely sta­tis­ti­cal sense.
> 
> Putting the mat­ter in a slightly dif­fer­ent way, we note that the grand mean of the two groups is 147.425. Thus, our best bet about the score of any child, not know­ing the method of his train­ing, is 147.425. If we guessed that any child drawn at ran­dom from the com­bined group should have a score above 147.425, we should be wrong about half the time. How­ever, among the orig­i­nal groups, ac­cord­ing to method I and method II, the pro­por­tions falling above and below this grand mean are ap­prox­i­mately as fol­lows:
> 
> This im­plies that if we know a child is from group I, and we guess that this score is below the grand mean, then we will be wrong about 49% of the time. Sim­i­larly, if a child is from group II, and we guess his score to be above the grand mean, we will be wrong about 49% of the time. If we are not given the group to which the child be­longs, ad we guess ei­ther above or below the grand mean, we will be wrong about 50% of the time. Know­ing the group does re­duce the prob­a­bil­ity of error in such a guess, but it does not re­duce it very much. The method by which the child was trained sim­ply doesn’t tell us a great deal about what the child’s score will be, even though the dif­fer­ence in mean scores is sig­nif­i­cant in the sta­tis­ti­cal sense.
> 
> This kind of test­man­ship flour­ishes best when peo­ple pay too much at­ten­tion to the sig­nif­i­cance test and too lit­tle to the de­gree of sta­tis­ti­cal as­so­ci­a­tion the find­ing rep­re­sents. This clut­ters up the lit­er­a­ture with find­ings that are often not worth pur­su­ing, and which serve only to ob­scure the re­ally im­por­tant pre­dic­tive re­la­tions that oc­ca­sion­ally ap­pear. The se­ri­ous sci­en­tist owes it to him­self and his read­ers to ask not only, “Is there any as­so­ci­a­tion be­tween _X_ and _Y_?” but also, “How much does my find­ing sug­gest about the power to pre­dict _Y_ from _X_?” Much too much em­pha­sis is paid to the for­mer, at the ex­pense of the lat­ter, ques­tion.

## Oakes1975

[“On the al­leged fal­sity of the null hy­poth­e­sis” ⁠](https://gwern.net/doc/statistics/causality/1975-oakes.pdf), Oakes 1975:

> Con­sid­er­a­tion is given to the con­tention by [⁠Bakan ⁠](https://gwern.net/everything#bakan-1966), [⁠Meehl ⁠](https://gwern.net/everything#meehl-1967), [⁠Nun­nally ⁠](https://gwern.net/everything#nunnally-1960), and oth­ers that the null hy­poth­e­sis in be­hav­ioral re­search is gen­er­ally false in na­ture and that the _N_ is large enough, it will al­ways be re­jected. A dis­tinc­tion is made be­tween self- selected-groups re­search de­signs and true ex­per­i­ments, and it is sug­gested that the null hy­poth­e­sis prob­a­bly is gen­er­ally false in the case of re­search in­volv­ing the for­mer de­sign, but is not in the case of re­search in­volv­ing the lat­ter. Rea­sons for the fal­sity of the null hy­poth­e­sis in the one case but not in the other are sug­gested.
> 
> The U.S. [Of­fice of Eco­nomic Op­por­tu­nity ⁠](https://en.wikipedia.org/wiki/Office_of_Economic_Opportunity) has re­cently re­ported the re­sults of re­search on per­for­mance con­tract­ing. With 23,000 _Ss_ —13,000 ex­per­i­men­tal and 10,000 con­trol—the null hy­poth­e­sis was not re­jected. The ex­per­i­men­tal _Ss_, who re­ceived spe­cial in­struc­tion in read­ing and math­e­mat­ics for 2 hours per day dur­ing the 1970–1971 54ya school year, did not dif­fer statistically- significantly from the con­trols in achieve­ment gains (Amer­i­can In­sti­tutes for Re­search 1972, pg 5). Such an in­abil­ity to re­ject the null hy­poth­e­sis might not be sur­pris­ing to the typ­i­cal class­room teacher or to most ed­u­ca­tional psy­chol­o­gists, but in view of the huge _N_ in­volved, it should give pause to [⁠Bakan 1966 ⁠](https://gwern.net/everything#bakan-1966), who con­tends that the null hy­poth­e­sis is gen­er­ally false in be­hav­ioral re­search, as well as to those writ­ers such as [⁠Nun­nally 1960 ⁠](https://gwern.net/everything#nunnally-1960) and [⁠Meehl 1967 ⁠](https://gwern.net/everything#meehl-1967), who agree with that con­tention. They hold that if the _N_ is large enough, the null is sure to be re­jected in be­hav­ioral re­search. This paper will sug­gest that the Fal­sity con­tention does not hold in the case of ex­per­i­men­tal re­search—that the null hy­poth­e­sis is not gen­er­ally false in such re­search.
> 
> -   Amer­i­can In­sti­tutes For Re­search. 1972 53ya. “OEO re­ports per­for­mance con­tract­ing a fail­ure”. _Be­hav­ioral Sci­ences Newslet­ter for Re­search Plan­ning_, 9, 4–5. [see also [“How We _All_ Failed In Per­for­mance Con­tract­ing” ⁠](https://gwern.net/doc/sociology/1972-page.pdf), Page 1972]
>     

## Loehlin & Nichols1976

Loehlin & Nichols 1976, [_Hered­ity, En­vi­ron­ment and Per­son­al­ity: A Study of 850 Sets of Twins_ ⁠](https://gwern.net/doc/genetics/heritable/1976-loehlin-heredityenvironmentandpersonality.pdf) (see also [_Hered­ity and En­vi­ron­ment: Major Find­ings from Twin Stud­ies of Abil­ity, Per­son­al­ity, and In­ter­ests_ ⁠](https://gwern.net/doc/genetics/heritable/1979-nichols-heredityandenvironment.pdf), Nichols 1976/ 1979):

> This vol­ume re­ports on a study of 850 pairs of twins who were tested to de­ter­mine the in­flu­ence of hered­ity and en­vi­ron­ment on in­di­vid­ual dif­fer­ences in per­son­al­ity, abil­ity, and in­ter­ests. It presents the back­ground, re­search de­sign, and pro­ce­dures of the study, a com­plete tab­u­la­tion of the test re­sults, and the au­thors’ ex­ten­sive analy­sis of their find­ings. Based on one of the largest stud­ies of twin be­hav­ior con­ducted in the twen­ti­eth cen­tury, the book chal­lenges a num­ber of tra­di­tional be­liefs about ge­netic and en­vi­ron­men­tal con­tri­bu­tions to per­son­al­ity de­vel­op­ment.
> 
> The sub­jects were cho­sen from par­tic­i­pants in the Na­tional Merit Schol­ar­ship Qual­i­fy­ing Test of 1962 63ya and were mailed a bat­tery of per­son­al­ity and in­ter­est ques­tion­naires. In ad­di­tion, par­ents of the twins were sent ques­tion­naires ask­ing about the twins’ early ex­pe­ri­ences. A sim­i­lar sam­ple of non­twin stu­dents who had taken the merit exam pro­vided a com­par­i­son group. The ques­tions in­ves­ti­gated in­cluded how twins are sim­i­lar to or dif­fer­ent from non- twins, how iden­ti­cal twins are sim­i­lar to or dif­fer­ent from fra­ter­nal twins, how the per­son­al­i­ties and in­ter­ests of twins re­flect ge­netic fac­tors, how the per­son­al­i­ties and in­ter­ests of twins re­flect early en­vi­ron­men­tal fac­tors, and what im­pli­ca­tions these ques­tions have for the gen­eral issue of how hered­ity and en­vi­ron­ment in­flu­ence the de­vel­op­ment of psy­cho­log­i­cal char­ac­ter­is­tics. In at­tempt­ing to an­swer these ques­tions, the au­thors shed light on the im­por­tance of both genes and en­vi­ron­ment and form the basis for dif­fer­ent ap­proaches in be­hav­ior ge­netic re­search.

The book is largely a dis­cus­sion of com­pre­hen­sive [sum­mary sta­tis­tics ⁠](https://en.wikipedia.org/wiki/Summary_statistics) of twin cor­re­la­tions from an early large- scale twin study (can­vassed via the Na­tional Merit Schol­ar­ship Qual­i­fy­ing Test, 1962 63ya). They at­tempted to com­pile a large- scale twin sam­ple with­out the bur­den of a full- blown twin reg­istry by an ex­ten­sive mail sur­vey of the _n_ = 1507 518ya 11th- grade ado­les­cent pairs of par­tic­i­pants in the high school Na­tional Merit Schol­ar­ship Qual­i­fy­ing Test of 1962 63ya (total _n_ ~600,000) who in­di­cated they were twins (as well as a con­trol sam­ple of non- twins), yield­ing 514 iden­ti­cal twin & 336 (same- sex) fra­ter­nal twin pairs; they were ques­tioned as fol­lows:

> …to these [par­tic­i­pants] were mailed a bat­tery of per­son­al­ity and in­ter­est tests, in­clud­ing the Cal­i­for­nia Psy­cho­log­i­cal In­ven­tory (CPI), the Hol­land Vo­ca­tional Pref­er­ence In­ven­tory (VPI), an ex­per­i­men­tal Ob­jec­tive Be­hav­ior In­ven­tory (OBI), an Ad­jec­tive Check List (ACL), and a num­ber of other, briefer self- rating scales, at­ti­tude mea­sures, and other items. In ad­di­tion, a par­ent was asked to fill out a ques­tion­naire de­scrib­ing the early ex­pe­ri­ences and home en­vi­ron­ment of the twins. Other brief ques­tion­naires were sent to teach­ers and friends, ask­ing them to rate the twins on a num­ber of per­son­al­ity traits; be­cause these rat­ings were avail­able for only part of our basic sam­ple, they have not been an­a­lyzed in de­tail and will not be dis­cussed fur­ther in this book. (The par­ent and twin ques­tion­naires, ex­cept for the CPI, are re­pro­duced in Ap­pen­dix A.)

Un­usu­ally, the book in­cludes ap­pen­dices re­port­ing raw twin- pair cor­re­la­tions for all of the re­ported items, not a mere hand­ful of se­lected analy­ses on full test- scales or sub­fac­tors. (Be­cause of this, I was able to ex­tract vari­ables re­lated to leisure time pref­er­ences & ac­tiv­i­ties for [an­other analy­sis ⁠](https://gwern.net/amuse#loehlin-nichols-1976-a-study-of-850-sets-of-twins).) One can see that even down to the item level, her­i­tabil­i­ties tend to be non- zero and most vari­ables are cor­re­lated within- individuals or with en­vi­ron­ments as well.

## Meehl1978

[“The­o­ret­i­cal risks and tab­u­lar as­ter­isks: Sir Karl, Sir Ronald, and the slow progress of soft psy­chol­ogy” ⁠](https://gwern.net/doc/psychology/1978-meehl.pdf), Meehl 1978:

> Since the null hy­poth­e­sis is quasi- always false, ta­bles sum­ma­riz­ing re­search in terms of pat­terns of “sig­nif­i­cant dif­fer­ences” are lit­tle more than com­plex, causally un­in­ter­pretable out­comes of [sta­tis­ti­cal power ⁠](https://en.wikipedia.org/wiki/Power_of_a_test) func­tions.
> 
> The kinds of the­o­ries and the kinds of the­o­ret­i­cal risks to which we put them in soft psy­chol­ogy when we use sig­nif­i­cance test­ing as our method are _not_ like test­ing Meehl’s the­ory of weather by see­ing how well it fore­casts the num­ber of inches it will rain on cer­tain days. In­stead, they are de­press­ingly close to test­ing the the­ory by see­ing whether it rains in April at all, or rains sev­eral days in April, or rains in April more than in May. It hap­pens mainly be­cause, as I be­lieve is gen­er­ally rec­og­nized by sta­tis­ti­cians today and by thought­ful so­cial sci­en­tists, the null hy­poth­e­sis, taken lit­er­ally, is al­ways false. I shall not at­tempt to doc­u­ment this here, be­cause among so­phis­ti­cated per­sons it is taken for granted. (See Mor­ri­son & Henkel, 1970 55ya [_The Sig­nif­i­cance Test Con­tro­versy: A Reader_], es­pe­cially the chap­ters by [⁠Bakan ⁠](https://gwern.net/everything#bakan-1966), Hog­ben, [⁠Lykken ⁠](https://gwern.net/everything#lykken-1968), [⁠Meehl ⁠](https://gwern.net/everything#meehl-1967), and [Roze­boom ⁠](https://pdfs.semanticscholar.org/b596/4787fc1abf739148d604abfbd2689e73e52f.pdf).) A lit­tle re­flec­tion shows us why it has to be the case, since an out­put vari­able such as adult IQ, or aca­d­e­mic achieve­ment, or ef­fec­tive­ness at com­mu­ni­ca­tion, or what­ever, will al­ways, in the so­cial sci­ences, be a func­tion of a siz­able but fi­nite num­ber of fac­tors. (The small­est con­tri­bu­tions may be con­sid­ered as es­sen­tially a ran­dom vari­ance term.) In order for two groups (males and fe­males, or whites and blacks, or manic de­pres­sives and [schiz­o­phren­ics ⁠](https://en.wikipedia.org/wiki/Schizophrenia), or Re­pub­li­cans and De­moc­rats) to be _ex­actly_ equal on such an out­put vari­able, we have to imag­ine that they are ex­actly equal _or_ del­i­cately coun­ter­bal­anced on all of the con­trib­u­tors in the causal equa­tion, which will never be the case.
> 
> Fol­low­ing the gen­eral line of rea­son­ing (pre­sented by my­self and sev­eral oth­ers over the last decade), from the fact that the null hy­poth­e­sis is al­ways false in soft psy­chol­ogy, it fol­lows that the prob­a­bil­ity of re­fut­ing it de­pends wholly on the sen­si­tiv­ity of the ex­per­i­ment—its log­i­cal de­sign, the net (at­ten­u­ated) con­struct va­lid­ity of the mea­sures, and, most im­por­tantly, the sam­ple size, which de­ter­mines where we are on the sta­tis­ti­cal power func­tion. Putting it crudely, if you have enough cases and your mea­sures are not to­tally un­re­li­able, the null hy­poth­e­sis will al­ways be fal­si­fied, _re­gard­less of the truth of the sub­stan­tive the­ory_. Of course, it could be fal­si­fied in the wrong di­rec­tion, which means that as the power im­proves, the prob­a­bil­ity of a cor­rob­o­ra­tive re­sult ap­proaches one- half. How­ever, if the the­ory has no verisimil­i­tude—such that we can imag­ine, so to speak, pick­ing our em­pir­i­cal re­sults ran­domly out of a di­rec­tional hat apart from any the­ory—the prob­a­bil­ity of re­fut­ing by get­ting a sig­nif­i­cant dif­fer­ence in the wrong di­rec­tion also ap­proaches one- half. Ob­vi­ously, this is quite un­like the sit­u­a­tion de­sired from ei­ther a Bayesian, a Pop­per­ian, or a com­mon­sense sci­en­tific stand­point. As I have pointed out else­where ([⁠Meehl, 1967 ⁠](https://gwern.net/everything#meehl-1967) / 1970 55ya b; but see crit­i­cism by [⁠Oakes, 1975 ⁠](https://gwern.net/everything#oakes-1975); [Keuth, 1973 ⁠](https://gwern.net/doc/statistics/causality/1973-keuth.pdf); and re­but­tal by [Swoyer & Mon­son, 1975 ⁠](https://gwern.net/doc/statistics/causality/1975-swoyer.pdf)), an im­prove­ment in in­stru­men­ta­tion or other sources of ex­per­i­men­tal ac­cu­racy tends, in physics or as­tron­omy or chem­istry or ge­net­ics, to sub­ject the the­ory to a greater risk of refu­ta­tion _modus tol­lens_, whereas im­proved pre­ci­sion in null hy­poth­e­sis test­ing usu­ally de­creases this risk. A suc­cess­ful sig­nif­i­cance test of a sub­stan­tive the­ory in soft psy­chol­ogy pro­vides a fee­ble cor­rob­o­ra­tion of the the­ory be­cause the pro­ce­dure has sub­jected the the­ory to a fee­ble risk.
> 
> …I am not mak­ing some nit- picking sta­tis­ti­cian’s cor­rec­tion. I am say­ing that the whole busi­ness is so rad­i­cally de­fec­tive as to be sci­en­tif­i­cally al­most point­less… I am mak­ing a philo­soph­i­cal com­plaint or, if you pre­fer, a com­plaint in the do­main of sci­en­tific method. I sug­gest that when a re­viewer tries to “make the­o­ret­i­cal sense” out of such a table of fa­vor­able and ad­verse sig­nif­i­cance test re­sults, what the re­viewer is ac­tu­ally en­gaged in, willy- nilly or un­wit­tingly, is mean­ing­less sub­stan­tive con­struc­tions on the prop­er­ties of the sta­tis­ti­cal power func­tion, and al­most noth­ing else.
> 
> …You may say, “But, Meehl, R. A. Fisher was a ge­nius, and we all know how valu­able his stuff has been in agron­omy. Why shouldn’t it work for soft psy­chol­ogy?” Well, I am not in­tim­i­dated by Fisher’s ge­nius, be­cause my com­plaint is not in the field of math­e­mat­i­cal sta­tis­tics, and as re­gards in­duc­tive logic and phi­los­o­phy of sci­ence, it is well- known that Sir Ronald per­mit­ted him­self a great deal of dog­ma­tism. I re­mem­ber my amaze­ment when the late [Rudolf Car­nap ⁠](https://en.wikipedia.org/wiki/Rudolf_Carnap) said to me, the first time I met him, “But, of course, on this sub­ject Fisher is just mis­taken: surely you must know that.” My sta­tis­ti­cian friends tell me that it is not clear just how use­ful the sig­nif­i­cance test has been in bi­o­log­i­cal sci­ence ei­ther, but I set that aside as be­yond my com­pe­tence to dis­cuss.

## Loftus & Loftus1982

[_Essence of Sta­tis­tics_ ⁠](https://gwern.net/doc/statistics/causality/1982-loftus-essenceofstatistics.pdf), Lof­tus & Lof­tus 1982/ 1988 (2nd ed), pg515–516 (pg498–499 in the 1982 43ya print­ing):

> **Rel­a­tive Im­por­tance Of These 3 Mea­sures**. It is a mat­ter of some de­bate as to which of these 3 mea­sures [σ 2 / _p_ / R 2] we should pay the most at­ten­tion to in an ex­per­i­ment. It’s our opin­ion that find­ing a “sig­nif­i­cant ef­fect” re­ally pro­vides very lit­tle in­for­ma­tion be­cause it’s al­most cer­tainly true that _some_ re­la­tion­ship (how­ever small) ex­ists be­tween _any_ two vari­ables. And in gen­eral _find­ing_ a sig­nif­i­cant ef­fect sim­ply means that enough ob­ser­va­tions have been col­lected in the ex­per­i­ment to make the sta­tis­ti­cal test of the ex­per­i­ment pow­er­ful enough to de­tect what­ever ef­fect there is. The smaller the ef­fect, the more pow­er­ful the ex­per­i­ments needs to be of course, but no mat­ter how small the ef­fect, it’s al­ways pos­si­ble in prin­ci­ple to de­sign an ex­per­i­ment suf­fi­ciently pow­er­ful to de­tect it. We saw a strik­ing ex­am­ple of this prin­ci­ple in the of­fice hours ex­per­i­ment. In this ex­per­i­ment there was a re­la­tion­ship be­tween the two vari­ables—and since there were so many sub­jects in the ex­per­i­ment (that is, since the test was so pow­er­ful), this re­la­tion­ship was re­vealed in the sta­tis­ti­cal analy­sis. But was it any­thing to write home about? Cer­tainly not. In any sort of prac­ti­cal con­text the size of the ef­fect, al­though nonzero, is so small it can al­most be ig­nored.
> 
> It is our judg­ment that ac­count­ing for vari­ance is re­ally much more mean­ing­ful than test­ing for sig­nif­i­cance.

## Meehl1990 (1)

[“Why sum­maries of re­search on psy­cho­log­i­cal the­o­ries are often un­in­ter­pretable” ⁠](https://meehl.umn.edu/sites/g/files/pua1696/f/144whysummaries.pdf), Meehl 1990a (also dis­cussed in Cohen’s 1994 paper [“The Earth is Round (_p_ < 0.05)” ⁠](https://www.sjsu.edu/faculty/gerstman/misc/Cohen1994.pdf)):

> Prob­lem 6. _Crud fac­tor_: In the so­cial sci­ences and ar­guably in the bi­o­log­i­cal sci­ences, “every­thing cor­re­lates to some ex­tent with every­thing else.” This tru­ism, which I have found no com­pe­tent psy­chol­o­gist dis­putes given 5 min­utes re­flec­tion, does not apply to pure ex­per­i­men­tal stud­ies in which at­trib­utes that the sub­jects bring with them are not the sub­ject of study (ex­cept in so far as they ap­pear as a source of error and hence in the de­nom­i­na­tor of a sig­nif­i­cance test).6 There is noth­ing mys­te­ri­ous about the fact that in psy­chol­ogy and so­ci­ol­ogy every­thing cor­re­lates with every­thing. Any mea­sured trait or at­tribute is some func­tion of a list of partly known and mostly un­known causal fac­tors in the genes and life his­tory of the in­di­vid­ual, and both ge­netic and en­vi­ron­men­tal fac­tors are known from tons of em­pir­i­cal re­search to be them­selves cor­re­lated. To take an ex­treme case, sup­pose we con­strue the null hy­poth­e­sis lit­er­ally (ob­ject­ing that we mean by it “al­most null” gets ahead of the story, and de­stroys the rigor of the Fish­er­ian math­e­mat­ics!) and ask whether we ex­pect males and fe­males in Min­nesota to be pre­cisely equal in some ar­bi­trary trait that has in­di­vid­ual dif­fer­ences, say, color nam­ing. In the case of color nam­ing we could think of some ob­vi­ous dif­fer­ences right off, but even if we didn’t know about them, what is the causal sit­u­a­tion? If we write a causal equa­tion (which is not the same as a re­gres­sion equa­tion for pure pre­dic­tive pur­poses but which, if we had it, would serve bet­ter than the lat­ter) so that the score of an in­di­vid­ual male is some func­tion (pre­sum­ably non­lin­ear if we knew enough about it but here sup­posed lin­ear for sim­plic­ity) of a rather long set of causal vari­ables of ge­netic and en­vi­ron­men­tal type _X_ 1, _X_ 2, … _X_ m. These val­ues are op­er­ated upon by re­gres­sion co­ef­fi­cients _b_ 1, _b_ 2, … _b_ m.
> 
> …Now we write a sim­i­lar equa­tion for the class of fe­males. Can any­one sup­pose that the beta co­ef­fi­cients for the two sexes will be ex­actly the same? Can any­one imag­ine that the mean val­ues of all of the _X_ s will be ex­actly the same for males and fe­males, even if the cul­ture were not still con­sid­er­ably sex­ist in child- rearing prac­tices and the like? If the betas are not ex­actly the same for the two sexes, and the mean val­ues of the _X_ s are not ex­actly the same, what kind of Leib­nitz­ian preestab­lished har­mony would we have to imag­ine in order for the mean color- naming score to come out ex­actly equal be­tween males and fe­males? It bog­gles the mind; it sim­ply would never hap­pen. As Ein­stein said, “the Lord God is sub­tle, but He is not ma­li­cious.” We can­not imag­ine that na­ture is out to fool us by this kind of del­i­cate bal­anc­ing. Any­body fa­mil­iar with large scale re­search data takes it as a mat­ter of course that when the _N_ gets big enough she will not be look­ing for the statistically- significant cor­re­la­tions but rather look­ing at their pat­terns, since al­most all of them will be sig­nif­i­cant. In say­ing this, I am not going counter to what is stated by math­e­mat­i­cal sta­tis­ti­cians or psy­chol­o­gists with sta­tis­ti­cal ex­per­tise. For ex­am­ple, the stan­dard psy­chol­o­gist’s text­book, the ex­cel­lent treat­ment by Hays ([⁠1973, page 415 ⁠](https://gwern.net/everything#hays-1973)), ex­plic­itly states that, taken lit­er­ally, the null hy­poth­e­sis is al­ways false.
> 
> 20 ago David Lykken and I con­ducted an ex­ploratory study of the crud fac­tor which we never pub­lished but I shall sum­ma­rize it briefly here. (I offer it not as “em­pir­i­cal proof”—that _H_ 0 taken lit­er­ally is quasi- always false hardly needs proof and is gen­er­ally ad­mit­ted—but as a punchy and some­what amus­ing ex­am­ple of an in­suf­fi­ciently ap­pre­ci­ated truth about soft cor­re­la­tional psy­chol­ogy.) In 1966 59ya, the Uni­ver­sity of Min­nesota Stu­dent Coun­sel­ing Bu­reau’s Statewide Test­ing Pro­gram ad­min­is­tered a ques­tion­naire to 57,000 high school se­niors, the items deal­ing with fam­ily facts, at­ti­tudes to­ward school, vo­ca­tional and ed­u­ca­tional plans, leisure time ac­tiv­i­ties, school or­ga­ni­za­tions, etc. We cross- tabulated a total of 15 (and then 45) vari­ables in­clud­ing the fol­low­ing (the num­ber of cat­e­gories for each vari­able given in paren­the­ses): fa­ther’s oc­cu­pa­tion (7), fa­ther’s ed­u­ca­tion (9), mother’s ed­u­ca­tion (9), num­ber of sib­lings (10), birth order (only, old­est, youngest, nei­ther), ed­u­ca­tional plans after high school (3), fam­ily at­ti­tudes to­wards col­lege (3), do you like school (3), sex (2), col­lege choice (7), oc­cu­pa­tional plan in 10 years (20), and re­li­gious pref­er­ence (20). In ad­di­tion, there were 22 “leisure time ac­tiv­i­ties” such as “act­ing”, “model build­ing”, “cook­ing”, etc., which could be treated ei­ther as a sin­gle 22- category vari­able or as 22 di­choto­mous vari­ables. There were also 10 “high school or­ga­ni­za­tions” such as “school sub­ject clubs”, “farm youth groups”, “po­lit­i­cal clubs”, etc., which also could be treated ei­ther as a sin­gle ten- category vari­able or as 10 di­choto­mous vari­ables. Con­sid­er­ing the lat­ter two vari­ables as mul­ti­chotomies gives a total of 15 vari­ables pro­duc­ing 105 dif­fer­ent cross- tabulations. All val­ues of χ 2 for these 105 cross- tabulations were statistically- significant, and 101 (96%) of them were sig­nif­i­cant with a prob­a­bil­ity of less than 10 −6.
> 
> …If “leisure ac­tiv­ity” and “high school or­ga­ni­za­tions” are con­sid­ered as sep­a­rate di­chotomies, this gives a total of 45 vari­ables and 990 dif­fer­ent crosstab­u­la­tions. Of these, 92% were statistically- significant and more than 78% were sig­nif­i­cant with a prob­a­bil­ity less than 10 −6. Looked at in an­other way, the me­dian num­ber of sig­nif­i­cant re­la­tion­ships be­tween a given vari­able and all the oth­ers was 41 out of a pos­si­ble 44!
> 
> We also com­puted [MCAT ⁠](https://en.wikipedia.org/wiki/Medical_College_Admission_Test) scores by cat­e­gory for the fol­low­ing vari­ables: num­ber of sib­lings, birth order, sex, oc­cu­pa­tional plan, and re­li­gious pref­er­ence. Highly sig­nif­i­cant de­vi­a­tions from chance al­lo­ca­tion over cat­e­gories were found for each of these vari­ables. For ex­am­ple, the fe­males score higher than the males; MCAT score steadily and markedly de­creases with in­creas­ing num­bers of sib­lings; el­dest or only chil­dren are statistically- significantly brighter than youngest chil­dren; there are marked dif­fer­ences in MCAT scores be­tween those who hope to be­come nurses and those who hope to be­come nurses aides, or be­tween those plan­ning to be farm­ers, en­gi­neers, teach­ers, or physi­cians; and there are sub­stan­tial MCAT dif­fer­ences among the var­i­ous re­li­gious groups. We also tab­u­lated the 5 prin­ci­pal Protes­tant re­li­gious de­nom­i­na­tions (Bap­tist, Epis­co­pal, Lutheran, Methodist, and Pres­by­ter­ian) against all the other vari­ables, find­ing highly sig­nif­i­cant re­la­tion­ships in most in­stances. For ex­am­ple, only chil­dren are nearly twice as likely to be Pres­by­ter­ian than Bap­tist in Min­nesota, more than half of the Epis­co­palians “usu­ally like school” but only 45% of Luther­ans do, 55% of Pres­by­te­ri­ans feel that their grades re­flect their abil­i­ties as com­pared to only 47% of Epis­co­palians, and Epis­co­palians are more likely to be male whereas Bap­tists are more likely to be fe­male. 83% of Bap­tist chil­dren said that they en­joyed danc­ing as com­pared to 68% of Lutheran chil­dren. More than twice the pro­por­tion of Epis­co­palians plan to at­tend an out of state col­lege than is true for Bap­tists, Luther­ans, or Methodists. The pro­por­tion of Methodists who plan to be­come con­ser­va­tion­ists is nearly twice that for Bap­tists, whereas the pro­por­tion of Bap­tists who plan to be­come re­cep­tion­ists is nearly twice that for Epis­co­palians.
> 
> In ad­di­tion, we tab­u­lated the 4 prin­ci­pal Lutheran Syn­ods (Mis­souri, ALC, LCA, and Wis­con­sin) against the other vari­ables, again find­ing highly sig­nif­i­cant re­la­tion­ships in most cases. Thus, 5.9% of Wis­con­sin Synod chil­dren have no sib­lings as com­pared to only 3.4% of Mis­souri Synod chil­dren. 58% of ALC Luther­ans are in­volved in play­ing a mu­si­cal in­stru­ment or singing as com­pared to 67% of Mis­souri Synod Luther­ans. 80% of Mis­souri Synod Luther­ans be­long to school or po­lit­i­cal clubs as com­pared to only 71% of LCA Luther­ans. 49% of ALC Luther­ans be­long to de­bate, dra­mat­ics, or mu­si­cal or­ga­ni­za­tions in high school as com­pared to only 40% of Mis­souri Synod Luther­ans. 36% of LCA Luther­ans be­long to or­ga­nized non- school youth groups as com­pared to only 21% of Wis­con­sin Synod Luther­ans. [Pre­ced­ing text cour­tesy of D. T. Lykken.]
> 
> These re­la­tion­ships are not, I re­peat, Type I er­rors. They are facts about the world, and with _N_ = 57,000 they are pretty sta­ble. Some are the­o­ret­i­cally easy to ex­plain, oth­ers more dif­fi­cult, oth­ers com­pletely baf­fling. The “easy” ones have mul­ti­ple ex­pla­na­tions, some­times com­pet­ing, usu­ally not. Draw­ing the­o­ries from a pot and as­so­ci­at­ing them whim­si­cally with vari­able pairs would yield an im­pres­sive batch of _H_ 0 - refuting “con­fir­ma­tions.”
> 
> An­other amus­ing ex­am­ple is the be­hav­ior of the items in the 550 items of the MMPI pool with re­spect to sex. Only 60 items ap­pear on the Mf scale, about the same num­ber that were put into the pool with the hope that they would dis­crim­i­nate fem­i­nin­ity. It turned out that over half the items in the scale were not put in the pool for that pur­pose, and of those that were, a bare ma­jor­ity did the job. Scale de­riva­tion was based on item analy­sis of a small group of cri­te­rion cases of male ho­mo­sex­ual in­vert syn­drome, a sig­nif­i­cant dif­fer­ence on a rather small _N_ of Dr. Starke Hath­away’s pri­vate pa­tients being then con­joined with the re­quire­ment of dis­crim­i­nat­ing be­tween male nor­mals and fe­male nor­mals. When the _N_ be­comes very large as in the data pub­lished by [Swen­son, Pear­son, and Os­borne (1973 52ya; _An MMPI Source Book: Basic Item, Scale, And Pat­tern Data On 50,000 Med­ical Pa­tients_. Min­neapo­lis, MN: Uni­ver­sity of Min­nesota Press.) ⁠](https://gwern.net/doc/statistics/causality/1973-wendell-anmmpisourcebook.pdf), ap­prox­i­mately 25,000 of each sex tested at the Mayo Clinic over a pe­riod of years, it turns out that 507 of the 550 items dis­crim­i­nate the sexes. Thus in a het­ero­ge­neous item pool we find only 8% of items fail­ing to show a sig­nif­i­cant dif­fer­ence on the sex di­chotomy. The fol­low­ing are sex- discriminators, the male/ fe­male dif­fer­ences rang­ing from a few per­cent­age points to over 30%:7
> 
> -   Some­times when I am not feel­ing well I am cross.
>     
> -   I be­lieve there is a Devil and a Hell in af­ter­life.
>     
> -   I think nearly any­one would tell a lie to keep out of trou­ble.
>     
> -   Most peo­ple make friends be­cause friends are likely to be use­ful to them.
>     
> -   I like po­etry.
>     
> -   I like to cook.
>     
> -   Po­lice­men are usu­ally hon­est.
>     
> -   I some­times tease an­i­mals.
>     
> -   My hands and feet are usu­ally warm enough.
>     
> -   I think Lin­coln was greater than Wash­ing­ton.
>     
> -   I am cer­tainly lack­ing in self- confidence.
>     
> -   Any man who is able and will­ing to work hard has a good chance of suc­ceed­ing.
>     
> 
> I in­vite the reader to guess which di­rec­tion scores “fem­i­nine.” Given this in­for­ma­tion, I find some items easy to “ex­plain” by one ob­vi­ous the­ory, oth­ers have com­pet­ing plau­si­ble ex­pla­na­tions, still oth­ers are baf­fling.
> 
> Note that we are not deal­ing here with some source of sta­tis­ti­cal error (the oc­cur­rence of ran­dom sam­pling fluc­tu­a­tions). That source of error is lim­ited by the sig­nif­i­cance level we choose, just as the prob­a­bil­ity of Type II error is set by ini­tial choice of the sta­tis­ti­cal power, based upon a pilot study or other an­tecedent data con­cern­ing an ex­pected av­er­age dif­fer­ence. Since in so­cial sci­ence every­thing cor­re­lates with every­thing to some ex­tent, due to com­plex and ob­scure causal in­flu­ences, in con­sid­er­ing the crud fac­tor we are talk­ing about _real_ dif­fer­ences, _real_ cor­re­la­tions, _real_ trends and pat­terns for which there is, of course, some true but com­pli­cated mul­ti­vari­ate causal the­ory. I am not sug­gest­ing that these cor­re­la­tions are fun­da­men­tally un­ex­plain­able. They would be com­pletely ex­plained if we had the knowl­edge of Om­ni­scient Jones, which we don’t. The point is that we are in the weak sit­u­a­tion of cor­rob­o­rat­ing our par­tic­u­lar sub­stan­tive the­ory by show­ing that _X_ and _Y_ are “re­lated in a non­chance man­ner”, when our the­ory is too weak to make a nu­mer­i­cal pre­dic­tion or even (usu­ally) to set up a range of ad­mis­si­ble val­ues that would be counted as cor­rob­o­ra­tive.
> 
> …Some psy­chol­o­gists play down the in­flu­ence of the ubiq­ui­tous crud fac­tor, what [⁠David Lykken (1968 57ya) ⁠](https://gwern.net/everything#lykken-1968) calls the “am­bi­ent cor­re­la­tional noise” in so­cial sci­ence, by say­ing that we are not in dan­ger of being mis­led by small dif­fer­ences that show up as sig­nif­i­cant in gi­gan­tic sam­ples. How much that soft­ens the blow of the crud fac­tor’s in­flu­ence de­pends upon the crud fac­tor’s av­er­age size in a given re­search do­main, about which nei­ther I nor any­body else has ac­cu­rate in­for­ma­tion. _But the no­tion that the cor­re­la­tion be­tween ar­bi­trar­ily paired trait vari­ables will be, while not lit­er­ally zero, of such mi­nus­cule size as to be of no im­por­tance, is surely wrong._ Every­body knows that there is a set of de­mo­graphic fac­tors, some un­der­stood and oth­ers quite mys­te­ri­ous, that cor­re­late quite re­spectably with a va­ri­ety of traits. (So­cioe­co­nomic sta­tus, SES, is the one usu­ally con­sid­ered, and fre­quently as­sumed to be only in the “input” causal role.) The clin­i­cal scales of the MMPI were de­vel­oped by em­pir­i­cal key­ing against a set of dis­junct noso­log­i­cal cat­e­gories, some of which are phe­nom­e­no­log­i­cally and psy­cho­dy­nam­i­cally op­po­site to oth­ers. Yet the 45 pair­wise cor­re­la­tions of these scales are al­most al­ways pos­i­tive (scale Ma pro­vides most of the neg­a­tives) and a rep­re­sen­ta­tive size is in the neigh­bor­hood of 0.35 to 0.40. The same is true of the scores on the Strong Vo­ca­tional In­ter­est Blank, where I find an av­er­age ab­solute value cor­re­la­tion close to 0.40. The ma­lig­nant in­flu­ence of so- called “meth­ods co­vari­ance” in psy­cho­log­i­cal re­search that re­lies upon tasks or tests hav­ing cer­tain kinds of be­hav­ioral sim­i­lar­i­ties such as ques­tion­naires or ink blots is com­mon­place and a reg­u­lar source of con­cern to clin­i­cal and per­son­al­ity psy­chol­o­gists. For fur­ther dis­cus­sion and ex­am­ples of crud fac­tor size, see [⁠Meehl (1990 35ya) ⁠](https://gwern.net/everything#meehl-1990-2).
> 
> Now sup­pose we imag­ine a so­ci­ety of psy­chol­o­gists doing re­search in this soft area, and each in­ves­ti­ga­tor sets his ex­per­i­ments up in a whim­si­cal, ir­ra­tional man­ner as fol­lows: First he picks a the­ory at ran­dom out of the the­ory pot. Then he picks a pair of vari­ables ran­domly out of the ob­serv­able vari­able pot. He then ar­bi­trar­ily as­signs a di­rec­tion (you un­der­stand there is no in­trin­sic con­nec­tion of con­tent be­tween the sub­stan­tive the­ory and the vari­ables, ex­cept once in a while there would be such by co­in­ci­dence) and says that he is going to test the ran­domly cho­sen sub­stan­tive the­ory by pre­tend­ing that it pre­dicts—al­though in fact it does not, hav­ing no in­trin­sic con­tentual re­la­tion—a pos­i­tive cor­re­la­tion be­tween ran­domly cho­sen ob­ser­va­tional vari­ables _X_ and _Y_. Now sup­pose that the crud fac­tor op­er­a­tive in the broad do­main were 0.30, that is, the av­er­age cor­re­la­tion be­tween all of the vari­ables pair­wise in this do­main is 0.30. This is not sam­pling error but the true cor­re­la­tion pro­duced by some com­plex un­known net­work of ge­netic and en­vi­ron­men­tal fac­tors. Sup­pose he di­vides a nor­mal dis­tri­b­u­tion of sub­jects at the me­dian and uses all of his cases (which fre­quently is not what is done, al­though if prop­erly treated sta­tis­ti­cally that is not method­olog­i­cally sin­ful). Let us take vari­able _X_ as the “input” vari­able (never mind its causal role). The mean score of the cases in the top half of the dis­tri­b­u­tion will then be at one mean de­vi­a­tion, that is, in stan­dard score terms they will have an av­er­age score of 0.80. Sim­i­larly, the sub­jects in the bot­tom half of the _X_ dis­tri­b­u­tion will have a mean stan­dard score of -0.80. So the mean dif­fer­ence in stan­dard score terms be­tween the high and low _X_ s, the one “ex­per­i­men­tal” and the other “con­trol” group, is 1.6. If the re­gres­sion of out­put vari­able _Y_ on _X_ is ap­prox­i­mately lin­ear, this yields an ex­pected dif­fer­ence in stan­dard score terms of 0.48, so the dif­fer­ence on the ar­bi­trar­ily de­fined “out­put” vari­able _Y_ is in the neigh­bor­hood of half a stan­dard de­vi­a­tion.
> 
> When the in­ves­ti­ga­tor runs a _t_ -test on these data, what is the prob­a­bil­ity of achiev­ing a statistically- significant re­sult? This de­pends upon the sta­tis­ti­cal power func­tion and hence upon the sam­ple size, which varies widely, more in soft psy­chol­ogy be­cause of the na­ture of the data col­lec­tion prob­lems than in ex­per­i­men­tal work. I do not have exact fig­ures, but an in­for­mal scan­ning of sev­eral is­sues of jour­nals in the soft areas of clin­i­cal, ab­nor­mal, and so­cial gave me a rep­re­sen­ta­tive value of the num­ber of cases in each of two groups being com­pared at around _N_ 1 = _N_ 2 = 37 (that’s a me­dian be­cause of the skew­ness, sam­ple sizes rang­ing from a low of 17 in one clin­i­cal study to a high of 1,000 in a so­cial sur­vey study). As­sum­ing equal vari­ances, this gives us a stan­dard error of the mean dif­fer­ence of 0.2357 in sigma- units, so that our _t_ is a lit­tle over 2.0. The sub­stan­tive the­ory in a real life case being al­most in­vari­ably pre­dic­tive of a di­rec­tion (it is hard to know what sort of sig­nif­i­cance test­ing we would be doing oth­er­wise), the 5% level of con­fi­dence can be le­git­i­mately taken as one- tailed and in fact could be crit­i­cized if it were not (as­sum­ing that the 5% level of con­fi­dence is given the usual spe­cial mag­i­cal sig­nif­i­cance af­forded it by so­cial sci­en­tists!). The di­rec­tional 5% level being at 1.65, the ex­pected value of our _t_ -test in this sit­u­a­tion is ap­prox­i­mately 0.35 _t_ units from the re­quired sig­nif­i­cance level. Things being es­sen­tially nor­mal for 72 df, this gives us a power of de­tect­ing a dif­fer­ence of around 0.64.
> 
> How­ever, since in our imag­ined “ex­per­i­ment” the as­sign­ment of di­rec­tion was ran­dom, the prob­a­bil­ity of de­tect­ing a dif­fer­ence in the pre­dicted di­rec­tion (even though in re­al­ity this pre­dic­tion was not me­di­ated by any ra­tio­nal re­la­tion of con­tent) is only half of that. Even this con­ser­v­a­tive power based upon the as­sump­tion of a com­pletely ran­dom as­so­ci­a­tion be­tween the the­o­ret­i­cal sub­stance and the pseudo­pre­dicted di­rec­tion should give one pause. We find that the prob­a­bil­ity of get­ting a pos­i­tive re­sult from a the­ory with no verisimil­i­tude what­so­ever, as­so­ci­ated in a to­tally whim­si­cal fash­ion with a pair of vari­ables picked ran­domly out of the ob­ser­va­tional pot, is _one chance in 3_! This is quite dif­fer­ent from the 0.05 level that peo­ple usu­ally think about. Of course, the rea­son for this is that the 0.05 level is based upon strictly hold­ing _H_ 0 if the the­ory were false. Whereas, be­cause in the so­cial sci­ences every­thing is cor­re­lated with every­thing, for epis­temic pur­poses (de­spite the rigor of the math­e­mati­cian’s ta­bles) the true base­line—if the the­ory has noth­ing to do with re­al­ity and has only a chance re­la­tion­ship to it (so to speak, “any con­nec­tion be­tween the the­ory and the facts is purely co­in­ci­den­tal”) - is 6 or 7 times as great as the re­as­sur­ing 0.05 level upon which the psy­chol­o­gist fo­cuses his mind. If the crud fac­tor in a do­main were run­ning around 0.40, the power func­tion is 0.86 and the “di­rec­tional power” for ran­dom the­ory/ pre­dic­tion pair­ings would be 0.43.
> 
> …A sim­i­lar sit­u­a­tion holds for psy­chopathol­ogy, and for many vari­ables in per­son­al­ity mea­sure­ment that refer to as­pects of so­cial com­pe­tence on the one hand or im­pair­ment of in­ter­per­sonal func­tion (as in men­tal ill­ness) on the other. [⁠Thorndike had a dic­tum ⁠](https://gwern.net/everything#thorndike-1920) “All good things tend to go to­gether.”

## Meehl1990 (2)

[“Ap­prais­ing and amend­ing the­o­ries: the strat­egy of Lakatosian de­fense and two prin­ci­ples that war­rant using it” ⁠](https://meehl.umn.edu/sites/meehl.umn.edu/files/files/147appraisingamending.pdf), Meehl 1990b:

> Re­search in the be­hav­ioral sci­ences can be ex­per­i­men­tal, cor­re­la­tional, or field study (in­clud­ing clin­i­cal); only the first two are ad­dressed here. For rea­sons to be ex­plained ([⁠Meehl, 1990 35ya c ⁠](https://gwern.net/everything#meehl-1990-1)), I treat as cor­re­la­tional those ex­per­i­men­tal stud­ies in which the chief the­o­ret­i­cal test pro­vided in­volves an in­ter­ac­tion ef­fect be­tween an ex­per­i­men­tal ma­nip­u­la­tion and an individual- differences vari­able (whether trait, sta­tus, or de­mo­graphic). In cor­re­la­tional re­search there arises a spe­cial prob­lem for the so­cial sci­en­tist from the em­pir­i­cal fact that “every­thing is cor­re­lated with every­thing, more or less.” My col­league David Lykken presses the point fur­ther to in­clude most, if not all, purely ex­per­i­men­tal re­search de­signs, say­ing that, speak­ing causally, “Every­thing in­flu­ences every­thing”, a stronger the­sis that I nei­ther as­sert nor deny but that I do not rely on here. The ob­vi­ous fact that every­thing is more or less cor­re­lated with every­thing in the so­cial sci­ences is read­ily fore­seen from the arm­chair on common- sense con­sid­er­a­tions. These are strength­ened by more ad­vanced the­o­ret­i­cal ar­gu­ments in­volv­ing such con­cepts as ge­netic link­age, auto- catalytic ef­fects be­tween cog­ni­tive and af­fec­tive processes, traits re­flect­ing in­flu­ences such as child- rearing prac­tices cor­re­lated with in­tel­li­gence, eth­nic­ity, so­cial class, re­li­gion, and so forth. If one asks, to take a triv­ial and the­o­ret­i­cally un­in­ter­est­ing ex­am­ple, whether we might ex­pect to find so­cial class dif­fer­ences in a color- naming test, there im­me­di­ately spring to mind nu­mer­ous in­flu­ences, rang­ing from (1) ver­bal in­tel­li­gence lead­ing to bet­ter ver­bal dis­crim­i­na­tions and re­ten­tion of color names to (2) class dif­fer­ences in ma­ter­nal teach­ing be­hav­ior (which one can read­ily ob­serve by watch­ing moth­ers ex­plain things to their chil­dren at a zoo) to (3) more sub­tle—but still nonzero—in­flu­ences, such as upper- class chil­dren being more likely An­gli­cans than Bap­tists, hence ex­posed to the changes in litur­gi­cal col­ors dur­ing the church year! Ex­am­ples of such mul­ti­ple pos­si­ble in­flu­ences are so easy to gen­er­ate, I shall re­sist the temp­ta­tion to go on. If some­body asks a psy­chol­o­gist or so­ci­ol­o­gist whether she might ex­pect a nonzero cor­re­la­tion be­tween den­tal caries and IQ, the best guess would be yes, small but statistically- significant. A small neg­a­tive cor­re­la­tion was in fact found dur­ing the 1920s, mis­lead­ing some hy­gien­ists to hold that IQ was low­ered by tox­ins from de­cayed teeth. (The re­ceived ex­pla­na­tion today is that den­tal caries and IQ are both cor­re­lates of so­cial class.) More than 75 years ago, Ed­ward Lee Thorndike enun­ci­ated the fa­mous dic­tum, “All good things tend to go to­gether, as do all bad ones.” Al­most all human per­for­mance (work com­pe­tence) dis­po­si­tions, if care­fully stud­ied, are sat­u­rated to some ex­tent with the gen­eral in­tel­li­gence fac­tor _g_, which for psy­cho­dy­namic and ide­o­log­i­cal rea­sons has been some­what ne­glected in re­cent years but is due for a come­back (Betz, 1986 39ya).⁠ [11](#fn:11)
> 
> The ubiq­uity of nonzero cor­re­la­tions gives rise to what is method­olog­i­cally dis­turb­ing to the the­ory tester and what I call, fol­low­ing Lykken, the crud fac­tor. I have dis­cussed this at length else­where (Meehl, 1990 35ya c), so I only sum­ma­rize and pro­vide a cou­ple of ex­am­ples here. The main point is that, when the sam­ple size is suf­fi­ciently large to pro­duce ac­cu­rate es­ti­mates of the pop­u­la­tion val­ues, al­most any pair of vari­ables in psy­chol­ogy will be cor­re­lated to some ex­tent. Thus, for in­stance, less than 10% of the items in the MMPI item pool were put into the pool with masculinity- femininity in mind, and the em­pir­i­cally de­rived _Mf_ scale con­tains only some of those plus oth­ers put into the item pool for other rea­sons, or with­out any the­o­ret­i­cal con­sid­er­a­tions. When one sam­ples thou­sands of in­di­vid­u­als, it turns out that only 43 of the 550 items (8%) fail to show a sig­nif­i­cant dif­fer­ence be­tween males and fe­males. In an un­pub­lished study (but see Meehl, 1990 35ya c) of the hob­bies, in­ter­ests, vo­ca­tional plans, school course pref­er­ences, so­cial life, and home fac­tors of Min­nesota col­lege fresh­men, when Lykken and I ran chi squares on all pos­si­ble pair­wise com­bi­na­tions of vari­ables, 92% were sig­nif­i­cant, and 78% were sig­nif­i­cant at _p_ < 10 −6. Looked at an­other way, the me­dian num­ber of sig­nif­i­cant re­la­tion­ships be­tween a given vari­able and all the oth­ers was 41 of a pos­si­ble 44. One finds such odd­i­ties as a re­la­tion­ship be­tween which kind of shop courses boys pre­ferred in high school and which of sev­eral Lutheran syn­ods they be­longed to!
> 
> …The third ob­jec­tion is some­what harder to an­swer be­cause it would re­quire an en­cy­clo­pe­dic sur­vey of re­search lit­er­a­ture over many do­mains. It is ar­gued that, al­though the crud fac­tor is ad­mit­tedly ubiq­ui­tous—that is, al­most no cor­re­la­tions of the so­cial sci­ences are lit­er­ally zero (as re­quired by the usual sig­nif­i­cance test)—the crud fac­tor is in most re­search do­mains not large enough to be worth wor­ry­ing about. With­out mak­ing a claim to know just how big it is, I think this ob­jec­tion is pretty clearly un­sound. Doubt­less the av­er­age cor­re­la­tion of any ran­domly picked pair of vari­ables in so­cial sci­ence de­pends on the do­main, and also on the in­stru­ments em­ployed (eg. it is well known that per­son­al­ity in­ven­to­ries often have as much methods- covariance as they do cri­te­rion va­lidi­ties).
> 
> A rep­re­sen­ta­tive pair­wise cor­re­la­tion among MMPI scales, de­spite the marked dif­fer­ences (some­times amount­ing to phe­nom­e­no­log­i­cal “op­po­site­ness”) of the noso­log­i­cal rubrics on which they were de­rived, is in the mid­dle to high 0.30s, in both nor­mal and ab­nor­mal pop­u­la­tions. The same is true for the oc­cu­pa­tional keys of the [Strong Vo­ca­tional In­ter­est Bank ⁠](https://en.wikipedia.org/wiki/Strong_Interest_Inventory). De­lib­er­ately aim­ing to di­ver­sify the qual­i­ta­tive fea­tures of cog­ni­tive tasks (and thus “pu­rify” the mea­sures) in his clas­sic stud­ies of pri­mary men­tal abil­i­ties (“pure fac­tors”, or­thog­o­nal), Thur­stone (1938 87ya; [Thur­stone & Thur­stone, 1941 ⁠](https://gwern.net/doc/iq/1941-thurstone-factorialstudiesofintelligence.pdf)) still found an av­er­age in­tertest cor­re­la­tion of.28 (range = 0.01 to.56!) in the cross- validation sam­ple. In the set of 20 [Cal­i­for­nia Psy­cho­log­i­cal In­ven­tory (CPI) ⁠](https://en.wikipedia.org/wiki/California_Psychological_Inventory) scales built to cover broadly the do­main of (nor­mal range) “folk- concept” traits, Gough (1987 38ya) found an av­er­age pair­wise cor­re­la­tion of.44 among both males and fe­males. Guil­ford’s So­cial [In­tro­ver­sion ⁠](https://en.wikipedia.org/wiki/Extraversion_and_introversion), Think­ing In­tro­ver­sion, De­pres­sion, Cy­cloid Ten­den­cies, and Rhathymia or Free­dom From Care scales, con­structed on the basis of (or­thog­o­nal) fac­tors, showed pair­wise cor­re­la­tions rang­ing from -.02 to.85, with 5 of the 10 _r_ s ≥ 0.33 de­spite the pu­rifi­ca­tion ef­fort ([Evans & Mc­Connell, 1941 ⁠](https://gwern.net/doc/psychology/personality/1941-evans.pdf)). Any trea­tise on fac­tor analy­sis ex­em­pli­fy­ing pro­ce­dures with em­pir­i­cal data suf­fices to make the point con­vinc­ingly. For ex­am­ple, in [⁠Har­man (1960 65ya) ⁠](https://archive.org/details/ModernFactorAnalysis), 8 “emo­tional” vari­ables cor­re­late.10 to.87, me­dian _r_ = 0.44 (p. 176), and 8 “po­lit­i­cal” vari­ables cor­re­late.03 to.88, me­dian (ab­solute value) _r_ = 0.62 (p. 178). For highly di­verse acquiescence- corrected mea­sures (per­son­al­ity traits, in­ter­ests, hob­bies, psy­chopathol­ogy, so­cial at­ti­tudes, and re­li­gious, po­lit­i­cal, and moral opin­ions), es­ti­mat­ing in­di­vid­u­als’ (or­thog­o­nal!) fac­tor scores, one can hold mean _r_ s down to an av­er­age of.12, means.04–.20, still some in­di­vid­ual _r_ s > 0.30 (Lykken, per­sonal com­mu­ni­ca­tion, 1990 35ya; cf. Mc­Closky & Meehl, in prepa­ra­tion). Pub­lic opin­ion polls and at­ti­tude sur­veys rou­tinely dis­ag­gre­gate data with re­spect to sev­eral de­mo­graphic vari­ables (eg. age, ed­u­ca­tion, sec­tion of coun­try, sex, eth­nic­ity, re­li­gion, ed­u­ca­tion, in­come, rural/ urban, self- described po­lit­i­cal af­fil­i­a­tion) be­cause these fac­tors are al­ways cor­re­lated with at­ti­tudes or elec­toral choices, some­times strongly so. One must also keep in mind that so­cioe­co­nomic sta­tus, al­though in­trin­si­cally in­ter­est­ing (es­pe­cially to so­ci­ol­o­gists) is prob­a­bly often func­tion­ing as a proxy for other un­mea­sured per­son­al­ity or sta­tus char­ac­ter­is­tics that are not part of the de­f­i­n­i­tion of so­cial class but are, for a va­ri­ety of com­pli­cated rea­sons, cor­re­lated with it. The proxy role is im­por­tant be­cause it pre­vents ad­e­quate “con­trol­ling for” un­known (or un­mea­sured) crud- factor in­flu­ences by sta­tis­ti­cal pro­ce­dures (match­ing, par­tial cor­re­la­tion, analy­sis of co­vari­ance, [path analy­sis ⁠](https://en.wikipedia.org/wiki/Path_analysis_\(statistics\))). [ie [“resid­ual con­found­ing” ⁠](https://gwern.net/doc/statistics/bayes/regression-to-mean/index)]
> 
> -   Thur­stone, L. L. (1938 87ya). _Pri­mary men­tal abil­i­ties_. Chicago: Uni­ver­sity of Chicago Press.
>     
> -   Gough, H. G. (1987 38ya). _CPI, Ad­min­is­tra­tor’s guide_. Palo Alto, CA: Con­sult­ing Psy­chol­o­gists Press.
>     
> -   Mc­Closky, Her­bert, & Meehl, P. E. (in prepa­ra­tion). _Ide­olo­gies in con­flict_.⁠ [12](#fn:12)
>     

## Tukey1991

[“The phi­los­o­phy of mul­ti­ple com­par­isons” ⁠](https://projecteuclid.org/download/pdf_1/euclid.ss/1177011945), Tukey 1991:

> Sta­tis­ti­cians clas­si­cally asked the wrong ques­tion—and were will­ing to an­swer with a lie, one that was often a down­right lie. They asked “Are the ef­fects of A and B dif­fer­ent?” and they were will­ing to an­swer “no”.
> 
> All we know about the world teaches us that the ef­fects of A and B are al­ways dif­fer­ent—in some dec­i­mal place—for any A and B. Thus ask­ing “Are the ef­fects dif­fer­ent?” is fool­ish.
> 
> What we should be an­swer­ing first is “Can we tell the di­rec­tion in which the ef­fects dif­fer from the ef­fects of B?” In other words, can we be con­fi­dent about the di­rec­tion from A to B? Is it “up”, “down”, or “un­cer­tain”?

## Raftery1995

[“Bayesian Model Se­lec­tion in So­cial Re­search (with Dis­cus­sion by An­drew Gel­man & Don­ald B. Rubin, and Robert M. Hauser, and a Re­join­der)”](https://wwwlegacy.stat.washington.edu/research/online/1994/bic.ps), Raftery 1995:

> In the past 15 years, how­ever, some quan­ti­ta­tive so­ci­ol­o­gists have been at­tach­ing less im­por­tance to _p_ - values be­cause of prac­ti­cal dif­fi­cul­ties and counter- intuitive re­sults. These dif­fi­cul­ties are most ap­par­ent with large sam­ples, where _p_ - values tend to in­di­cate re­jec­tion of the null hy­poth­e­sis even when the null model seems rea­son­able the­o­ret­i­cally and in­spec­tion of the data fails to re­veal any strik­ing dis­crep­an­cies with it. Be­cause much so­ci­o­log­i­cal re­search is based on sur­vey data, often with thou­sands of cases, so­ci­ol­o­gists fre­quently come up against this prob­lem. In the early 1980s, some so­ci­ol­o­gists dealt with this prob­lem by ig­nor­ing the re­sults of _p_ - value-based tests when they seemed counter- intuitive, and by bas­ing model se­lec­tion in­stead on the­o­ret­i­cal con­sid­er­a­tions and in­for­mal as­sess­ment of dis­crep­an­cies be­tween model and data (eg. Fien­berg and Mason, 1979 46ya; Hout, 1983 42ya, 1984 41ya; Grusky and Hauser, 1984 41ya).
> 
> …It is clear that mod­els 1 and 2 are un­sat­is­fac­tory and should be re­jected in favor of model 3.3 By the stan­dard test, model 3 should also be re­jected, in favor of model 4, given the de­viance dif­fer­ence of 150 on 16 de­grees of free­dom, cor­re­spond­ing to a _p_ - value of about 10 −120. Grusky & Hauser 1984 nev­er­the­less adopted model 3 be­cause it ex­plains most (99.7%) of the de­viance under the base­line model of in­de­pen­dence, fits well in the sense that the dif­fer­ences be­tween ob­served and ex­pected counts are a small pro­por­tion of the total, and makes good the­o­ret­i­cal sense. This seems sen­si­ble, and yet is in dra­matic con­flict with the _p_ - value-based test. This type of con­flict often arises in large sam­ples, and hence is fre­quent in so­ci­ol­ogy with its sur­vey data sets com­pris­ing thou­sands of cases. The main re­sponse to it has been to claim that there is a dis­tinc­tion be­tween “sta­tis­ti­cal” and “sub­stan­tive” sig­nif­i­cance, with dif­fer­ences that are statistically- significant not nec­es­sar­ily being sub­stan­tively im­por­tant.

## Thompson1995

[⁠“Ed­i­to­r­ial Poli­cies Re­gard­ing Statistical- Significance Test­ing: 3 Sug­gested Re­forms” ⁠](https://files.eric.ed.gov/fulltext/ED392819.pdf#page=9), Thomp­son 1995:

> One se­ri­ous prob­lem with this sta­tis­ti­cal test­ing logic is that the in re­al­ity _H_ 0 is never true in the pop­u­la­tion, as rec­og­nized by any num­ber of promi­nent sta­tis­ti­cians ([⁠Tukey, 1991 ⁠](https://gwern.net/everything#tukey-1991)), ie. there will al­ways be some dif­fer­ences in pop­u­la­tion pa­ra­me­ters, al­though the dif­fer­ences may be in­cred­i­bly triv­ial. Near 40 years ago Sav­age ([⁠1957 ⁠](https://gwern.net/everything#savage-1957), pp. 332–333) noted that, “Null hy­pothe­ses of no dif­fer­ence are usu­ally known to be false be­fore the data are col­lected.” Sub­se­quently, Meehl (1978 47ya, p.822) ar­gued, “As I be­lieve is gen­er­ally rec­og­nized by sta­tis­ti­cians today and by thought­ful so­cial sci­en­tists, the null hy­poth­e­sis, taken lit­er­ally, is al­ways false.” Sim­i­larly, noted sta­tis­ti­cian [⁠Hays ⁠](https://gwern.net/everything#hays-1973) (1981 44ya, p. 293 [_Sta­tis­tics_], 3 rd ed.) pointed out that “[t]here is surely noth­ing on earth that is com­pletely in­de­pen­dent of any­thing else. The strength of as­so­ci­a­tion may ap­proach zero, but it should sel­dom or never be ex­actly zero.” And [⁠Lof­tus and Lof­tus ⁠](https://gwern.net/everything#loftus-loftus-1982) (1982 43ya, pp. 498–499) ar­gued that, “find­ing a ‘[sta­tis­ti­cally] sig­nif­i­cant ef­fect’ re­ally pro­vides very lit­tle in­for­ma­tion, be­cause it’s al­most cer­tain that some re­la­tion­ship (how­ever small) ex­ists be­tween any two vari­ables.” The very im­por­tant im­pli­ca­tion of all this is that statistical- significance test­ing pri­mar­ily be­comes only a test of re­searcher en­durance, be­cause “vir­tu­ally any study can be made to show [sta­tis­ti­cally] sig­nif­i­cant re­sults if one uses enough sub­jects” (Hays, 1981 44ya, p. 293). As [⁠Nun­nally ⁠](https://gwern.net/everything#nunnally-1960) (1960 65ya, p. 643) noted some 35 years ago, “If the null hy­poth­e­sis is not re­jected, it is usu­ally be­cause the _N_ is too small. If enough data are gath­ered, the hy­poth­e­sis will gen­er­ally be re­jected.” The im­pli­ca­tion is that:
> 
> > Statistical- significance test­ing can in­volve a tau­to­log­i­cal logic in which tired re­searchers, hav­ing col­lected data from hun­dreds of sub­jects, then con­duct a sta­tis­ti­cal test to eval­u­ate whether there were a lot of sub­jects, which the re­searchers al­ready know, be­cause they col­lected the data and know they’re tired. This tau­tol­ogy has cre­ated con­sid­er­able dam­age as re­gards the cu­mu­la­tion of knowl­edge… ([Thomp­son, 1992 ⁠](https://gwern.net/doc/psychology/1992-thompson.pdf), p. 436)

## Mulaik Et Al 1997

[“There Is a Time and a Place for Statistical- Significance Test­ing” ⁠](https://gwern.net/doc/statistics/causality/1997-muzaik.pdf), Mu­laik et al 1997 (in _What If There Were No Sig­nif­i­cance Tests_ ed Har­low et al 1997):

> Most of these ar­ti­cles ex­pose mis­con­cep­tions about sig­nif­i­cance test­ing com­mon among re­searchers and writ­ers of psy­cho­log­i­cal text­books on sta­tis­tics and mea­sure­ment. But the crit­i­cisms do not stop with mis­con­cep­tions about sig­nif­i­cance test­ing. Oth­ers like [⁠Meehl 1967 ⁠](https://gwern.net/everything#meehl-1967) ex­pose the lim­i­ta­tions of a sta­tis­ti­cal prac­tice that fo­cuses only on test­ing for zero dif­fer­ences be­tween means and zero cor­re­la­tions in­stead of test­ing pre­dic­tions about spe­cific nonzero val­ues for pa­ra­me­ters de­rived from the­ory or prior ex­pe­ri­ence, as is done in the phys­i­cal sci­ences. Still oth­ers em­pha­size that sig­nif­i­cance tests do not alone con­vey the in­for­ma­tion needed to prop­erly eval­u­ate re­search find­ings and per­form ac­cu­mu­la­tive re­search.
> 
> …Other than em­pha­siz­ing a need to prop­erly un­der­stand the in­ter­pre­ta­tion of [con­fi­dence in­ter­vals ⁠](https://en.wikipedia.org/wiki/Confidence_interval), we have no dis­agree­ments with these crit­i­cisms and pro­pos­als. But a few of the crit­ics go even fur­ther. In this chap­ter we will look at ar­gu­ments made by [Carver (1978 47ya) ⁠](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.120.780&rep=rep1&type=pdf), [⁠Cohen (1994 31ya) ⁠](https://gwern.net/everything#cohen-1994), [Roze­boom (1960 65ya) ⁠](https://pdfs.semanticscholar.org/b596/4787fc1abf739148d604abfbd2689e73e52f.pdf), Schmidt ([1992 ⁠](https://gwern.net/doc/statistics/meta-analysis/1992-schmidt.pdf), 1996 29ya), and Schmidt and Hunter (chap­ter 3 of this vol­ume), in favor of not merely rec­om­mend­ing the re­port­ing of point es­ti­mates of [ef­fect sizes ⁠](https://en.wikipedia.org/wiki/Effect_size) and con­fi­dence in­ter­vals based on them, but of aban­don­ing al­to­gether the use of sig­nif­i­cance tests in re­search. Our focus will be prin­ci­pally on Schmidt’s (1992 33ya, 1996 29ya) pa­pers, be­cause they in­cor­po­rate ar­gu­ments from ear­lier pa­pers, es­pe­cially Carver’s (1978 47ya), and also carry the ar­gu­ment to its most ex­treme con­clu­sions. Where ap­pro­pri­ate, we will also com­ment on Schmidt and Hunter’s (chap­ter 3 of this vol­ume) re­but­tal of ar­gu­ments against their po­si­tion.
> 
> **The Null Hy­poth­e­sis Is Al­ways False?**
> 
> Cohen (1994 31ya), in­flu­enced by Meehl (1978 47ya), ar­gued that “the nil hy­poth­e­sis is al­ways false” (p. 1000). Get a large enough sam­ple and you will al­ways re­ject the null hy­poth­e­sis. He cites a num­ber of em­i­nent sta­tis­ti­cians in sup­port of this view. He quotes Tukey (1991 34ya, p. 100) to the ef­fect that there are al­ways dif­fer­ences be­tween ex­per­i­men­tal treatments- for some dec­i­mal places. Cohen cites an un­pub­lished study by Meehl and Lykken in which cross tab­u­la­tions for 15 Min­nesota Mul­ti­pha­sic Per­son­al­ity In­ven­tory (MMPI) items for a sam­ple of 57,000 sub­jects yielded 105 chi- square tests of as­so­ci­a­tion and every one of them was sig­nif­i­cant, and 96% of them were sig­nif­i­cant at _p_ < 0.000001 (Cohen, 1994, p. 1000). Cohen cites Meehl (1990 35ya) as sug­gest­ing that this re­flects a “crud fac­tor” in na­ture. “Every­thing is re­lated to every­thing else” to some de­gree. So, the ques­tion is, why do a sig­nif­i­cance test if you know it will al­ways be sig­nif­i­cant if the sam­ple is large enough? But if this is an em­pir­i­cal hy­poth­e­sis, is it not one that is es­tab­lished using sig­nif­i­cance test­ing?
> 
> But the ex­am­ple may not be an apt demon­stra­tion of the prin­ci­ple Cohen sought to es­tab­lish: It is gen­er­ally ex­pected that re­sponses to dif­fer­ent items re­sponded to by the same sub­jects are not in­de­pen­dently dis­trib­uted across sub­jects, so it would not be re­mark­able to find sig­nif­i­cant cor­re­la­tions be­tween many such items.
> 
> Much more in­ter­est­ing would be to demon­strate sys­tem­atic and replic­a­ble sig­nif­i­cant treat­ment ef­fects when sub­jects are as­signed at ran­dom to dif­fer­ent treat­ment groups but the _same_ treat­ments are ad­min­is­tered to each group. But in this case, small but sig­nif­i­cant ef­fects in stud­ies with high power that de­vi­ate from ex­pec­ta­tions of no ef­fect when no dif­fer­ences in treat­ments are ad­min­is­tered are rou­tinely treated as sys­tem­atic ex­per­i­menter er­rors, and knowl­edge of ex­per­i­men­tal tech­nique is im­proved by their de­tec­tion and re­moval or con­trol. Sys­tem­atic error and ex­per­i­men­tal ar­ti­fact must al­ways be con­sid­ered a pos­si­bil­ity when re­ject­ing the null hy­poth­e­sis. Nev­er­the­less, do we know a pri­ori that a test will _al­ways_ be sig­nif­i­cant if the sam­ple is large enough? Is the propo­si­tion “Every sta­tis­ti­cal hy­poth­e­sis is false” an _axiom_ that needs no test­ing? Ac­tu­ally, we be­lieve that to re­gard this as an axiom would in­tro­duce an in­ter­nal con­tra­dic­tion into sta­tis­ti­cal rea­son­ing, com­pa­ra­ble to ar­gu­ing that all propo­si­tions and de­scrip­tions are false. You could not think and rea­son about the world with such an axiom. So it seems prefer­able to re­gard this as some kind of em­pir­i­cal gen­er­al­iza­tion. But no em­pir­i­cal gen­er­al­iza­tion is ever in­cor­ri­gi­ble and be­yond test­ing. Nev­er­the­less, if in­deed there is a phe­nom­e­non of na­ture known as “the crud fac­tor”, then it is some­thing we know to be ob­jec­tively a fact only be­cause of sig­nif­i­cance tests. Some­thing in the back­ground noise stands out as a sig­nal against that noise, be­cause we have suf­fi­ciently pow­er­ful tests using huge sam­ples to de­tect it. At that point it may be­come a chal­lenge to sci­ence to de­velop a bet­ter un­der­stand­ing of what pro­duces it. How­ever, it may tum out to re­flect only ex­per­i­menter ar­ti­fact. But in any case the hy­poth­e­sis of a crud fac­tor is not be­yond fur­ther test­ing.
> 
> The point is that it doesn’t mat­ter if the null hy­poth­e­sis is al­ways judged false at some sam­ple size, as long as we re­gard this as an em­pir­i­cal phe­nom­e­non. What mat­ters is whether _at the sam­ple size we have_ we can dis­tin­guish ob­served de­vi­a­tions from our hy­poth­e­sized val­ues to be suf­fi­ciently large and im­prob­a­ble under a hy­poth­e­sis of chance that we can treat them rea­son­ably but pro­vi­sion­ally as not due to chance error. There is no a pri­ori rea­son to be­lieve that one will al­ways re­ject the null hy­poth­e­sis at any given sam­ple size. On the other hand, ac­cept­ing the null hy­poth­e­sis does not mean the hy­poth­e­sized value is true, but rather that the ev­i­dence ob­served is not dis­tin­guish­able from what we would re­gard as due to chance if the null hy­poth­e­sis were true and thus is not suf­fi­cient to dis­prove it. The re­main­ing un­cer­tainty re­gard­ing the truth of our null hy­poth­e­sis is mea­sured by the width of the re­gion of ac­cep­tance or a func­tion of the stan­dard error. And this will be closely re­lated to the power of the test, which also pro­vides us with in­for­ma­tion about our un­cer­tainty. The fact that the width of the re­gion of ac­cep­tance shrinks with in­creas­ing sam­ple size, means we are able to re­duce our un­cer­tainty re­gard­ing the pro­vi­sional va­lid­ity of an ac­cepted null hy­poth­e­sis with larger sam­ples. In huge sam­ples the issue of un­cer­tainty due to chance looms not as im­por­tant as it does in small- and moderate- size sam­ples.

## Waller2004

[“The fal­lacy of the null hy­poth­e­sis in soft psy­chol­ogy” ⁠](https://www3.nd.edu/~ghaeffel/Waller2004%20Applied%20&%20Preventive%20Psychology.pdf), Waller 2004:

> In his clas­sic ar­ti­cle on the fal­lacy of the null hy­poth­e­sis in soft psy­chol­ogy, [⁠Paul Meehl ⁠](https://gwern.net/everything#meehl-1978) claimed that, in non­ex­per­i­men­tal set­tings, the prob­a­bil­ity of re­ject­ing the null hy­poth­e­sis of nil group dif­fer­ences in favor of a di­rec­tional al­ter­na­tive was 0.50—a value that is an order of mag­ni­tude higher than the cus­tom­ary Type I error rate. In a se­ries of real data sim­u­la­tions, using Min­nesota Mul­ti­pha­sic Per­son­al­ity Inventory- Revised (MMPI-2) data col­lected from more than 80,000 in­di­vid­u­als, I found strong sup­port for Meehl’s claim.
> 
> …Be­fore run­ning the ex­per­i­ments I re­al­ized that, to be fair to Meehl, I needed a large data set with a broad range of bioso­cial vari­ables. For­tu­nately, I had ac­cess to data from 81,485 in­di­vid­u­als who ear­lier had com­pleted the 567 items of the Min­nesota Mul­ti­pha­sic Per­son­al­ity Inventory- Revised (MMPI-2; Butcher, Dahlstrom, Gra­ham, Tel­le­gen, & Kaem­mer, 1989 36ya). The MMPI-2, in my opin­ion, is an ideal ve­hi­cle for test­ing Meehl’s claim be­cause it in­cludes items in such var­ied con­tent do­mains as gen­eral health con­cerns; per­sonal habits and in­ter­ests; at­ti­tudes to­wards sex, mar­riage, and fam­ily; af­fec­tive func­tion­ing; nor­mal range per­son­al­ity; and ex­treme man­i­fes­ta­tions of psy­chopathol­ogy (for a more com­plete de­scrip­tion of the la­tent con­tent of the MMPI, see Waller, 1999 26ya, “Search­ing for struc­ture in the MMPI”).
> 
> …Next, the com­puter se­lected (with­out re­place­ment) a ran­dom item from the pool of MMPI-2 items. Using data from the 41,491 males and 39,994 fe­males, it then (1) per­formed a dif­fer­ence of pro­por­tions test on the item group means; (2) recorded the signed _z_ - value; and (3) recorded the as­so­ci­ated sig­nif­i­cance level. Fi­nally, the pro­gram tal­lied the num­ber of “sig­nif­i­cant” test re­sults (ie. those with | _z_ |≥1.96). The re­sults of this mini sim­u­la­tion were en­light­en­ing and in ex­cel­lent ac­cord with the out­come of Meehl’s _gedanken_ ex­per­i­ment. Specif­i­cally, 46% of the di­rec­tional hy­pothe­ses were sup­ported at sig­nif­i­cance lev­els that far ex­ceeded tra­di­tional _p_ - value cut­offs. A sum­mary of the re­sults is por­trayed in Fig. 1. No­tice in this fig­ure, which dis­plays the dis­tri­b­u­tion of _z_ - values for the 511 tests, that many of the item mean dif­fer­ences were 50–100 times larger than their as­so­ci­ated stan­dard er­rors!
> 
> ![Figure 1: Distribution of z -values for 511 hypothesis tests.](https://gwern.net/doc/statistics/2004-waller-figure1.png)
> 
> **Fig­ure 1**: Dis­tri­b­u­tion of _z_ - values for 511 hy­poth­e­sis tests.
> 
> ![Figure 2: Distribution of the frequency of rejected null hypotheses, in favor of a randomly chosen directional alternative, in 320,922 hypothesis test.](https://gwern.net/doc/statistics/2004-waller-figure2.png)
> 
> **Fig­ure 2**: Dis­tri­b­u­tion of the fre­quency of re­jected null hy­pothe­ses, in favor of a ran­domly cho­sen di­rec­tional al­ter­na­tive, in 320,922 hy­poth­e­sis test.

Waller also high­lights Bill Thomp­son’s 2001 bib­li­og­ra­phy [⁠“402 Ci­ta­tions Ques­tion­ing the In­dis­crim­i­nate Use of Null Hy­poth­e­sis Sig­nif­i­cance Tests in Ob­ser­va­tional Stud­ies” ⁠](https://gwern.net/doc/statistics/causality/2001-thompson.html) as a source for crit­i­cisms of NHST but un­for­tu­nately it’s un­clear which of them might bear on the spe­cific crit­i­cism of ‘the null hy­poth­e­sis is al­ways false’.

## Starbuck2006

[_The Pro­duc­tion of Knowl­edge: The Chal­lenge of So­cial Sci­ence Re­search_ ⁠](https://www.amazon.com/Production-Knowledge-Challenge-Science-Research/dp/0199288534), Star­buck 2006, pg47–49:

> In­duc­tion re­quires dis­tin­guish­ing mean­ing­ful re­la­tion­ships (sig­nals) in the midst of an ob­scur­ing back­ground of [con­found­ing ⁠](https://en.wikipedia.org/wiki/Confounding) re­la­tion­ships (noise). The weak and mean­ing­less or sub­stan­tively sec­ondary cor­re­la­tions in the back­ground make in­duc­tion un­trust­wor­thy. In many tasks, peo­ple can dis­tin­guish weak sig­nals against rather strong back­ground noise. The rea­son is that both the sig­nals and the back­ground noise match fa­mil­iar pat­terns. For ex­am­ple, a dri­ver trav­el­ing to a fa­mil­iar des­ti­na­tion fo­cuses on land­marks that ex­pe­ri­ence has shown to be rel­e­vant. Peo­ple have trou­ble mak­ing such dis­tinc­tions where sig­nals and noise look much alike or where sig­nals and noise have un­fa­mil­iar char­ac­ter­is­tics. For ex­am­ple, a dri­ver trav­el­ing a new road to a new des­ti­na­tion is likely to have dif­fi­culty spot­ting land­marks and turns on a rec­om­mended route.
> 
> So­cial sci­ence re­search has the lat­ter char­ac­ter­is­tics. This ac­tiv­ity is called re­search be­cause its out­puts are un­known; and the sig­nals and noise look a lot alike in that both have sys­tem­atic com­po­nents and both con­tain com­po­nents that vary er­rat­i­cally. There­fore, re­searchers rely upon sta­tis­ti­cal tech­niques to dis­tin­guish sig­nals from noise. How­ever, these tech­niques as­sume: (1) that the so- called ran­dom er­rors re­ally do can­cel each other out so that their av­er­age val­ues are close to zero; and (2) that the so- called ran­dom er­rors in dif­fer­ent vari­ables are un­cor­re­lated. These are very strong as­sump­tions be­cause they pre­sume that the re­searchers’ hy­pothe­ses en­com­pass ab­solutely all of the sys­tem­atic ef­fects in the data, in­clud­ing ef­fects that the re­searchers have not fore­seen or mea­sured. When these as­sump­tions are not met, the sta­tis­ti­cal tech­niques tend to mis­take noise for sig­nal, and to at­tribute more im­por­tance to the re­searchers’ hy­pothe­ses than they de­serve.
> 
> I re­mem­bered what [Ames & Re­iter 1961 ⁠](https://gwern.net/doc/economics/1961-ames.pdf) had said about how easy it is for macro­econ­o­mists to dis­cover statistically- significant cor­re­la­tions that have no sub­stan­tive sig­nif­i­cance, and I could see 5 rea­sons why a sim­i­lar phe­nom­e­non might occur with cross- sectional data. Firstly, a few broad char­ac­ter­is­tics of peo­ple and so­cial sys­tems per­vade so­cial sci­ence data—ex­am­ples being sex, age, in­tel­li­gence, so­cial class, in­come, ed­u­ca­tion, or or­ga­ni­za­tion size. Such char­ac­ter­is­tics cor­re­late with many be­hav­iors and with each other. Sec­ondly, re­searchers’ de­ci­sions about how to treat data can cre­ate cor­re­la­tions be­tween vari­ables. For ex­am­ple, when the Aston re­searchers used fac­tor analy­sis to cre­ate ag­gre­gate vari­ables, they im­plic­itly de­ter­mined the cor­re­la­tions among these ag­gre­gate vari­ables. Thirdly, so- called ‘sam­ples’ are fre­quently not ran­dom, and many of them are com­plete sub­pop­u­la­tions—say, every em­ployee of a com­pany—even though study after study has turned up ev­i­dence that peo­ple who live close to­gether, who work to­gether, or who so­cial­ize to­gether tend to have more at­ti­tudes, be­liefs, and be­hav­iors in com­mon than do peo­ple who are far apart phys­i­cally and so­cially. Fourthly, some stud­ies ob­tain data from re­spon­dents at one time and through one method. By in­clud­ing items in a sin­gle ques­tion­naire or in­ter­view, re­searchers sug­gest to re­spon­dents that re­la­tion­ships exist among these items. Lastly, most re­searchers are in­tel­li­gent peo­ple who are liv­ing suc­cess­ful lives. They are likely to have some in­tu­itive abil­ity to pre­dict the be­hav­iors of peo­ple and of so­cial sys­tems. They are much more likely to for­mu­late hy­pothe­ses that ac­cord with their in­tu­ition than ones that vi­o­late it; they are quite likely to in­ves­ti­gate cor­re­la­tions and dif­fer­ences that de­vi­ate from zero; and they are less likely than chance would imply to ob­serve cor­re­la­tions and dif­fer­ences near zero.
> 
> [⁠Web­ster and I hy­poth­e­sized ⁠](https://gwern.net/doc/economics/1988-webster.html) that sta­tis­ti­cal tests with a null hy­poth­e­sis of no cor­re­la­tion are bi­ased to­ward statistical- significance. Web­ster culled through _Ad­min­is­tra­tive Sci­ence Quar­terly_, the _Acad­emy of Man­age­ment Jour­nal_, and the _Jour­nal of Ap­plied Psy­chol­ogy_ seek­ing ma­tri­ces of cor­re­la­tions. She tab­u­lated only com­plete ma­tri­ces of cor­re­la­tions in order to ob­serve the re­la­tions among all of the vari­ables that the re­searchers per­ceived when draw­ing in­duc­tive in­fer­ences, not only those vari­ables that re­searchers ac­tu­ally in­cluded in hy­pothe­ses. Of course, some re­searchers prob­a­bly gath­ered data on ad­di­tional vari­ables be­yond those pub­lished, and then omit­ted these ad­di­tional vari­ables be­cause they cor­re­lated very weakly with the de­pen­dent vari­ables. We es­ti­mated that 64% of the cor­re­la­tions in our data were as­so­ci­ated with re­searchers’ hy­pothe­ses.
> 
> ![Figure 2.6: Correlations reported in 3 journals](https://gwern.net/doc/statistics/2006-starbuck-websterstarbuck1988-figure26-managementsciencecorrelations.jpg)
> 
> **Fig­ure 2.6**: Cor­re­la­tions re­ported in 3 jour­nals
> 
> Fig­ure 2.6 shows the dis­tri­b­u­tions of 14,897 cor­re­la­tions. In all 3 jour­nals, both the mean cor­re­la­tion and the me­dian cor­re­la­tion were close to +0.09 and the dis­tri­b­u­tions of cor­re­la­tions were very sim­i­lar. Find­ing sig­nif­i­cant cor­re­la­tions is ab­surdly easy in this pop­u­la­tion of vari­ables, es­pe­cially when re­searchers make two- tailed tests with a null hy­poth­e­sis of no cor­re­la­tion. Choos­ing two vari­ables ut­terly at ran­dom, a re­searcher has 2-to-1 odds of find­ing a sig­nif­i­cant cor­re­la­tion on the first try, and 24-to-1 odds of find­ing a sig­nif­i­cant cor­re­la­tion within 3 tries (also see [Hub­bard and Arm­strong 1992 ⁠](https://web-archive.southampton.ac.uk/cogprints.org/5178/1/are_null_results_becoming.pdf)). Fur­ther­more, the odds are bet­ter than 2-to-1 that an ob­served cor­re­la­tion will be pos­i­tive, and pos­i­tive cor­re­la­tions are more likely than neg­a­tive ones to be statistically- significant. Be­cause re­searchers gather more data when they are get­ting small cor­re­la­tions, stud­ies with large num­bers of ob­ser­va­tions ex­hibit slightly less pos­i­tive bias. The mean cor­re­la­tion in stud­ies with fewer than 70 ob­ser­va­tions is about twice the mean cor­re­la­tion in stud­ies with over 180 ob­ser­va­tions. The main in­fer­ence I drew from these sta­tis­tics was that the so­cial sci­ences are drown­ing in statistically- significant but mean­ing­less noise. Be­cause the dif­fer­ences and cor­re­la­tions that so­cial sci­en­tists test have dis­tri­b­u­tions quite dif­fer­ent from those as­sumed in hy­poth­e­sis tests, so­cial sci­en­tists are using tests that as­sign statistical- significance to con­found­ing back­ground re­la­tion­ships. Be­cause so­cial sci­en­tists equate statistical- significance with mean­ing­ful re­la­tion­ships, they often mis­take con­found­ing back­ground re­la­tion­ships for the­o­ret­i­cally im­por­tant in­for­ma­tion. One re­sult is that so­cial sci­ence re­search cre­ates a cloud of statistically- significant dif­fer­ences and cor­re­la­tions that not only have no real mean­ing but also im­pede sci­en­tific progress by ob­scur­ing the truly mean­ing­ful re­la­tion­ships.
> 
> Sup­pose that roughly 10% of all ob­serv­able re­la­tions could be the­o­ret­i­cally mean­ing­ful and that the re­main­ing 90% ei­ther have no mean­ings or can be de­duced as im­pli­ca­tions of the key 10%. How­ever, we do not know now which re­la­tions con­sti­tute the key 10%, and so our re­search re­sem­bles a search through a haystack in which we are try­ing to sep­a­rate nee­dles from more nu­mer­ous straws. Now sup­pose that we adopt a search method that makes al­most every straw look very much like a nee­dle and that turns up thou­sands of ap­par­ent nee­dles an­nu­ally; 90% of these ap­par­ent nee­dles are ac­tu­ally straws, but we have no way of know­ing which ones. Next, we fab­ri­cate a the­ory that ‘ex­plains’ these ap­par­ent nee­dles. Some of the propo­si­tions in our the­ory are likely to be cor­rect, merely by chance; but many, many more propo­si­tions are in­cor­rect or mis­lead­ing in that they de­scribe straws. Even if this the­ory were to ac­count ra­tio­nally for all of the nee­dles that we have sup­pos­edly dis­cov­ered in the past, which is ex­tremely un­likely, the the­ory has very lit­tle chance of mak­ing highly ac­cu­rate pre­dic­tions about the con­se­quences of our ac­tions un­less the the­ory it­self acts as a pow­er­ful self- fulfilling prophecy (Eden and Ravid 1982). Our the­ory would make some cor­rect pre­dic­tions, of course, be­cause with so many cor­re­lated vari­ables, even a com­pletely false the­ory would have a rea­son­able chance of gen­er­at­ing pre­dic­tions that come true. Thus, we dare not even take cor­rect pre­dic­tions as de­pend­able ev­i­dence of our the­ory’s cor­rect­ness ([⁠Deese 1972: 61–67 ⁠](https://archive.org/details/psychologyasscie0000dees) [_Psy­chol­ogy as Sci­ence and Art_]).

## Smith Et Al 2007

[“Clus­tered En­vi­ron­ments and Ran­dom­ized Genes: A Fun­da­men­tal Dis­tinc­tion be­tween Con­ven­tional and Ge­netic Epi­demi­ol­ogy” ⁠](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0040352), Smith et al 2007:

> …We ex­am­ined the ex­tent to which ge­netic vari­ants, on the one hand, and non­genetic en­vi­ron­men­tal ex­po­sures or phe­no­typic char­ac­ter­is­tics on the other, tend to be as­so­ci­ated with each other, to as­sess the de­gree of con­found­ing that would exist in con­ven­tional epi­demi­o­log­i­cal stud­ies com­pared with [Mendelian Ran­dom­iza­tion ⁠](https://en.wikipedia.org/wiki/Mendelian_randomization) stud­ies. _Meth­ods & Find­ings_: We es­ti­mated pair­wise cor­re­la­tions be­tween [96] non­genetic base­line vari­ables and ge­netic vari­ables in a cross- sectional study [British Women’s Heart and Health Study; _n_ = 4,286] com­par­ing the num­ber of cor­re­la­tions that were statistically- significant at the 5%, 1%, and 0.01% level (α = 0.05, 0.01, and 0.0001, re­spec­tively) with the num­ber ex­pected by chance if all vari­ables were in fact un­cor­re­lated, using a two- sided bi­no­mial exact test. We demon­strate that be­hav­ioral, so­cioe­co­nomic, and phys­i­o­log­i­cal fac­tors are strongly in­ter­re­lated, with 45% of all pos­si­ble pair­wise as­so­ci­a­tions be­tween 96 non­genetic char­ac­ter­is­tics (_n_ = 4,560 cor­re­la­tions) being sig­nif­i­cant at the _p_ < 0.01 level (the ratio of ob­served to ex­pected sig­nif­i­cant as­so­ci­a­tions was 45; _p_ - value for dif­fer­ence be­tween ob­served and ex­pected < 0.000001). Sim­i­lar find­ings were ob­served for other lev­els of sig­nif­i­cance.
> 
> …The 96 non­genetic vari­ables gen­er­ated 4,560 pair­wise com­par­isons, of which, as­sum­ing no as­so­ci­a­tions ex­isted, 5 in 100 (total 228) would be ex­pected to be as­so­ci­ated by chance at the 5% sig­nif­i­cance level (α = 0.05). How­ever, 2,447 (54%) of the cor­re­la­tions were sig­nif­i­cant at the α = 0.05 level, giv­ing an ob­served to ex­pected (O:E) ratio of 11, _p_ for dif­fer­ence O:E < 0.000001 (Table 1). At the 1% sig­nif­i­cance level, 45.6 of the cor­re­la­tions would be ex­pected to be as­so­ci­ated by chance, but we found that 2,036 (45%) of the pair­wise as­so­ci­a­tions were statistically- significant at α = 0.01, giv­ing an O:E ratio of 45, _p_ for dif­fer­ence O:E < 0.000001 (Table 2). At the 0.01% sig­nif­i­cance level, 0.456 of the cor­re­la­tions would be ex­pected to be as­so­ci­ated by chance, but we found that 1,378 (30%) were statistically- significantly as­so­ci­ated at α = 0.0001, giv­ing an O:E ratio of 3,022, _p_ for dif­fer­ence O:E < 0.000001.
> 
> …Over 50% of the pair­wise as­so­ci­a­tions be­tween base­line non­genetic char­ac­ter­is­tics in our study were statistically- significant at the 0.05 level; an 11-fold in­crease from what would be ex­pected, as­sum­ing these char­ac­ter­is­tics were in­de­pen­dent. Sim­i­lar find­ings were found for statistically- significant as­so­ci­a­tions at the 0.01 level (45-fold in­crease from ex­pected) and the 0.0001 level (3,000-fold in­crease from ex­pected). This il­lus­trates the con­sid­er­able dif­fi­culty of de­ter­min­ing which as­so­ci­a­tions are valid and po­ten­tially causal from a back­ground of highly cor­re­lated fac­tors, re­flect­ing that be­hav­ioral, so­cioe­co­nomic, and phys­i­o­log­i­cal char­ac­ter­is­tics tend to clus­ter. This ten­dency will mean that there will often be high lev­els of con­found­ing when study­ing any sin­gle fac­tor in re­la­tion to an out­come. Given the com­plex­ity of such con­found­ing, even after for­mal sta­tis­ti­cal ad­just­ment, a lack of data for some con­founders, and mea­sure­ment error in as­sessed con­founders will leave con­sid­er­able scope for [resid­ual con­found­ing ⁠](https://gwern.net/doc/statistics/bayes/regression-to-mean/index) [[4] ⁠](https://gwern.net/doc/statistics/causality/1992-phillips.pdf). When epi­demi­o­log­i­cal stud­ies present ad­justed as­so­ci­a­tions as a re­flec­tion of the mag­ni­tude of a causal as­so­ci­a­tion, they are as­sum­ing that all pos­si­ble con­found­ing fac­tors have been ac­cu­rately mea­sured and that their re­la­tion­ships with the out­come have been ap­pro­pri­ately mod­elled. We think this is un­likely to be the case in most ob­ser­va­tional epi­demi­o­log­i­cal stud­ies [[26] ⁠](https://gwern.net/doc/statistics/causality/1991-phillips.pdf).
> 
> Pre­dictably, such con­founded re­la­tion­ships will be par­tic­u­larly marked for highly so­cially and cul­tur­ally pat­terned risk fac­tors, such as di­etary in­take. This high de­gree of con­found­ing might un­der­lie the poor con­cor­dance of ob­ser­va­tional epi­demi­o­log­i­cal stud­ies that iden­ti­fied di­etary fac­tors (such as beta carotene, vi­t­a­min E, and vi­t­a­min C in­take) as pro­tec­tive against car­dio­vas­cu­lar dis­ease and can­cer, with the find­ings of [ran­dom­ized con­trolled tri­als ⁠](https://en.wikipedia.org/wiki/Randomized_controlled_trial) of these di­etary fac­tors [1,27]. In­deed, with 45% of the pair­wise as­so­ci­a­tions of non­genetic char­ac­ter­is­tics being “statistically- significant” at the _p_ < 0.01 level in our study, and our study being un­ex­cep­tional with re­gard to the lev­els of con­found­ing that will be found in ob­ser­va­tional in­ves­ti­ga­tions, it is clear that the large ma­jor­ity of as­so­ci­a­tions that exist in ob­ser­va­tional data­bases will not reach pub­li­ca­tion. We sug­gest that those that do achieve pub­li­ca­tion will re­flect ap­par­ent bi­o­log­i­cal plau­si­bil­ity (a weak causal cri­te­rion [[28] ⁠](https://gwern.net/doc/statistics/causality/1992-smith.pdf)) and the in­ter­ests of in­ves­ti­ga­tors. Ex­am­ples exist of in­ves­ti­ga­tors re­port­ing pro­vi­sional analy­ses in ab­stracts—such as an­tiox­i­dant vi­t­a­min in­take being ap­par­ently pro­tec­tive against fu­ture car­dio­vas­cu­lar events in women with clin­i­cal ev­i­dence of car­dio­vas­cu­lar dis­ease [29]—but not going on to full pub­li­ca­tion of these find­ings, per­haps be­cause ran­dom­ized con­trolled tri­als ap­peared soon after the pre­sen­ta­tion of the ab­stracts [30] that ren­dered their find­ings as being un­likely to re­flect causal re­la­tion­ships. Con­versely, it is likely that the large ma­jor­ity of null find­ings will not achieve pub­li­ca­tion, un­less they con­tra­dict high- profile prior find­ings, as has been demon­strated in mol­e­c­u­lar ge­netic re­search [31].
> 
> ![https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0040352#pmed-0040352-g001](https://gwern.net/doc/genetics/heritable/correlation/2007-smith-figure1-correlationdistribution.jpg)
> 
> **Fig­ure 1**: His­togram of Statistically- Significant (at α = 1%) Age- Adjusted Pair­wise Cor­re­la­tion Co­ef­fi­cients be­tween 96 Non­genetic Char­ac­ter­is­tics. British Women Aged 60–79 y
> 
> The mag­ni­tudes of most of the sig­nif­i­cant cor­re­la­tions be­tween non­genetic char­ac­ter­is­tics were small (see Fig­ure 1), with a me­dian value at _p_ ≤ 0.01 and _p_ ≤ 0.05 of 0.08, and it might be con­sid­ered that such weak as­so­ci­a­tions are un­likely to be im­por­tant sources of con­found­ing. How­ever, so many as­so­ci­ated non­genetic vari­ables, even with weak cor­re­la­tions, can present a very im­por­tant po­ten­tial for resid­ual con­found­ing. For ex­am­ple, we have pre­vi­ously demon­strated how 15 so­cioe­co­nomic and be­hav­ioral risk fac­tors, each with weak but sta­tis­ti­cally in­de­pen­dent (at _p_ ≤ 0.05) as­so­ci­a­tions with both vi­t­a­min C lev­els and coro­nary heart dis­ease (CHD), could to­gether ac­count for an ap­par­ent strong pro­tec­tive ef­fect (odds ratio = 0.60 com­par­ing top to bot­tom quar­ter of vi­t­a­min C dis­tri­b­u­tion) of vi­t­a­min C on CHD ([⁠32 ⁠](https://www.thelancet.com/journals/lancet/article/PIIS0140-6736\(04\)16925-0/fulltext) [see also [Lawlor et al 2004b ⁠](https://gwern.net/doc/statistics/causality/2004-lawlor.pdf)]).

## Hecht & Moxley2009

[“Ter­abytes of To­bler: eval­u­at­ing the first law in a mas­sive, domain- neutral rep­re­sen­ta­tion of world knowl­edge” ⁠](https://brenthecht.com/papers/cosit2009.pdf), Hecht & Mox­ley 2009:

> The First Law of Ge­og­ra­phy states, “every­thing is re­lated to every­thing else, but near things are more re­lated than dis­tant things.” De­spite the fact that it is to a large de­gree what makes “spa­tial spe­cial”, the law has never been em­pir­i­cally eval­u­ated on a large, domain- neutral rep­re­sen­ta­tion of world knowl­edge. We ad­dress the gap in the lit­er­a­ture about this crit­i­cal idea by sta­tis­ti­cally ex­am­in­ing the mul­ti­tude of en­ti­ties and re­la­tions be­tween en­ti­ties present across 22 dif­fer­ent lan­guage edi­tions of Wikipedia. We find that, at least ac­cord­ing to the myr­iad au­thors of Wikipedia, the First Law is true to an over­whelm­ing ex­tent re­gard­less of language- defined cul­tural do­main.

## Andrew Gelman

## Gelman2004

[“Type 1, type 2, type S, and type M er­rors” ⁠](https://statmodeling.stat.columbia.edu/2004/12/29/type_1_type_2_t/)

> I’ve never in my pro­fes­sional life made a Type I error _or_ a Type II error. But I’ve made lots of er­rors. How can this be?
> 
> A Type 1 error oc­curs only if the null hy­poth­e­sis is true (typ­i­cally if a cer­tain pa­ra­me­ter, or dif­fer­ence in pa­ra­me­ters, equals zero). In the ap­pli­ca­tions I’ve worked on, in so­cial sci­ence and pub­lic health, I’ve never come across a null hy­poth­e­sis that could ac­tu­ally be true, or a pa­ra­me­ter that could ac­tu­ally be zero.

## Gelman2007

[“Sig­nif­i­cance test­ing in eco­nom­ics: Mc­Closkey, Zil­iak, Hoover, and Siegler” ⁠](https://statmodeling.stat.columbia.edu/2007/10/05/significance_te/):

> I think that Mc­Closkey and Zil­iak, and also Hoover and Siegler, would agree with me that the null hy­poth­e­sis of zero co­ef­fi­cient is es­sen­tially al­ways false. (The par­a­dig­matic ex­am­ple in eco­nom­ics is pro­gram eval­u­a­tion, and I think that just about every pro­gram being se­ri­ously con­sid­ered will have ef­fects—pos­i­tive for some peo­ple, neg­a­tive for oth­ers—but not av­er­ag­ing to ex­actly zero in the pop­u­la­tion.) From this per­spec­tive, the point of hy­poth­e­sis test­ing (or, for that mat­ter, of con­fi­dence in­ter­vals) is not to as­sess the null hy­poth­e­sis but to give a sense of the un­cer­tainty in the in­fer­ence. As Hoover and Siegler put it, “while the eco­nomic sig­nif­i­cance of the co­ef­fi­cient does not de­pend on the statistical- significance, our cer­tainty about the ac­cu­racy of the mea­sure­ment surely does.... Sig­nif­i­cance tests, prop­erly used, are a tool for the as­sess­ment of sig­nal strength and not mea­sures of eco­nomic sig­nif­i­cance.” Cer­tainly, I’d rather see an es­ti­mate with an as­sess­ment of statistical- significance than an es­ti­mate with­out such an as­sess­ment.

## Gelman2010a

[“Bayesian Sta­tis­tics Then and Now” ⁠](https://stat.columbia.edu/~gelman/research/published/gelman_discussion_of_efron.pdf), Gel­man 2010a:

> My third meta- principle is that _dif­fer­ent ap­pli­ca­tions de­mand dif­fer­ent philoso­phies_. This prin­ci­ple comes up for me in Efron’s dis­cus­sion of hy­poth­e­sis test­ing and the so- called false dis­cov­ery rate, which I label as “so- called” for the fol­low­ing rea­son. In Efron’s for­mu­la­tion (which fol­lows the clas­si­cal [mul­ti­ple com­par­isons ⁠](https://en.wikipedia.org/wiki/Multiple_comparisons_problem) lit­er­a­ture), a “false dis­cov­ery” is a zero ef­fect that is iden­ti­fied as nonzero, whereas, in my own work, I never study zero ef­fects. The ef­fects I study are some­times small but it would be silly, for ex­am­ple, to sup­pose that the dif­fer­ence in vot­ing pat­terns of men and women (after con­trol­ling for some other vari­ables) could be ex­actly zero. My prob­lems with the “false dis­cov­ery” for­mu­la­tion are partly a mat­ter of taste, I’m sure, but I be­lieve they also arise from the dif­fer­ence be­tween prob­lems in ge­net­ics (in which some genes re­ally have es­sen­tially zero ef­fects on some traits, so that the clas­si­cal hypothesis- testing model is plau­si­ble) and in so­cial sci­ence and en­vi­ron­men­tal health (where es­sen­tially every­thing is con­nected to every­thing else, and ef­fect sizes fol­low a con­tin­u­ous dis­tri­b­u­tion rather than a mix of large ef­fects and near- exact ze­roes).

## Gelman2010b

[“Causal­ity and Sta­tis­ti­cal Learn­ing” ⁠](http://www.stat.columbia.edu/~gelman/research/published/causalreview4.pdf), Gel­man 2010b:

> _There are (al­most) no true ze­roes: dif­fi­cul­ties with the re­search pro­gram of learn­ing causal struc­ture_
> 
> We can dis­tin­guish be­tween learn­ing within a causal model (that is, in­fer­ence about pa­ra­me­ters char­ac­ter­iz­ing a spec­i­fied di­rected graph) and learn­ing causal struc­ture it­self (that is, in­fer­ence about the graph it­self). In so­cial sci­ence re­search, I am ex­tremely skep­ti­cal of this sec­ond goal.
> 
> The dif­fi­culty is that, in so­cial sci­ence, there are no true ze­roes. For ex­am­ple, re­li­gious at­ten­dance is as­so­ci­ated with at­ti­tudes on eco­nomic as well as so­cial is­sues, and both these cor­re­la­tions vary by state. And it does not in­ter­est me, for ex­am­ple, to test a model in which so­cial class af­fects vote choice through party iden­ti­fi­ca­tion but not along a di­rect path.
> 
> More gen­er­ally, any­thing that plau­si­bly could have an ef­fect will not have an ef­fect that is ex­actly zero. I can re­spect that some so­cial sci­en­tists find it use­ful to frame their re­search in terms of con­di­tional in­de­pen­dence and the test­ing of null ef­fects, but I don’t gen­er­ally find this ap­proach help­ful—and I cer­tainly don’t be­lieve that it is nec­es­sary to think in terms of con­di­tional in­de­pen­dence in order to study causal­ity. With­out struc­tural ze­roes, it is im­pos­si­ble to iden­tify graph­i­cal [struc­tural equa­tion mod­els ⁠](https://en.wikipedia.org/wiki/Structural_equation_modeling).
> 
> The most com­mon ex­cep­tions to this rule, as I see it, are in­de­pen­dences from de­sign (as in a de­signed or [nat­ural ex­per­i­ment ⁠](https://en.wikipedia.org/wiki/Natural_experiment)) or ef­fects that are zero based on a plau­si­ble sci­en­tific hy­poth­e­sis (as might arise, for ex­am­ple, in ge­net­ics where genes on dif­fer­ent chro­mo­somes might have es­sen­tially in­de­pen­dent ef­fects), or in a study of ESP. In such set­tings I can see the value of test­ing a null hy­poth­e­sis of zero ef­fect, ei­ther for its own sake or to rule out the pos­si­bil­ity of a con­di­tional cor­re­la­tion that is sup­posed not to be there.
> 
> An­other sort of ex­cep­tion to the “no ze­roes” rule comes from in­for­ma­tion re­stric­tion: a per­son’s de­ci­sion should not be af­fected by knowl­edge that he or she doesn’t have. For ex­am­ple, a con­sumer in­ter­ested in buy­ing ap­ples cares about the total price he pays, not about how much of that goes to the seller and how much goes to the gov­ern­ment in the form of taxes. So the re­stric­tion is that the util­ity de­pends on prices, not on the share of that going to taxes. That is the type of re­stric­tion that can help iden­tify de­mand func­tions in eco­nom­ics.
> 
> I re­al­ize, how­ever, that my per­spec­tive that there are no ze­roes (in­for­ma­tion re­stric­tions aside) is a mi­nor­ity view among so­cial sci­en­tists and per­haps among peo­ple in gen­eral, on the ev­i­dence of psy­chol­o­gist Slo­man’s book. For ex­am­ple, from chap­ter 2: “A good politi­cian will know who is mo­ti­vated by greed and who is mo­ti­vated by larger prin­ci­ples in order to dis­cern how to so­licit each one’s vote when it is needed.” I can well be­lieve that peo­ple think in this way but I don’t buy it! Just about every­one is mo­ti­vated by greed and by larger prin­ci­ples! This sort of dis­crete think­ing doesn’t seem to me to be at all re­al­is­tic about how peo­ple behave- although it might very well be a good model about how peo­ple char­ac­ter­ize oth­ers!
> 
> In the next chap­ter, Slo­man writes, “No mat­ter how many times A and B occur to­gether, mere co- occurrence can­not re­veal _whether_ A causes B, or B causes A, or some­thing else causes both.” [ital­ics added] Again, I am both­ered by this sort of dis­crete think­ing. I will re­turn in a mo­ment with an ex­am­ple, but just to speak gen­er­ally, if A _could_ cause B, and B _could_ cause A, then I would think that, yes, they could cause each other. And if some­thing else _could_ cause them both, I imag­ine that could be hap­pen­ing along with the cau­sa­tion of A on B and of B on A.
> 
> Here we’re get­ting into some of the dif­fer­ences be­tween a nor­ma­tive view of sci­ence, a de­scrip­tive view of sci­ence, and a de­scrip­tive view of how peo­ple per­ceive the world. Just as there are lim­its to what “folk physics” can tell us about the mo­tion of par­ti­cles, sim­i­larly I think we have to be care­ful about too closely iden­ti­fy­ing “folk causal in­fer­ence” from the stuff done by the best so­cial sci­en­tists. To con­tinue the anal­ogy: it is in­ter­est­ing to study how we de­velop phys­i­cal in­tu­itions using com­mon­sense no­tions of force, en­ergy, mo­men­tum, and so on—but it’s also im­por­tant to see where these in­tu­itions fail. Sim­i­larly, ideas of causal­ity are fun­da­men­tal but that doesn’t stop or­di­nary peo­ple and even ex­perts from mak­ing basic mis­takes.
> 
> Now I would like to re­turn to the graph­i­cal model ap­proach de­scribed by Slo­man. In chap­ter 5, he dis­cusses an ex­am­ple with 3 vari­ables:
> 
> > If two of the vari­ables are de­pen­dent, say, in­tel­li­gence and so­cioe­co­nomic sta­tus, but con­di­tion­ally in­de­pen­dent given the third vari­able [beer con­sump­tion], then ei­ther they are re­lated by one of two chains:
> > 
> > ```
> > (Intelligence → Amount of beer consumed → Socioeconomic status)
> > (Socio-economic status → Amount of beer consumed → Intelligence)
> > ```
> > 
> > or by a fork:
> > 
> > ```
> > Socioeconomic status
> >                          ⤴
> >  Amount of beer consumed
> >                          ⤵
> >                            Intelligence
> > ```
> > 
> > and then we must use some other means [other than ob­ser­va­tional data] to de­cide be­tween these 3 pos­si­bil­i­ties. In some cases, com­mon sense may be suf­fi­cient, but we can also, if nec­es­sary, run an ex­per­i­ment. If we in­ter­vene and vary the amount of beer con­sumed and see that we af­fect in­tel­li­gence, that im­plies that the sec­ond or third model is pos­si­ble; the first one is not. Of course, all this as­sumes that there aren’t other vari­ables me­di­at­ing be­tween the ones shown that pro­vide al­ter­na­tive ex­pla­na­tions of the de­pen­den­cies.
> 
> This makes no sense to me. I don’t see why only one of the 3 mod­els can be true. This is a math­e­mat­i­cal pos­si­bil­ity, but it seems highly im­plau­si­ble to me. And, in par­tic­u­lar, run­ning an ex­per­i­ment that re­veals one of these causal ef­fects does _not_ rule out the other pos­si­ble paths. For ex­am­ple, sup­pose that Slo­man were to per­form the above ex­per­i­ment (find­ing that beer con­sump­tion af­fects in­tel­li­gence) and then _an­other_ ex­per­i­ment, this time vary­ing in­tel­li­gence (in some way; the method of doing this can very well de­ter­mine the causal ef­fect) and find­ing that it af­fects the amount of beer con­sumed.
> 
> Be­yond this fun­da­men­tal prob­lem, I have a sta­tis­ti­cal cri­tique, which is that in so­cial sci­ence you won’t have these sorts of con­di­tional in­de­pen­den­cies, ex­cept from de­sign or as ar­ti­facts of small sam­ple sizes that do not allow us to dis­tin­guish small de­pen­den­cies from zero.
> 
> I think I see where Slo­man is com­ing from, from a psy­cho­log­i­cal per­spec­tive: you see these vari­ables that are re­lated to each other, and you want to know which is the cause and which is the ef­fect. But I don’t think this is a use­ful way of un­der­stand­ing the world, just as I don’t think it’s use­ful to cat­e­go­rize po­lit­i­cal play­ers as being mo­ti­vated ei­ther by greed or by larger prin­ci­ples, but not both. Exclusive- or might feel right to us in­ter­nally, but I don’t think it works as sci­ence.
> 
> One im­por­tant place where I agree with Slo­man (and thus with Pearl and Sprites et al.) is in the em­pha­sis that causal struc­ture can­not in gen­eral be learned from ob­ser­va­tional data alone; they hold the very rea­son­able po­si­tion that we can use ob­ser­va­tional data to rule out pos­si­bil­i­ties and for­mu­late hy­pothe­ses, and then use some sort of in­ter­ven­tion or ex­per­i­ment (whether ac­tual or hy­po­thet­i­cal) to move fur­ther. In this way they con­nect the ob­ser­va­tional/ ex­per­i­men­tal di­vi­sion to the hy­poth­e­sis/ de­duc­tion for­mu­la­tion that is fa­mil­iar to us from the work of Pop­per, Kuhn, and other mod­ern philoso­phers of sci­ence.
> 
> The place where I think Slo­man is mis­guided is in his for­mu­la­tion of sci­en­tific mod­els in an ei­ther/ or way, as if, in truth, so­cial vari­ables are linked in sim­ple causal paths, with a sci­en­tific goal of fig­ur­ing out if A causes B or the re­verse. I don’t know much about in­tel­li­gence, beer con­sump­tion, and so­cioe­co­nomic sta­tus, but I cer­tainly don’t see any sim­ple re­la­tion­ships be­tween in­come, re­li­gious at­ten­dance, party iden­ti­fi­ca­tion, and vot­ing—and I don’t see how a search for such a pat­tern will ad­vance our un­der­stand­ing, at least given cur­rent tech­niques. I’d rather start with de­scrip­tion and then go to­ward causal­ity fol­low­ing the ap­proach of econ­o­mists and sta­tis­ti­cians by think­ing about po­ten­tial in­ter­ven­tions one at a time. I’d love to see Slo­man’s and Pearl’s ideas of the in­ter­play be­tween ob­ser­va­tional and ex­per­i­men­tal data de­vel­oped in a frame­work that is less strongly tied to the no­tion of choice among sim­ple causal struc­tures.

## Gelman2012

[“The”hot hand” and prob­lems with hy­poth­e­sis test­ing” ⁠](https://statmodeling.stat.columbia.edu/2012/03/16/hot-hand-debate-is-warming-up/), Gel­man 2012:

> The ef­fects are cer­tainly not zero. We are not ma­chines, and any­thing that can af­fect our ex­pec­ta­tions (for ex­am­ple, our suc­cess in pre­vi­ous tries) should af­fect our per­for­mance…What­ever the lat­est re­sults on par­tic­u­lar sports, I can’t see any­one over­turn­ing the basic find­ing of Gilovich, Val­lone, and Tver­sky that play­ers and spec­ta­tors alike will _per­ceive_ the hot hand even when it does not exist and dra­mat­i­cally _over­es­ti­mate_ the mag­ni­tude and con­sis­tency of any hot- hand phe­nom­e­non that does exist. In sum­mary, this is yet an­other prob­lem where much is lost by going down the stan­dard route of null hy­poth­e­sis test­ing.

## Gelman Et Al 2013

[“In­her­ent dif­fi­cul­ties of non- Bayesian likelihood- based in­fer­ence, as re­vealed by an ex­am­i­na­tion of a re­cent book by Aitkin” ⁠](http://www.stat.columbia.edu/~gelman/research/published/GRR18.pdf) ([ear­lier ver­sion ⁠](http://www.stat.columbia.edu/~gelman/research/unpublished/GRR16.pdf)):

> 1.  _Solv­ing non- problems_
>     
> 
> Sev­eral of the ex­am­ples in _Sta­tis­ti­cal In­fer­ence_ rep­re­sent so­lu­tions to prob­lems that seem to us to be ar­ti­fi­cial or con­ven­tional tasks with no clear anal­ogy to ap­plied work.
> 
> > “They are ar­ti­fi­cial and are ex­pressed in terms of a sur­vey of 100 in­di­vid­u­als ex­press­ing sup­port (Yes/ No) for the pres­i­dent, be­fore and after a pres­i­den­tial ad­dress (…) The ques­tion of in­ter­est is whether there has been a change in sup­port be­tween the sur­veys (…). We want to as­sess the ev­i­dence for the hy­poth­e­sis of equal­ity _H_ 1 against the al­ter­na­tive hy­poth­e­sis _H_ 2 of a change.” — _Sta­tis­ti­cal In­fer­ence_, page 147
> 
> Based on our ex­pe­ri­ence in pub­lic opin­ion re­search, this is not a real ques­tion. Sup­port for any po­lit­i­cal po­si­tion is al­ways chang­ing. The real ques­tion is how much the sup­port has changed, or per­haps how this change is dis­trib­uted across the pop­u­la­tion.
> 
> A de­fender of Aitkin (and of clas­si­cal hy­poth­e­sis test­ing) might re­spond at this point that, yes, every­body knows that changes are never ex­actly zero and that we should take a more “grown- up” view of the null hy­poth­e­sis, not that the change is zero but that it is nearly zero. Un­for­tu­nately, the metaphor­i­cal in­ter­pre­ta­tion of hy­poth­e­sis tests has prob­lems sim­i­lar to the the­o­log­i­cal doc­trines of the Uni­tar­ian church. Once you have aban­doned lit­eral be­lief in the Bible, the ques­tion soon arises: why fol­low it at all? Sim­i­larly, once one rec­og­nizes the in­ap­pro­pri­ate­ness of the point null hy­poth­e­sis, we think it makes more sense not to try to re­ha­bil­i­tate it or treat it as trea­sured metaphor but rather to at­tack our sta­tis­ti­cal prob­lems di­rectly, in this case by per­form­ing in­fer­ence on the change in opin­ion in the pop­u­la­tion.
> 
> To be clear: we are not deny­ing the value of hy­poth­e­sis test­ing. In this ex­am­ple, we find it com­pletely rea­son­able to ask whether ob­served changes are statistically- significant, i.e. whether the data are con­sis­tent with a null hy­poth­e­sis of zero change. What we do not find rea­son­able is the state­ment that “the ques­tion of in­ter­est is whether there has been a change in sup­port.”
> 
> All this is application- specific. Sup­pose pub­lic opin­ion was ob­served to re­ally be flat, punc­tu­ated by oc­ca­sional changes, as in the left graph in Fig­ure 7.1. In that case, Aitkin’s ques­tion of “whether there has been a change” would be well- defined and ap­pro­pri­ate, in that we could in­ter­pret the null hy­poth­e­sis of no change as some min­i­mal level of base­line vari­a­tion.
> 
> Real pub­lic opin­ion, how­ever, does not look like base­line noise plus jumps, but rather shows con­tin­u­ous move­ment on many time scales at once, as can be seen from the right graph in Fig­ure 7.1, which shows ac­tual pres­i­den­tial ap­proval data. In this ex­am­ple, we do not see Aitkin’s ques­tion as at all rea­son­able. Any at­tempt to work with a null hy­poth­e­sis of opin­ion sta­bil­ity will be in­her­ently ar­bi­trary. It would make much more sense to model opin­ion as a continuously- varying process. The sta­tis­ti­cal prob­lem here is not merely that the null hy­poth­e­sis of zero change is non­sen­si­cal; it is that the null is in no sense a rea­son­able ap­prox­i­ma­tion to any in­ter­est­ing model. The so­ci­o­log­i­cal prob­lem is that, from [⁠Sav­age (1954 71ya) ⁠](https://gwern.net/everything#savage-1954) on­ward, many Bayesians have felt the need to mimic the clas­si­cal null- hypothesis test­ing frame­work, even where it makes no sense.

## Lin Et Al 2013

[“Too Big to Fail: Large Sam­ples and the _p_ - Value Prob­lem” ⁠](https://www.galitshmueli.com/system/files/Print%20Version.pdf), Lin et al 2013:

> The In­ter­net has pro­vided IS re­searchers with the op­por­tu­nity to con­duct stud­ies with ex­tremely large sam­ples, fre­quently well over 10,000 ob­ser­va­tions. There are many ad­van­tages to large sam­ples, but re­searchers using sta­tis­ti­cal in­fer­ence must be aware of the _p_ - value prob­lem as­so­ci­ated with them. In very large sam­ples, _p_ - values go quickly to zero, and solely re­ly­ing on _p_ - values can lead the re­searcher to claim sup­port for re­sults of no prac­ti­cal sig­nif­i­cance. In a sur­vey of large sam­ple IS re­search, we found that a sig­nif­i­cant num­ber of pa­pers rely on a low _p_ - value and the sign of a re­gres­sion co­ef­fi­cient alone to sup­port their hy­pothe­ses. This re­search com­men­tary rec­om­mends a se­ries of ac­tions the re­searcher can take to mit­i­gate the _p_ - value prob­lem in large sam­ples and il­lus­trates them with an ex­am­ple of over 300,000 cam­era sales on eBay. We be­lieve that ad­dress­ing the _p_ - value prob­lem will in­crease the cred­i­bil­ity of large sam­ple IS re­search as well as pro­vide more in­sights for read­ers.
> 
> …A key issue with ap­ply­ing small- sample sta­tis­ti­cal in­fer­ence to large sam­ples is that even mi­nus­cule ef­fects can be­come statistically- significant. The in­creased power leads to a dan­ger­ous pit­fall as well as to a huge op­por­tu­nity. The issue is one that sta­tis­ti­cians have long been aware of: “the _p_ - value prob­lem.” Chat­field (1995 30ya, p. 70 [_Prob­lem Solv­ing: A Sta­tis­ti­cian’s Guide, 2 nd ed_]) com­ments, “The ques­tion is not whether dif­fer­ences are ‘sig­nif­i­cant’ (they nearly al­ways are in large sam­ples), but whether they are in­ter­est­ing. For­get statistical- significance, what is the prac­ti­cal sig­nif­i­cance of the re­sults?” The in­creased power of large sam­ples means that re­searchers can de­tect smaller, sub­tler, and more com­plex ef­fects, but re­ly­ing on _p_ - values alone can lead to claims of sup­port for hy­pothe­ses of lit­tle or no prac­ti­cal sig­nif­i­cance.
> 
> …In re­view­ing the lit­er­a­ture, we found only a few men­tions of the large- sample issue and its ef­fect on _p_ - values; we also saw lit­tle recog­ni­tion that the au­thors’ low _p_ - values might be an ar­ti­fact of their large- sample sizes. Au­thors who rec­og­nized the “large- sample, small _p_ - values” issue ad­dressed it by one of the fol­low­ing ap­proaches: re­duc­ing the sig­nif­i­cance level thresh­old 5 (which does not re­ally help), by re­com­put­ing the _p_ - value for a small sam­ple (Gefen and Carmel 2008), or by fo­cus­ing on prac­ti­cal sig­nif­i­cance and com­ment­ing about the use­less­ness of statistical- significance ([Mithas and Lucas 2010 ⁠](https://terpconnect.umd.edu/~smithas/papers/mithaslucas2010ms.pdf)).

## Schwitzgebel2013

[“Pre­lim­i­nary Ev­i­dence That the World Is Sim­ple (An Ex­er­cise in Stu­pid Epis­te­mol­ogy)”](https://philosophycommons.typepad.com/xphi/2013/02/preliminary-evidence-that-the-world-is-simple-an-exercise-in-stupid-epistemology.html) (hu­mor­ous blog post)

> Here’s what I did. I thought up 30 pairs of vari­ables that would be easy to mea­sure and that might re­late in di­verse ways. Some vari­ables were phys­i­cal (the dis­tance ver­sus ap­par­ent bright­ness of nearby stars), some bi­o­log­i­cal (the length ver­sus weight of sticks found in my back yard), and some psy­cho­log­i­cal or so­cial (the S&P 500 index clos­ing value ver­sus num­ber of days past). Some I would ex­pect to show no re­la­tion­ship (the num­ber of pages in a li­brary book ver­sus how high up it is shelved in the li­brary), some I would ex­pect to show a roughly lin­ear re­la­tion­ship (dis­tance of Mc­Don­ald’s fran­chises from my house ver­sus [MapQuest ⁠](https://en.wikipedia.org/wiki/MapQuest) es­ti­mated dri­ving time), and some I ex­pected to show a curved or com­plex re­la­tion­ship (fore­casted tem­per­a­ture ver­sus time of day, size in KB of a JPG photo of my of­fice ver­sus the angle at which the photo was taken). [⁠See here](https://schwitzsplintersunderblog.blogspot.com/2013/02/variables-measured-for-post-preliminary.html) for the full list of vari­ables. I took 11 mea­sure­ments of each vari­able pair. Then I an­a­lyzed the re­sult­ing data.
> 
> Now, if the world is mas­sively com­plex, then it should be dif­fi­cult to pre­dict a third dat­a­point from any two other data points. Sup­pose that two mea­sure­ments of some con­tin­u­ous vari­able yield val­ues of 27 and 53. What should I ex­pect the third mea­sured value to be? Why not 1,457,002? Or 3.22 × 10 −17? There are just as many func­tions (that is, in­fi­nitely many) con­tain­ing 27, 53, and 1,457,002 as there are con­tain­ing 27, 53, and some more pedestrian- seeming value like 44.
> 
> …To con­duct the test, I used each pair of de­pen­dent vari­ables to pre­dict the value of the next vari­able in the se­ries (the 1 st and 2 nd ob­ser­va­tions pre­dict­ing the value of the 3 rd, the 2 nd and 3 rd pre­dict­ing the value of the 4 th, etc.), yield­ing 270 pre­dic­tions for the 30 vari­ables. I counted an ob­ser­va­tion “wild” if its ab­solute value was 10 times the max­i­mum of the ab­solute value of the two pre­vi­ous ob­ser­va­tions or if its ab­solute value was below 1⁄10 of the min­i­mum of the ab­solute value of the two pre­vi­ous ob­ser­va­tions. Sep­a­rately, I also looked for flipped signs (ei­ther two neg­a­tive val­ues fol­lowed by a pos­i­tive or two pos­i­tive val­ues fol­lowed by a neg­a­tive), though most of the vari­ables only ad­mit­ted pos­i­tive val­ues. This mea­sure of wild­ness yielded 3 wild ob­ser­va­tions out of 270 (1%) plus an­other 3 flipped- sign cases (total 2%). (A few vari­ables were capped, ei­ther top or bot­tom, in a way that would make an above- 10x or below- 1/ 10 th ob­ser­va­tion an­a­lyt­i­cally un­likely, but ex­clud­ing such vari­ables wouldn’t af­fect the re­sult much.) So it looks like the Wild Com­plex­ity The­sis might be in trou­ble.

## Ellenberg2014

Jor­dan El­len­berg, [“The Myth Of The Myth Of The Hot Hand”](https://deadspin.com/the-myth-of-the-myth-of-the-hot-hand-1588112937/) (ex­cerpted from _How Not to Be Wrong: The Power of Math­e­mat­i­cal Think­ing_, 2014 11ya):

> A sig­nif­i­cance test is a sci­en­tific in­stru­ment, and like any other in­stru­ment, it has a cer­tain de­gree of pre­ci­sion. If you make the test more sen­si­tive—by in­creas­ing the size of the stud­ied pop­u­la­tion, for ex­am­ple—you en­able your­self see ever- smaller ef­fects. That’s the power of the method, but also its dan­ger. The truth is, the null hy­poth­e­sis is prob­a­bly _al­ways_ false! When you drop a pow­er­ful drug into a pa­tient’s blood­stream, it’s hard to be­lieve the in­ter­ven­tion lit­er­ally has zero ef­fect on the prob­a­bil­ity that the pa­tient will de­velop esophageal can­cer, or throm­bo­sis, or bad breath. Each part of the body speaks to every other, in a com­plex feed­back loop of in­flu­ence and con­trol. Every­thing you do ei­ther gives you can­cer or pre­vents it. And in prin­ci­ple, if you carry out a pow­er­ful enough study, you can find out which it is. But those ef­fects are usu­ally so mi­nus­cule that they can be safely ig­nored. Just be­cause we can de­tect them doesn’t al­ways mean they mat­ter…The right ques­tion isn’t, “Do bas­ket­ball play­ers some­times tem­porar­ily get bet­ter or worse at mak­ing shots?”—the kind of yes/ no ques­tion a sig­nif­i­cance test ad­dresses. The right ques­tion is “How _much_ does their abil­ity vary with time, and to what ex­tent can ob­servers de­tect in real time whether a player is hot?” Here, the an­swer is surely “not as much as peo­ple think, and hardly at all.”

## Lakens2014

[⁠“The Null Is Al­ways False (Ex­cept When It Is True)”](https://daniellakens.blogspot.com/2014/06/the-null-is-always-false-except-when-it.html), Daniel Lak­ens:

> The more im­por­tant ques­tion is whether it is true that there are al­ways real dif­fer­ences in the real world, and what the ‘real world’ is. Let’s con­sider the pop­u­la­tion of peo­ple in the real world. While you read this sen­tence, some in­di­vid­u­als in this pop­u­la­tion have died, and some were born. For most ques­tions in psy­chol­ogy, the pop­u­la­tion is sur­pris­ingly sim­i­lar to an eter­nally run­ning Monte Carlo sim­u­la­tion. Even if you could mea­sure all peo­ple in the world in a mil­lisec­ond, and the test- retest cor­re­la­tion was per­fect, the an­swer you would get now would be dif­fer­ent from the an­swer you would get in an hour. Fre­quen­tists (the peo­ple that use NHST) are not specif­i­cally in­ter­ested in the exact value now, or in one hour, or next week Thurs­day, but in the av­er­age value in the ‘long’ run. The value in the real world today might never be zero, but it’s never any­thing, be­cause it’s con­tin­u­ously chang­ing. If we want to make gen­er­al­iz­able state­ments about the world, I think the fact that the null- hypothesis is never pre­cisely true at any spe­cific mo­ment is not a prob­lem. I’ll ig­nore more com­plex ques­tions for now, such as how we can es­tab­lish whether ef­fects vary over time.
> 
> …Meehl talks about how in psy­chol­ogy every individual- difference vari­able (eg. trait, sta­tus, de­mo­graphic) cor­re­lates with every other vari­able, which means the null is prac­ti­cally never true. In these sit­u­a­tions, it’s not that test­ing against the null- hypothesis is mean­ing­less, but it’s not in­for­ma­tive. If every­thing cor­re­lates with every­thing else, you need to cre­ate good mod­els, and test those. A sim­ple null- hypothesis sig­nif­i­cance test will not get you very far. I agree.
> 
> **_Ran­dom As­sign­ment ver­sus Crud_**
> 
> To il­lus­trate when NHST can be used to as a source of in­for­ma­tion in large sam­ples, and when NHST is not in­for­ma­tive in large sam­ples, I’ll an­a­lyze data of large dataset with 6344 par­tic­i­pants from the Many Labs project. I’ve an­a­lyzed 10 de­pen­dent vari­ables to see whether they were in­flu­enced by (1) Gen­der, and (2) As­sign­ment to the high or low an­chor­ing con­di­tion in the first study. Gen­der is a mea­sured in­di­vid­ual dif­fer­ence vari­able, and not a ma­nip­u­lated vari­able, and might thus be af­fected by what Meehl calls the crud fac­tor. Here, I want to il­lus­trate this is (1) prob­a­bly often true for in­di­vid­ual dif­fer­ence vari­ables, but per­haps not al­ways true, and (2) it is prob­a­bly never true for when an­a­lyz­ing dif­fer­ences be­tween groups in­di­vid­u­als were ran­domly as­sign­ment to.
> 
> …When we an­a­lyze the 10 de­pen­dent vari­ables as a func­tion of the an­chor­ing con­di­tion, none of the dif­fer­ences are statistically- significant (even though there are more than 6000 par­tic­i­pants). You can play around with the script, re­peat­ing the analy­sis for the con­di­tions re­lated to the other 3 an­chor­ing ques­tions (re­mem­ber to cor­rect for mul­ti­ple com­par­isons if you per­form many tests), and see how ran­dom­iza­tion does a pretty good job at re­turn­ing non- statistically-significant re­sults even in very large sam­ple sizes. If the null is al­ways false, it is re­mark­ably dif­fi­cult to re­ject. Ob­vi­ously, when we an­a­lyze the an­swer peo­ple gave on the first an­chor­ing ques­tion, we find a huge ef­fect of the high ver­sus low an­chor­ing con­di­tion they were ran­domly as­signed to. Here, NHST works. There is prob­a­bly some­thing going on. If the an­chor­ing ef­fect was a com­pletely novel phe­nom­e­non, this would be an im­por­tant first find­ing, to be fol­lowed by repli­ca­tions and ex­ten­sions, and fi­nally model build­ing and test­ing.
> 
> The re­sults change dra­mat­i­cally if we use Gen­der as a fac­tor. There are Gen­der ef­fects on de­pen­dent vari­ables re­lated to quote at­tri­bu­tion, sys­tem jus­ti­fi­ca­tion, the gam­bler’s fal­lacy, imag­ined con­tact, the ex­plicit eval­u­a­tion of arts and math, and the norm of reci­procity. There are no sig­nif­i­cant dif­fer­ences in po­lit­i­cal iden­ti­fi­ca­tion (as con­ser­v­a­tive or lib­eral), on the re­sponse scale ma­nip­u­la­tion, or on gain ver­sus loss fram­ing (even though _p_ = 0.025, such a high _p_ - value is stronger sup­port for the null- hypothesis than for the al­ter­na­tive hy­poth­e­sis with 5500 par­tic­i­pants). It’s sur­pris­ing that the null- hypothesis (gen­der does not in­flu­ence the re­sponses par­tic­i­pants give) is re­jected for 7 out of 10 ef­fects. Per­son­ally (per­haps be­cause I’ve got very lit­tle ex­per­tise in gen­der ef­fects) I was ac­tu­ally ex­tremely sur­prised, even though the ef­fects are small (with Cohen _d_ ’s or around 0.09). This, iron­i­cally, shows that NHST works—I’ve learned gen­der ef­fects are much more wide­spread than I’d have though be­fore I wrote this blog post.

## Kirkegaard2014

[“The in­ter­na­tional gen­eral so­cioe­co­nomic fac­tor: Fac­tor an­a­lyz­ing in­ter­na­tional rank­ings” ⁠](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.669.378&rep=rep1&type=pdf):

> Many stud­ies have ex­am­ined the cor­re­la­tions be­tween na­tional IQs and var­i­ous country- level in­dexes of well- being. The analy­ses have been un­sys­tem­atic and not gath­ered in one sin­gle analy­sis or dataset. In this paper I gather a large sam­ple of country- level in­dexes and show that there is a strong gen­eral so­cioe­co­nomic fac­tor (S fac­tor) which is highly cor­re­lated (.86–.87) with na­tional cog­ni­tive abil­ity using ei­ther Lynn and Van­hanen’s dataset or Al­ti­nok’s. Fur­ther­more, the method of cor­re­lated vec­tors shows that the cor­re­la­tions be­tween vari­able load­ings on the S fac­tor and cog­ni­tive mea­sure­ments are.99 in both datasets using both cog­ni­tive mea­sure­ments, in­di­cat­ing that it is the S fac­tor that dri­ves the re­la­tion­ship with na­tional cog­ni­tive mea­sure­ments, not the re­main­ing vari­ance.

See also [⁠“Coun­tries Are Ranked On Every­thing From Health To Hap­pi­ness. What’s The Point?” ⁠](https://www.npr.org/sections/goatsandsoda/2019/06/14/730257541/countries-are-ranked-on-everything-from-health-to-happiness-whats-the-point):

> It’s a brand new rank­ing. Called the Sus­tain­able De­vel­op­ment Goals Gen­der Index, it gives 129 coun­tries a score for progress on achiev­ing gen­der equal­ity by 2030. Here’s the quick sum­mary: Things are “good” in much of Eu­rope and North Amer­ica. And “very poor” in much of sub- Saharan Africa. In fact, that’s the way it looks in many in­ter­na­tional rank­ings, which tackle every­thing from the worst places to be a child to the most cor­rupt coun­tries to world hap­pi­ness…As for the fact that many rank­ings look the same at the top and bot­tom, one rea­son has to do with money. Many in­dexes are cor­re­lated with GDP per capita, a mea­sure of a coun­try’s pros­per­ity, says Kenny. That in­cludes the World Bank’s Human Cap­i­tal Index, which mea­sures the eco­nomic pro­duc­tiv­ity of a coun­try’s young peo­ple; and Free­dom House’s Free­dom in the World index, which ranks the world by its level of democ­racy, in­clud­ing eco­nomic free­dom. And coun­tries that have more money can spend more money on health, ed­u­ca­tion and in­fra­struc­ture.

## Shen Et Al 2014

[“When Cor­rect­ing for Un­re­li­a­bil­ity of Job Per­for­mance Rat­ings, the Best Es­ti­mate Is Still 0.52” ⁠](https://gwern.net/doc/psychology/2014-shen.pdf), Shen et al 2014:

> **Is Too Much Vari­ance Ex­plained?** It is in­ter­est­ing that his­tor­i­cally the I-O lit­er­a­ture has be­moaned the pres­ence of a “va­lid­ity ceil­ing”, and the field seemed to be un­able to make large gains in the pre­dic­tion of job per­for­mance (High­house, 2008 17ya). In con­trast, Le­Bre­ton et al. ap­pear to have the op­po­site con­cern—that we maybe able to pre­dict too much, per­haps even all, of the vari­ance in job per­for­mance once ac­count­ing for sta­tis­ti­cal ar­ti­facts. In ad­di­tion to their 4 focal pre­dic­tors (ie. GMA, in­tegrity, struc­tured in­ter­view, work sam­ple), Le­Bre­ton et al. list an ad­di­tional 24 vari­ables that have been shown to be re­lated to job per­for­mance meta- analytically. How­ever, we be­lieve that many of the vari­ables Le­Bre­ton et al. in­cluded in their list are vari­ables that Sack­ett, Borne­man, and Con­nelly (2009 16ya) would argue are likely un­know­able at time of hire.
> 
> …Fur­ther­more, in con­trast to Le­Bre­ton et al.’s as­ser­tion that or­ga­ni­za­tional vari­ables, such as pro­ce­dural jus­tice, are likely un­re­lated to their focal pre­dic­tors, our be­lief is that many of these vari­ables are likely to be at least mod­er­ately cor­re­lated–lim­it­ing the in­cre­men­tal va­lid­ity we could ex­pect with the in­clu­sion of these ad­di­tional vari­ables. For ex­am­ple, re­search has shown that in­tegrity tests mostly tap into [Con­sci­en­tious­ness ⁠](https://en.wikipedia.org/wiki/Conscientiousness#Personality_models), [Agree­able­ness ⁠](https://en.wikipedia.org/wiki/Agreeableness), and Emo­tional Sta­bil­ity (Ones & Viswes­varan, 2001 24ya), and a re­cent [meta- analysis ⁠](https://en.wikipedia.org/wiki/Meta-analysis) of or­ga­ni­za­tional jus­tice shows that all 3 per­son­al­ity traits are mod­er­ately re­lated to one’s ex­pe­ri­ence of pro­ce­dural jus­tice (ρ=0.19–0.23; Hutchin­son et al 2014), sug­gest­ing that even ap­par­ently un­re­lated vari­ables can share a sur­pris­ing amount of construct- level vari­ance. In sup­port of this per­spec­tive, Pa­ter­son, Harms, and Crede (2012 13ya) [“The meta of all metas: 30 years of meta- analysis re­viewed”] con­ducted a meta- analysis of over 200 meta- analyses and found an av­er­age cor­re­la­tion of 0.27, sug­gest­ing that most vari­ables we study are at least some­what cor­re­lated and val­i­dat­ing the first au­thor’s long- held per­sonal as­sump­tion that the world is cor­re­lated 0.30 (on av­er­age; see also [⁠Meehl’s, 1990 ⁠](https://gwern.net/everything#meehl-1990-1), crud fac­tor)!

## Gordon Et Al 2019

[“A Com­par­i­son of Ap­proaches to Ad­ver­tis­ing Mea­sure­ment: Ev­i­dence from Big Field Ex­per­i­ments at Face­book” ⁠](https://gwern.net/doc/statistics/causality/2019-gordon.pdf), Gor­don et al 2019:

> We ex­am­ine how com­mon tech­niques used to mea­sure the causal im­pact of ad ex­po­sures on users’ con­ver­sion out­comes com­pare to the “gold stan­dard” of a true ex­per­i­ment (ran­dom­ized con­trolled trial). Using data from 12 US ad­ver­tis­ing lift stud­ies at Face­book com­pris­ing 435 mil­lion user- study ob­ser­va­tions and 1.4 bil­lion total im­pres­sions we con­trast the ex­per­i­men­tal re­sults to those ob­tained from ob­ser­va­tional meth­ods, such as com­par­ing ex­posed to un­ex­posed users, match­ing meth­ods, model- based ad­just­ments, syn­thetic matched- markets tests, and before- after tests. We show that ob­ser­va­tional meth­ods often fail to pro­duce the same re­sults as true ex­per­i­ments even after con­di­tion­ing on in­for­ma­tion from thou­sands of be­hav­ioral vari­ables and using non- linear mod­els. We ex­plain why this is the case. Our find­ings sug­gest that com­mon ap­proaches used to mea­sure ad­ver­tis­ing ef­fec­tive­ness in in­dus­try fail to mea­sure ac­cu­rately the true ef­fect of ads.
> 
> An im­por­tant input to [propen­sity score match­ing ⁠](https://en.wikipedia.org/wiki/Propensity_score_matching) (PSM) is the set of vari­ables used to pre­dict the propen­sity score it­self. We tested 3 dif­fer­ent PSM spec­i­fi­ca­tions for study 4, each of which used a larger set of in­puts.
> 
> 1.  **PSM 1**: In ad­di­tion to age and gen­der, the basis of our exact match­ing (EM) ap­proach, this spec­i­fi­ca­tion uses com­mon Face­book vari­ables, such as how long users have been on Face­book, how many Face­book friends the have, their re­ported re­la­tion­ship sta­tus, and their phone OS, in ad­di­tion to other user char­ac­ter­is­tics.
>     
> 2.  **PSM 2**: In ad­di­tion to the vari­ables in PSM 1, this spec­i­fi­ca­tion uses Face­book’s es­ti­mate of the user’s zip code of res­i­dence to as­so­ciate with each user nearly 40 vari­ables drawn from the most re­cent Cen­sus and Amer­i­can Com­mu­ni­ties Sur­veys (ACS).
>     
> 3.  **PSM 3**: In ad­di­tion to the vari­ables in PSM 2, this spec­i­fi­ca­tion adds a com­pos­ite met­ric of Face­book data that sum­ma­rizes thou­sands of be­hav­ioral vari­ables. This is a machine- learning based met­ric used by Face­book to con­struct tar­get au­di­ences that are sim­i­lar to con­sumers that an ad­ver­tiser has iden­ti­fied as de­sir­able.[16 ⁠](https://www.facebook.com/business/help/164749007013531) Using this met­ric bases the es­ti­ma­tion of our propen­sity score on a non- linear machine- learning model with thou­sands of fea­tures.17
>     
> 
> …When we go from exact match­ing (EM) to our most par­si­mo­nious propen­sity score match­ing model (PSM 1), the con­ver­sion rate for un­ex­posed users in­creases from 0.032% to 0.042%, de­creas­ing the im­plied ad­ver­tis­ing lift from 221% to 147%. PSM 2 per­forms sim­i­larly to PSM 1, with an im­plied lift of 154%.21 Fi­nally, adding the com­pos­ite mea­sure of Face­book vari­ables in PSM 3 im­proves the fit of the propen­sity model (as mea­sured by a higher AUC/ ROC) and fur­ther in­creases the con­ver­sion rate for matched un­ex­posed users to 0.051%. The re­sult is that our best per­form­ing PSM model es­ti­mates an ad­ver­tis­ing lift of 102%…We sum­ma­rize the re­sult of all our propen­sity score match­ing and re­gres­sion meth­ods for study 4 in Fig­ure 7.
> 
> ![Figure 7: Summary of lift estimates and confidence intervals.](https://gwern.net/doc/statistics/2016-gordon-figure7-propensityscoringresults.png)
> 
> **Fig­ure 7**: Sum­mary of lift es­ti­mates and con­fi­dence in­ter­vals.

While not di­rectly test­ing statistical- significance in its propen­sity scor­ing, the in­creas­ing ac­cu­racy in es­ti­mat­ing the true causal ef­fect of adding in ad­di­tional be­hav­ioral vari­ables im­plies that (es­pe­cially at Facebook- scale, using bil­lions of dat­a­points) the cor­re­la­tions of the thou­sands of used vari­ables with the ad­ver­tis­ing be­hav­ior would be statistically- significant and demon­strate that every­thing is cor­re­lated. (See also my [ad harms ⁠](https://gwern.net/banner) & [“How Often Does Cor­re­la­tion=Causal­ity?” ⁠](https://gwern.net/correlation) pages.)

## Kirkegaard2020

[⁠“En­hanc­ing archival datasets with ma­chine learned psy­cho­met­rics”](https://emilkirkegaard.dk/en/2020/03/enhancing-archival-datasets-with-machine-learned-psychometrics/), Kirkegaard 2020 (pub­lished as [Kirkegaard & Ny­borg 2021 ⁠](https://gwern.net/doc/iq/2021-kirkegaard.pdf)):

> In our ISIR 2019 pre­sen­ta­tion ([⁠“Ma­chine learn­ing psy­cho­met­rics: Im­proved cog­ni­tive abil­ity va­lid­ity from su­per­vised train­ing on item level data” ⁠](https://docs.google.com/presentation/d/1xxfYTWP2R0ZFvbI24sR1jVyFC_qN_Lw85782zE5Q2Jo/edit)), we showed that one can use ma­chine learn­ing on cog­ni­tive data to im­prove the pre­dic­tive va­lid­ity of it. The ef­fect sizes can be quite large, eg. one could pre­dict ed­u­ca­tional at­tain­ment in the Viet­nam Ex­pe­ri­ence Study (VES) sam­ple (_n_ = 4.5k US army re­cruits) at R 2 =32.3% with [ridge re­gres­sion ⁠](https://en.wikipedia.org/wiki/Ridge_regression) ver­sus 17.7% with [IRT ⁠](https://en.wikipedia.org/wiki/Item_response_theory). Pre­dic­tion is more than _g_, after all. What if we had a dataset of 185 di­verse items, and we train the model to pre­dict IRT- based _g_ from the full set, but using only a lim­ited set using the LASSO? How many items do we need when op­ti­mally weighted? Turns out that with 42 items, one can get a test that cor­re­lates at 0.96 with the full _g_. That’s an ab­bre­vi­a­tion of nearly 80%!
> 
> Now comes the fancy part. What if we have archival datasets with only a few cog­ni­tive items (eg. datasets with [MMSE ⁠](https://en.wikipedia.org/wiki/Mini%E2%80%93mental_state_examination) items) or maybe even no items. Can we im­prove things here? Maybe! If the dataset has a lot of other items, we may be able to train an ma­chine learn­ing (ML) model that pre­dict _g_ quite well from them, even if they seem un­re­lated. Every item has some vari­ance over­lap with _g_ how­ever small (crud fac­tor), it is only a ques­tion of hav­ing a good enough al­go­rithm and enough data to ex­ploit this co­vari­ance. For in­stance, I have found that if one uses the 556 items in the [MMPI ⁠](https://en.wikipedia.org/wiki/Minnesota_Multiphasic_Personality_Inventory) in the VES to pre­dict the very well mea­sured _g_ based on all the cog­ni­tive data (18 tests), how well can one do? I was sur­prised to learn that one can do _ex­tremely_ well:
> 
> [⁠“Elas­tic net pre­dic­tion of _g_: _r_ = 0.83 (0.82–0.84), _n_ = 4,320” ⁠](https://gwern.net/doc/iq/2020-kirkegaard-elasticnetpredictionofiqfrommmpiintheves.png)
> 
> [There are 203 (elas­tic)/ 217 ([lasso ⁠](https://en.wikipedia.org/wiki/Lasso_\(statistics\))) non- zero co­ef­fi­cients out of 556]
> 
> Thus, one can mea­sure _g_ as well as one could with a de­cent test like Won­der­lic, or Raven’s with­out hav­ing any cog­ni­tive data at all! The big ques­tion here is whether these mod­els gen­er­al­ize well. If one can train a model to pre­dict _g_ from MMPI items in dataset 1, and then apply it to dataset 2 with­out much loss of ac­cu­racy, this means that one could im­pute _g_ in po­ten­tially thou­sands of old archival datasets that in­clude the same MMPI items, or a sub­set of them.

A sim­i­lar analy­sis is done by Rev­elle et al 2020 ’s [“Ex­plor­ing the per­some: The power of the item in un­der­stand­ing per­son­al­ity struc­ture” ⁠](https://www.sciencedirect.com/science/article/pii/S0191886920300945) (es­pe­cially “Study 4: Pro­file cor­re­la­tions using 696 items”); they do not di­rectly re­port an equiv­a­lent to pos­te­ri­ors/ _p_ - values or non- zero cor­re­la­tions after pe­nal­ized re­gres­sion or any­thing like that, but the per­va­sive­ness of cor­re­la­tion is ap­par­ent from their re­sults & data vi­su­al­iza­tions.

## Ferguson & Heene2021

[“Pro­vid­ing a Lower- Bound Es­ti­mate for Psy­chol­ogy’s ‘Crud Fac­tor’: The Case of Ag­gres­sion” ⁠](https://gwern.net/doc/psychology/2021-ferguson.pdf), Fer­gu­son & Heene 2021:

> When con­duct­ing re­search on large data sets, statistically- significant find­ings hav­ing only triv­ial in­ter­pre­tive mean­ing may ap­pear. Lit­tle con­sen­sus ex­ists whether such small ef­fects can be mean­ing­fully in­ter­preted. The cur­rent analy­sis ex­am­ines the pos­si­bil­ity that triv­ial ef­fects may emerge in large datasets, but that some such ef­fects may lack in­ter­pre­tive value. When such re­sults match an in­ves­ti­ga­tor’s hy­poth­e­sis, they may be over- interpreted.
> 
> The cur­rent study ex­am­ines this issue as re­lated to ag­gres­sion re­search in 2 large sam­ples. Specif­i­cally, in the first study, the Na­tional Lon­gi­tu­di­nal Study of Ado­les­cent to Adult Health (Add Health) dataset was used. 15 vari­ables with lit­tle the­o­ret­i­cal rel­e­vance to ag­gres­sion were se­lected, then cor­re­lated with self- reported delin­quency. For the sec­ond study, the Un­der­stand­ing So­ci­ety data­base was used. As with Study 1, 14 non­sen­si­cal vari­ables were cor­re­lated with con­duct prob­lems.
> 
> Many vari­ables achieved “statistical- significance” and some effect- sizes ap­proached or ex­ceeded _r_ = 0.10, de­spite lit­tle the­o­ret­i­cal rel­e­vance be­tween the vari­ables.
> 
> It is rec­om­mended that ef­fect sizes below _r_ = 0.10 should not be in­ter­preted as hy­poth­e­sis sup­port­ive.
> 
> ![Table 1: Correlations Between Crud and Delinquency for Study 1](https://gwern.net/doc/psychology/2021-ferguson-table1-correlationsbetweenirrelevantfactorsandjuveniledelinquencyinaddhealth.png)
> 
> **Table 1**: Cor­re­la­tions Be­tween Crud and Delin­quency for Study 1
> 
> ![Table 2: Correlations Between Crud and Conduct Problems for Study 2](https://gwern.net/doc/psychology/2021-ferguson-table2-correlationsbetweenirrelevantfactorsandconductproblemsinunderstandingsocietywave1.png)
> 
> **Table 2**: Cor­re­la­tions Be­tween Crud and Con­duct Prob­lems for Study 2

## Iliev & Bennis2023

[“The Con­ver­gence of Pos­i­tiv­ity: Are Happy Peo­ple All Alike?”, Iliev & Ben­nis 2023 ⁠](https://link.springer.com/article/10.1007/s10902-023-00631-9)

## Downey2023

[“How Cor­re­lated Are You?”, Downey 2023](https://www.allendowney.com/blog/2023/08/20/how-correlated-are-you/)

## External Links

-   [All mod­els are wrong ⁠](https://en.wikipedia.org/wiki/All_models_are_wrong)
    
-   [“Stereo­type (In)Ac­cu­racy in Per­cep­tions of Groups and In­di­vid­u­als” ⁠](https://pdfs.semanticscholar.org/8d4e/4958856859323ecacca91807a0ea2e847dfc.pdf), Jus­sim et al 2015
    
-   [_Hand­book of So­cial Sta­tus Cor­re­lates_ ⁠](https://www.amazon.com/Handbook-Social-Status-Correlates-Ellis/dp/0128053712), Ellis et al 2018
    
-   [⁠“Black Peo­ple Less Likely” ⁠](https://slatestarcodex.com/2015/02/11/black-people-less-likely/)
    
-   [“Per­son­al­ity and the Pre­dic­tion of Con­se­quen­tial Out­comes” ⁠](https://gwern.net/doc/psychology/personality/2006-ozer.pdf), Ozer & Benet- Martínez 2006
    
-   [⁠“Against NHST” ⁠](https://www.lesswrong.com/posts/ttvnPRTxFyru9Hh2H/against-nhst)
    
-   [“The sur­pris­ing im­pli­ca­tions of fa­mil­ial as­so­ci­a­tion in dis­ease risk” ⁠](https://arxiv.org/abs/1707.00014), Val­berg et al 2017
    
-   [“The Ab­bre­vi­a­tion of Per­son­al­ity, or how to Mea­sure 200 Per­son­al­ity Scales with 200 Items” ⁠](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2858332/), Yarkoni 2010
    
-   [⁠“The Best Ques­tions For A First Date” ⁠](https://gwern.net/doc/psychology/okcupid/thebestquestionsforafirstdate.html), Chris­t­ian Rud­der 2011 (OKCu­pid)
    
-   [⁠“Pushed around by stars”](https://eighteenthelephant.com/2021/11/29/pushed-around-by-stars/)
    
-   [“The­o­ret­i­cal false pos­i­tive psy­chol­ogy” ⁠](https://gwern.net/doc/statistics/bias/2022-wilson.pdf), Wil­son et al 2022
    
-   Dis­cus­sion: [⁠HN ⁠](https://news.ycombinator.com/item?id=19797844)
    

## Bibliography

1.  Some­times para­phrased as “All good things tend to go to­gether, as do all bad ones”.[](https://gwern.net/everything#fnref1) [↩](#fnref:1)
    
2.  Tib­shi­rani 2014:
    
    In de­scrib­ing some of this work, [Hastie et al 2001 ⁠](https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12.pdf#page=630) coined the in­for­mal “Bet on Spar­sity” prin­ci­ple [“Use a pro­ce­dure that does well in sparse prob­lems, since no pro­ce­dure does well in dense prob­lems.”]. The ℓ 1 meth­ods as­sume that the truth is sparse, in some basis. If the as­sump­tion holds true, then the pa­ra­me­ters can be ef­fi­ciently es­ti­mated using ℓ 1 penal­ties. If the as­sump­tion does not hold—so that the truth is dense—then no method will be able to re­cover the un­der­ly­ing model with­out a large amount of data per pa­ra­me­ter. This is typ­i­cally not the case when _p_ ≫ _N_, a com­monly oc­cur­ring sce­nario.
    
    This can be seen as a kind of decision- theoretic jus­ti­fi­ca­tion for Occam- style as­sump­tions: if the real world is not pre­dictable in the sense of being pre­dictable by sim­ple/ fast al­go­rithms, or in­duc­tion doesn’t work at all, then no method works in ex­pec­ta­tion, and the “re­gret” (dif­fer­ence be­tween [ex­pected value ⁠](https://en.wikipedia.org/wiki/Expected_value) of ac­tual de­ci­sion and ex­pected value of op­ti­mal de­ci­sion) from mis­tak­enly as­sum­ing that the world is sim­ple/ sparse is zero. So one should as­sume the world is sim­ple.[](https://gwern.net/everything#fnref2) [↩](#fnref:2)
    
3.  A ma­chine learn­ing prac­ti­tioner as of 2019, will be struck by the thought that To­bler’s first law nicely en­cap­su­lates the prin­ci­ple be­hind the “un­rea­son­able ef­fec­tive­ness” of [con­vo­lu­tions in ap­pli­ca­tions of neural net­works ⁠](https://en.wikipedia.org/wiki/Convolutional_neural_network) to so many do­mains far be­yond im­ages; this con­nec­tion has been made by [⁠John Hessler](https://blogs.loc.gov/maps/2016/04/alphago-neural-networks-and-toblers-first-law/).[](https://gwern.net/everything#fnref3) [↩](#fnref:3)
    
4.  The most in­ter­est­ing ex­am­ple of this is ESP/ psi para­psy­chol­ogy re­search: the more rig­or­ously con­ducted the ESP ex­per­i­ments are, the smaller the ef­fects be­come—but, while dis­cred­it­ing all claims of human ESP, fre­quently they aren’t pushed to _ex­actly_ zero and are “statistically- significant”. [There must be ⁠](https://gwern.net/modus) some resid­ual crud fac­tor in the ex­per­i­ments, even when con­ducted & an­a­lyzed as best as we know how.[](https://gwern.net/everything#fnref4) [↩](#fnref:4)
    
5.  Gos­set 1904 has been dis­cussed in sev­eral sources, like [Pear­son 1939 ⁠](https://gwern.net/doc/statistics/decision/1939-pearson.pdf).[](https://gwern.net/everything#fnref5) [↩](#fnref:5)
    
6.  The ver­sion in the sec­ond edi­tion, [_The Foun­da­tions of Sta­tis­tics_, 2 nd edi­tion, Sav­age 1972 ⁠](https://gwern.net/doc/statistics/decision/1972-savage-foundationsofstatistics.pdf#page=270), is iden­ti­cal to the first.[](https://gwern.net/everything#fnref6) [↩](#fnref:6)
    
7.  Note: “I. Richard Sav­age” is not to be con­fused with his brother, _Leonard Jim­mie_ Sav­age, who also worked in [Bayesian sta­tis­tics ⁠](https://en.wikipedia.org/wiki/Bayesian_statistics) & is cited pre­vi­ously.[](https://gwern.net/everything#fnref7) [↩](#fnref:7)
    
8.  [2nd edi­tion, 1986 ⁠](https://gwern.net/doc/statistics/decision/1986-lehmann-testingstatisticalhypotheses.pdf); after skim­ming the 2 nd edi­tion, I have not been able to find a rel­e­vant pas­sage, but Lehmann re­marks that he sub­stan­tially rewrote the text­book for a more ro­bust decision- theoretic ap­proach, so it may have been re­moved.[](https://gwern.net/everything#fnref8) [↩](#fnref:8)
    
9.  This analy­sis was never pub­lished, ac­cord­ing to [⁠Meehl 1990a ⁠](https://gwern.net/everything#meehl-1990-1).[](https://gwern.net/everything#fnref9) [↩](#fnref:9)
    
10.  I would note there is [⁠a dan­ger­ous fal­lacy here ⁠](https://gwern.net/note/statistic#expectations-are-not-expected-deviations-and-large-number-of-variables-are-not-large-samples) even if one does be­lieve the Law of Large Num­bers should apply here with an ex­pec­ta­tion of zero ef­fect: even if the ex­pec­ta­tion of the pair­wise cor­re­la­tion of 2 ar­bi­trary vari­ables was in fact pre­cisely zero (as is not too im­plau­si­ble in some do­mains such as op­ti­miza­tion or feed­back loops—such as the fa­mous ex­am­ple of the ther­mo­stat/ room- temperature), that does not mean any spe­cific pair will be ex­actly zero no mat­ter how many num­bers get added up to cre­ate their re­la­tion­ship, as the ab­solute size of the de­vi­a­tion in­creases.
    
    So for ex­am­ple, imag­ine 2 ge­netic traits which may be genetically- correlated, and their her­i­tabil­ity may be caused by a num­ber of genes rang­ing from 1 (mono­genic) to tens of thou­sands (highly poly­genic); the spe­cific over­lap is cre­ated by a chance draw of evo­lu­tion­ary processes through­out the or­gan­ism’s evo­lu­tion; does the Law of Large Num­bers jus­tify say­ing that while 2 mono­genic traits may have a sub­stan­tial cor­re­la­tion, 2 highly poly­genic traits must have much closer to zero cor­re­la­tion sim­ply be­cause they are in­flu­enced by more genes? No, be­cause the dis­tri­b­u­tion around the ex­pec­ta­tion of 0 can be­come wider & wider the more rel­e­vant genes there are.
    
    To rea­son oth­er­wise is, as Samuel­son noted, to think like an in­surer who is wor­ried about los­ing $100 on an in­sur­ance con­tract so it goes out & makes 100 more $100 con­tracts.[](https://gwern.net/everything#fnref10) [↩](#fnref:10)
    
11.  Betz 1986 spe­cial issue’s con­tents:
    
    [“The _g_ fac­tor in em­ploy­ment” ⁠](https://gwern.net/doc/iq/1986-gottfredson.pdf), Got­tfred­son 1986
    
    [“Ori­gins of and Re­ac­tions to the PTC con­fer­ence on _The _g_ Fac­tor In Em­ploy­ment Test­ing_ ” ⁠](https://gwern.net/doc/iq/1986-avery.pdf), Avery 1986
    
    [“ _g_: Ar­ti­fact or re­al­ity?” ⁠](https://gwern.net/doc/iq/1986-jensen-2.pdf), Jensen 1986
    
    [“The role of gen­eral abil­ity in pre­dic­tion” ⁠](https://gwern.net/doc/iq/1986-thorndike.pdf), Thorndike 1986
    
    [“Cog­ni­tive abil­ity, cog­ni­tive ap­ti­tudes, job knowl­edge, and job per­for­mance” ⁠](https://gwern.net/doc/iq/1986-hunter.pdf), Hunter 1986
    
    [“Va­lid­ity ver­sus util­ity of men­tal tests: Ex­am­ple of the SAT” ⁠](https://gwern.net/doc/iq/1986-gottfredson-2.pdf), Got­tfred­son & Crouse 1986
    
    [“So­ci­etal con­se­quences of the _g_ fac­tor in em­ploy­ment” ⁠](https://gwern.net/doc/iq/1986-gottfredson-3.pdf), Got­tfred­son 1986
    
    [“Real world im­pli­ca­tions of _g_ ” ⁠](https://gwern.net/doc/iq/1986-hawk.pdf), Hawk 1986
    
    [“Gen­eral abil­ity in em­ploy­ment: A dis­cus­sion” ⁠](https://gwern.net/doc/iq/1986-arvey.pdf), Arvey 1986
    
    [“Com­men­tary” ⁠](https://gwern.net/doc/iq/1986-humphreys.pdf), Humphreys 1986
    
    [“Com­ments on the _g_ fac­tor in Em­ploy­ment Test­ing” ⁠](https://gwern.net/doc/iq/1986-linn.pdf), Linn 1986
    
    [“Back to Spear­man?” ⁠](https://gwern.net/doc/iq/1986-tyler.pdf), Tyler 1986 [↩](#fnref:11)
    
12.  This work does not seem to have been pub­lished, as I can find no books pub­lished by them jointly, or nor nay Mc­Closky books pub­lished be­tween 1990 35ya & his death in 2004 21ya.[](https://gwern.net/everything#fnref12) [↩](#fnref:12)
    
13.  For de­f­i­n­i­tions & ev­i­dence for, see: [Cheverud 1988 ⁠](https://gwern.net/doc/genetics/heritable/correlation/1988-cheverud.pdf), [Roff 1996 ⁠](https://gwern.net/doc/genetics/heritable/correlation/1995-roff.pdf), [Kruuk et al 2008 ⁠](https://gwern.net/doc/genetics/heritable/correlation/2008-kruuk.pdf), [Dochter­mann 2011 ⁠](https://gwern.net/doc/genetics/heritable/correlation/2011-dochtermann.pdf), [Jor­dan et al 2018 ⁠](https://www.biorxiv.org/content/10.1101/311332.full), & [So­dini et al 2018 ⁠](https://www.biorxiv.org/content/10.1101/291062.full).[](https://gwern.net/everything#fnref13)