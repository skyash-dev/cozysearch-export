It’s been 13 years since Yudkowsky published the sequences, and 11 years since he wrote “ [Rationality is Systematized Winning](https://www.lesswrong.com/posts/4ARtkT3EYox3THYjF/rationality-is-systematized-winning) ”.

So where are all the winners?

The people that jump to mind are Nick Bostrom (Oxford Professor of Philosophy, author), Holden Karnofsky and Elie Hassenfeld (run OpenPhil and GiveWell, directing ~300M in annual donations) and Will MacAskill (Oxford Professor of Philosophy, author).

But somehow that feels like cheating. We know rationalism is a good meme, so it doesn’t seem fair to cite people whose accomplishments are largely built off of convincing someone else that rationalism is important. They’re successful, but at a meta-level, only in the same way Steve Bannon is successful, and to a much lesser extent.

By comparison, Miyamoto Musashi, who Yudkowsky quotes extensively in the post, and who coined the phrase “the Ichi school is the spirit of winning”, appears to have been one of the greatest swordsmen of his era. According to [Wikipedia](https://en.wikipedia.org/wiki/Miyamoto_Musashi#First_duel), he:

-   Fought duels against the most famous schools in Kyoto, and never lost
-   Defeated an “adept” at age 13
-   Was granted the title “Unrivaled Under Heaven” by the shogun at 21
-   Single handedly ended the Yoshioka School by defeating 3 masters, and then a [70 person ambush](https://en.wikipedia.org/wiki/Yoshioka-ry%C5%AB) of followers

One explanation is that rationalism entails a particular value system that guides rationalists away from becoming prominent politicians or entrepreneurs. This isn’t obviously absurd. Very successful people are often described as ambitious, smart and hardworking, but rarely as intellectually honest. Analogously, Ecclesiastical orders often had the highest literacy rates, but had philosophical objections to using that leverage to gain influence in other spheres.

Except that they totally did anyway. Pope Urban II initiated the crusades, a holy war of massive proportions. Pope Clement VII was so influential that King Henry VIII had to leave the church and invent in Anglicanism just to escape. And Pope Innocent III claimed influence over all the kings of Europe, single handedly annulled the Magna Carta, and in an act of nominative anti-determinism, directed the sack of Constantinople.

Even today, churches are enormously powerful and wealthy. The [LDS](https://en.wikipedia.org/wiki/Finances_of_The_Church_of_Jesus_Christ_of_Latter-day_Saints#:~:text=After%20the%20Time%20article%20was,at%20%2425%20to%20%2430%20billion.) has an estimated $5B in annual revenue, with [an estimated $100B](https://www.wsj.com/articles/the-mormon-church-amassed-100-billion-it-was-the-best-kept-secret-in-the-investment-world-11581138011) in funds, and [protestants can make claims to nearly every US president](https://en.wikipedia.org/wiki/Religious_affiliations_of_presidents_of_the_United_States#:~:text=Almost%20all%20of%20the%20presidents,Presbyterians%20being%20the%20most%20prevalent.).

Even if you think rationalism is genuinely rooted in altruism, they strongly recommend “earning to give”, which ought to result in rationalists leading hedge funds, startups and so on.

Tyler Cowen once [called rationalism](https://www.vox.com/2017/3/30/15122162/tyler-cowen-ezra-klein-interview-podcast) “just another kind of religion”, but if so, it seems to be a fairly unsuccessful one.

Still, no one can deny that [they have noticed the skulls](https://slatestarcodex.com/2017/04/07/yes-we-have-noticed-the-skulls/). Whatever criticism one makes of rationalists, they have likely made of themselves many times over.

Immediately after the Systematised Winning, Scott Alexander wrote [Extreme Rationality: It’s Not That Great](https://www.lesswrong.com/posts/LgavAYtzFQZKg95WC/extreme-rationality-it-s-not-that-great) claiming that there is “approximately zero empirical evidence that x-rationality has a large effect on your practical success”, and giving several possible explanations:

-   Weakness of will: Even with good ideas, rationalists can’t execute
-   Specific knowledge beats out generalized theoretical knowledge
-   Noise

The second is particularly interesting, Scott goes on to say that “people in finance tend to know this and use specially developed x-rationalist techniques on the job already without making it a lifestyle commitment.” In addition to specialization, practitioners have the benefit of learning a system of practice built by [tradition smarter than you](https://scholars-stage.blogspot.com/2018/08/tradition-is-smarter-than-you-are.html). A person can compete against their own irrational self, but they shouldn’t expect to win against a centuries old practice.

What about dynamic fields, or new fields without precedent?

Many startup CEOs are young generalists who did not learn their craft through a career at McKinsey or an MBA program at Harvard. It’s not clear that tech entrepreneurship has a storied tradition of practice, aside from maybe the abundance of podcasts free to all.

A better excuse is that startup founders are just totally insane, in a way that precludes any rational or even rationalist person from joining their ranks. Elon Musk famously compared entrepreneurship to “chewing glass and staring into the abyss”; Paul Graham warned “If you start a startup, it will take over your life to a degree you cannot imagine”.

That’s the best case scenario where things go according to plan. The much more likely outcome is that you stagnate for years, raise money from, hire or alienate all your friends, are resented by every employee you sold on a billion dollar exit, who by the way, are the only people you ever see, all while sacrificing your physical and emotional health.

This should also be considered against a very high baseline quality of life. The alternative to running a tech company isn’t the median human existence, it’s being an incredibly well paid engineer or tech executive and retiring in your 30s.

Speaking of baselines, what exactly should we expect from the rationalist community?

If it was a bunch of uneducated basement dwellers, we shouldn’t be shocked to learn that rationalism was unable to transform them into heads of state and captains of industry.

But the [SCC survey results](https://slatestarcodex.com/blog_images/2019%20SSC%20Survey.html) paint a different picture. Over 40% of readers work in software engineering, IT, and computer science, with another 4.7% in finance and 3% in physics, 6.7% in engineering, and 3.8% in law. 12% have PhDs, another 25% have Master’s degrees, and another 40% have Bachelor’s.

All else equal, we should expect these people to do fairly well, or at least have the potential to once imbued with some kind of cognitive superpower.

There are two possible conspiratorial views:

1.  The world is run by rationalists, but you haven’t heard of them. From an [earlier SSC post](https://slatestarcodex.com/2013/04/15/things-i-learned-by-spending-five-thousand-years-in-an-alternate-universe/): “large organizations tend to have a position that pretty much controls everything from behind the scenes but doesn’t have to cope with the appearance of power”
    
2.  The world is run by rationalists, but they haven’t outed themselves.
    

As with all conspiracies, these views are not only crazy, but also unfalsifiable. I could just as easily claim that the world is secretly run by freemasons. From a Bayesian perspective, the evidence is equally well explained by the both hypotheses, so we should stick with our prior that there is not a conspiracy.

Instead, I propose the weak conspiracy, which is that many successful people have read rationalist blogs, but do not make it part of their identity. For example:

-   Tyler Cowen: “ [I enjoy all those sources, and I read them. That’s obviously a kind of endorsement.](https://www.vox.com/2017/3/30/15122162/tyler-cowen-ezra-klein-interview-podcast)”
-   Paul Graham: “ [There’s no one writing now that I admire more than Scott Alexander](https://twitter.com/paulg/status/909070952465600512) ”
-   Matt Levine: “ [Does anyone else think about this [SSC] article every day?](https://twitter.com/matt_levine/status/795743276036923392)”
-   Patrick Collison: “ […Scott Alexander’s deservedly famous Meditations on Moloch](https://twitter.com/patrickc/status/1005903040115814400) ”
-   Steven Pinker: “ [Tragedy in the blogosphere: One of the best [SSC] is being taken down.](https://twitter.com/sapinker/status/1275406500492697601?lang=en)”

Of course, now the question is: “do rationalists become successful, or do successful people just like blogs on the internet?” We might try to infer causality from timing. As of 2009, every one of those people was already successful, which points in favor of the latter hypothesis.

Instead of more theorizing, I’ll end by offering my own testimony.

The primary impacts of reading rationalist blogs are that 1) I have been frequently distracted at work, and 2) my conversations have gotten much worse. Talking to non-rationalists, I am perpetually holding myself back from saying “oh yes, that’s just the thing where [no one has coherent](https://slatestarcodex.com/2017/08/01/is-it-possible-to-have-coherent-principles-around-free-speech-norms/) [meta-principles](https://slatestarcodex.com/2014/09/30/i-can-tolerate-anything-except-the-outgroup/) ” or “that’s the thing where [facts are purpose-dependent](https://meaningness.com/eggplant/refrigerator) ”. Talking to rationalists is not much better, since it feels less like a free exchange of ideas, and more like an exchange of “have you read post?”

There are some specific areas where rationality might help, like using Yudkowsky’s [Inadequate Equilibria](https://equilibriabook.com/) to know when it’s plausible to think I have an original insight that is not already “priced into the market”, but even here, I’m not convinced these beat out specific knowledge. If you want to start a defensible monopoly, reading about [business strategy](https://florentcrivello.com/index.php/2018/07/29/mind-the-moat-a-7-powers-review/) or [startup-specific strategy](https://blakemasters.com/peter-thiels-cs183-startup) will probably be more useful than trying to reason about “efficiency” in a totally abstract sense.

And yet, I will continue reading these blogs, and if Slate Star Codex ever releases a new post, I will likely drop whatever I am doing to read it. This has nothing to do with self-improvement or “systematized winning”.

It’s solely because weird blogs on the internet make me feel less alone.

Before reading [A Human’s Guide to Words](https://wiki.lesswrong.com/wiki/A_Human's_Guide_to_Words) and [The Categories Were Made For Man](https://slatestarcodex.com/2014/11/21/the-categories-were-made-for-man-not-man-for-the-categories/), I went around thinking “oh god, no one is using language coherently, and I seem to be the only one seeing it, but I cannot even express my horror in a comprehensible way.” This felt like a hellish combination of being trapped in an illusion, questioning my own sanity, and simultaneously being unable to scream. For years, I wondered if I was just uniquely broken, and living in a reality that no one else seemed to see or understand.

So the defense of “rationality as systematized winning” is not even a causal one. I don’t claim to have benefited in any real way from rationality, nor can I really recommend it to anyone who isn’t already inclined to read these blogs. The point of winning is not to make money, or gain influence, or even to altruistically produce more QALYs. The point is that for my reality to even feel legitimate, it needs to have bearing on the consensus shared reality.

Now that Slate Star Codex is gone, this project feels more necessary than ever. Substack promised to fund independent writing, but instead, has largely replaced eclectic blogs with rapid-fire newsletters that follow the news-cycle. The kinds of people who would have been rationalist writers are instead mass-producing insight-porn on Twitter, or starting Progress Studies blogs to share historical trivia.

A smart person would take this opportunity to join the bandwagon before it takes off. But I’m not smart, just rationalist, so instead I’m here on a poorly styled blog with a long name lamenting the last days of a community.

Or as Richard Hamming should have said: “the purpose of computers is community, not insight.”

### [EDIT 10/16/2020]

[Thing of Things](https://thingofthings.wordpress.com/2015/10/30/the-world-is-mad/) points out:

_I think a formative moment for any rationalist– our “Uncle Ben shot by the mugger” moment, if you will– is the moment you go “holy shit, everyone in the world is fucking insane.”_

_…I don’t think it’s an accident that a lot of rationalists are mentally ill. Those of us who are mentally ill learn early and painfully that your brain is constantly lying to you for no reason. I don’t think our brains lie to us more than neurotypicals’ brains do; but they lie more dramatically, about things society is not set up to accommodate, and so the lesson is drilled in._

So perhaps rationalism appeals to extremely irrational people, who are, through immense effort and many blogs posts, able to approach baseline sanity.

The SSC survey I cited was from 2019. Maybe all the rationalists with PhDs and lucrative tech jobs in 2019 were totally devoid of any hope for the future in 2009, and it is only thanks to these blogs that they’ve achieved the success they have.

I don’t know of any data that supports this, but it’s not obviously wrong.

### [EDIT 02/18/2021]

[Byrne Hobart writes](https://diff.substack.com/p/why-did-one-internet-subculture-spot):

> The influence of people who read rationalist blogs, but don’t self-identify as rationalists, is quite wide—the blogs are very widely followed in technology circles, and anecdotally have a large audience in the more quantitative branches of finance. Identifying as a rationalist is a losing move, because non-rationalists will think it’s weird (objectively true!) and rationalists will be relatively indifferent to a personal label

### [EDIT 03/08/2021]

Scott Alexander has [some data](https://slatestarcodex.com/2015/03/06/effective-altruists-not-as-mentally-ill-as-you-think/) from the LessWrong survey showing that EAs do not have more mental health problems than the general LW community. But it’s possible they all have mental health problems. I wouldn’t bother comparing this to results about the general population since survey responses depend a lot on the particular survey being administered.

For what it’s worth, I don’t think “mental illness” explains a lack of success. Especially among startup founders, at least according to myth, there are high rates of various “mental health problems”. Though maybe some are more conducive to success than others, and it’s not useful to talk about the generic category.